10/26 04:52:17 PM | args = Namespace(output_dir='result/test/test4', loaded_model_path='./checkpoints/cifar10/vgg_16_bn.pt', resume=False, test_only=False, mode='prune', batch_size=64, nb_batches=200, Mflops_target=None, lr=0.6, momentum=0.9, beta=6.0, gamma=0.2, gpu='4', num_workers=4, dataset='cifar10', arch='vgg_16_bn', save_plot=False, seed=1, lr_finetuning=0.03, epoch_finetuning=200, wd=0.002, data_dir='./data/cifar10/', print_freq=200, num_classes=10, device_ids=[4], device=device(type='cuda', index=0), name_base='')
10/26 04:52:34 PM | ----------------------------------------
10/26 04:52:34 PM | ==> Building model...
10/26 04:52:34 PM | ----------------------------------------
10/26 04:52:34 PM | ==> Loading weights into the model...
10/26 04:52:34 PM | ----------------------------------------
10/26 04:52:34 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=512, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/26 04:52:36 PM | Save modules info...
10/26 04:52:36 PM | FLOPS pruning ratio is 0.77
10/26 04:52:36 PM | Pruning with information flow
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: None
10/26 04:52:36 PM | cout: [0]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [0]
10/26 04:52:36 PM | cout: [0]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [0]
10/26 04:52:36 PM | cout: [0]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [0]
10/26 04:52:36 PM | cout: [1]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [1]
10/26 04:52:36 PM | cout: [1]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [1]
10/26 04:52:36 PM | cout: [1]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/26 04:52:36 PM | cin: [1]
10/26 04:52:36 PM | cout: [1]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [1]
10/26 04:52:36 PM | cout: [2]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [2]
10/26 04:52:36 PM | cout: [2]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [2]
10/26 04:52:36 PM | cout: [2]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [2]
10/26 04:52:36 PM | cout: [3]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [3]
10/26 04:52:36 PM | cout: [3]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [3]
10/26 04:52:36 PM | cout: [3]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/26 04:52:36 PM | cin: [3]
10/26 04:52:36 PM | cout: [3]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [3]
10/26 04:52:36 PM | cout: [4]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [4]
10/26 04:52:36 PM | cout: [4]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [4]
10/26 04:52:36 PM | cout: [4]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [4]
10/26 04:52:36 PM | cout: [5]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [5]
10/26 04:52:36 PM | cout: [5]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [5]
10/26 04:52:36 PM | cout: [5]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [5]
10/26 04:52:36 PM | cout: [6]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [6]
10/26 04:52:36 PM | cout: [6]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [6]
10/26 04:52:36 PM | cout: [6]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/26 04:52:36 PM | cin: [6]
10/26 04:52:36 PM | cout: [6]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [6]
10/26 04:52:36 PM | cout: [7]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [7]
10/26 04:52:36 PM | cout: [7]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [7]
10/26 04:52:36 PM | cout: [7]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [7]
10/26 04:52:36 PM | cout: [8]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [8]
10/26 04:52:36 PM | cout: [8]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [8]
10/26 04:52:36 PM | cout: [8]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [8]
10/26 04:52:36 PM | cout: [9]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [9]
10/26 04:52:36 PM | cout: [9]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [9]
10/26 04:52:36 PM | cout: [9]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/26 04:52:36 PM | cin: [9]
10/26 04:52:36 PM | cout: [9]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [9]
10/26 04:52:36 PM | cout: [10]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [10]
10/26 04:52:36 PM | cout: [10]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [10]
10/26 04:52:36 PM | cout: [10]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [10]
10/26 04:52:36 PM | cout: [11]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [11]
10/26 04:52:36 PM | cout: [11]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [11]
10/26 04:52:36 PM | cout: [11]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/26 04:52:36 PM | cin: [11]
10/26 04:52:36 PM | cout: [12]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/26 04:52:36 PM | cin: [12]
10/26 04:52:36 PM | cout: [12]
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: [12]
10/26 04:52:36 PM | cout: [12]
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Linear(in_features=512, out_features=512, bias=True)
10/26 04:52:36 PM | cin: [12]
10/26 04:52:36 PM | cout: None
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: ReLU(inplace=True)
10/26 04:52:36 PM | cin: None
10/26 04:52:36 PM | cout: None
10/26 04:52:36 PM | active
10/26 04:52:36 PM | -----
10/26 04:52:36 PM | module: Linear(in_features=512, out_features=10, bias=True)
10/26 04:52:36 PM | cin: None
10/26 04:52:36 PM | cout: None
10/26 04:52:36 PM | inactive
10/26 04:52:36 PM | Used masks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10/26 04:52:36 PM | 13 unique masks in total
10/26 04:52:37 PM | === Batch 1/200
10/26 04:52:37 PM | loss = 0.494 + 4.717 + 0.040 = 5.251
10/26 04:52:39 PM | attribution_score[0:12]: 
[ 0.94  0.94  0.94  0.83  0.83  0.83  0.83  0.83  0.83  0.94  0.83  0.94 ]
[ 0.83  0.83  0.94  0.83  0.94  0.83  0.94  0.83  0.83  0.94  0.94  0.83 ]
[ 0.94  0.94  0.83  0.94  0.94  0.94  0.94  0.94  0.94  0.83  0.83  0.94 ]
[ 0.83  0.83  0.94  0.94  0.83  0.83  0.94  0.83  0.94  0.94  0.94  0.94 ]
[ 0.83  0.94  0.94  0.83  0.83  0.94  0.83  0.83  0.94  0.94  0.94  0.94 ]
[ 0.83  0.94  0.94  0.83  0.83  0.83  0.83  0.94  0.83  0.83  0.83  0.94 ]
[ 0.94  0.94  0.83  0.83  0.94  0.83  0.83  0.94  0.94  0.94  0.94  0.83 ]
[ 0.83  0.83  0.83  0.94  0.83  0.83  0.94  0.83  0.94  0.94  0.83  0.83 ]
[ 0.83  0.83  0.83  0.83  0.94  0.83  0.83  0.83  0.83  0.83  0.83  0.94 ]
[ 0.83  0.83  0.83  0.83  0.94  0.83  0.83  0.83  0.83  0.83  0.83  0.83 ]
[ 0.83  0.83  0.94  0.83  0.83  0.94  0.83  0.83  0.83  0.83  0.94  0.83 ]
[ 0.94  0.94  0.94  0.83  0.94  0.94  0.83  0.94  0.94  0.94  0.94  0.83 ]
[ 0.94  0.94  0.94  0.83  0.94  0.94  0.83  0.83  0.94  0.94  0.83  0.83 ]

10/26 04:52:39 PM | === Batch 2/200
10/26 04:52:40 PM | loss = 0.120 + 4.539 + 0.045 = 4.705
10/26 04:52:40 PM | attribution_score[0:12]: 
[ 0.96  0.96  0.94  0.73  0.73  0.75  0.73  0.73  0.73  0.96  0.75  0.96 ]
[ 0.83  0.77  0.96  0.73  0.96  0.81  0.96  0.74  0.81  0.96  0.96  0.76 ]
[ 0.96  0.96  0.76  0.97  0.92  0.96  0.96  0.96  0.95  0.75  0.76  0.96 ]
[ 0.73  0.77  0.96  0.96  0.75  0.88  0.95  0.86  0.96  0.97  0.96  0.97 ]
[ 0.73  0.96  0.95  0.74  0.73  0.96  0.74  0.74  0.96  0.94  0.94  0.94 ]
[ 0.74  0.96  0.96  0.74  0.74  0.81  0.80  0.96  0.75  0.74  0.75  0.96 ]
[ 0.93  0.95  0.75  0.74  0.96  0.74  0.81  0.94  0.96  0.92  0.96  0.73 ]
[ 0.73  0.75  0.73  0.96  0.73  0.74  0.95  0.73  0.96  0.95  0.73  0.74 ]
[ 0.73  0.73  0.74  0.76  0.95  0.73  0.73  0.73  0.74  0.73  0.75  0.96 ]
[ 0.76  0.73  0.74  0.74  0.95  0.74  0.73  0.73  0.73  0.74  0.73  0.73 ]
[ 0.73  0.85  0.96  0.77  0.76  0.96  0.82  0.73  0.85  0.79  0.96  0.82 ]
[ 0.96  0.97  0.95  0.84  0.96  0.93  0.74  0.96  0.95  0.96  0.96  0.86 ]
[ 0.96  0.96  0.96  0.80  0.96  0.97  0.73  0.86  0.95  0.96  0.86  0.78 ]

10/26 04:52:40 PM | === Batch 3/200
10/26 04:52:41 PM | loss = 0.279 + 4.131 + 0.058 = 4.468
10/26 04:52:41 PM | attribution_score[0:12]: 
[ 0.97  0.97  0.92  0.60  0.60  0.72  0.61  0.63  0.60  0.96  0.65  0.96 ]
[ 0.78  0.78  0.97  0.60  0.96  0.82  0.96  0.69  0.83  0.97  0.96  0.65 ]
[ 0.97  0.97  0.65  0.97  0.88  0.96  0.96  0.97  0.95  0.63  0.79  0.97 ]
[ 0.60  0.79  0.96  0.97  0.64  0.92  0.95  0.89  0.97  0.97  0.97  0.97 ]
[ 0.62  0.96  0.96  0.64  0.77  0.97  0.73  0.77  0.97  0.95  0.92  0.92 ]
[ 0.78  0.97  0.96  0.61  0.64  0.74  0.77  0.97  0.64  0.79  0.66  0.97 ]
[ 0.90  0.96  0.81  0.75  0.97  0.70  0.74  0.93  0.97  0.93  0.98  0.72 ]
[ 0.63  0.66  0.60  0.97  0.60  0.73  0.95  0.64  0.97  0.95  0.61  0.62 ]
[ 0.71  0.72  0.62  0.65  0.93  0.60  0.60  0.61  0.76  0.60  0.65  0.97 ]
[ 0.74  0.61  0.61  0.61  0.94  0.70  0.60  0.60  0.63  0.63  0.61  0.61 ]
[ 0.60  0.89  0.97  0.68  0.75  0.96  0.86  0.76  0.89  0.83  0.97  0.86 ]
[ 0.96  0.98  0.95  0.88  0.96  0.90  0.80  0.97  0.94  0.97  0.96  0.90 ]
[ 0.97  0.97  0.98  0.83  0.97  0.98  0.79  0.90  0.96  0.97  0.90  0.79 ]

10/26 04:52:41 PM | === Batch 4/200
10/26 04:52:41 PM | loss = 0.474 + 3.749 + 0.069 = 4.291
10/26 04:52:41 PM | attribution_score[0:12]: 
[ 0.98  0.98  0.89  0.45  0.45  0.74  0.49  0.51  0.46  0.95  0.51  0.96 ]
[ 0.70  0.76  0.98  0.46  0.96  0.83  0.94  0.66  0.83  0.98  0.97  0.60 ]
[ 0.97  0.98  0.54  0.98  0.81  0.97  0.95  0.97  0.95  0.50  0.81  0.97 ]
[ 0.58  0.80  0.97  0.97  0.60  0.94  0.96  0.90  0.97  0.98  0.97  0.97 ]
[ 0.49  0.97  0.97  0.65  0.82  0.97  0.76  0.79  0.98  0.95  0.93  0.89 ]
[ 0.83  0.97  0.94  0.51  0.61  0.69  0.79  0.97  0.50  0.83  0.54  0.98 ]
[ 0.86  0.96  0.86  0.81  0.97  0.70  0.66  0.90  0.98  0.95  0.98  0.73 ]
[ 0.50  0.54  0.47  0.98  0.47  0.78  0.92  0.52  0.97  0.94  0.46  0.61 ]
[ 0.70  0.65  0.48  0.57  0.90  0.45  0.48  0.53  0.80  0.45  0.64  0.98 ]
[ 0.76  0.52  0.47  0.47  0.92  0.73  0.45  0.46  0.66  0.50  0.48  0.52 ]
[ 0.45  0.93  0.97  0.59  0.79  0.95  0.90  0.82  0.92  0.87  0.98  0.90 ]
[ 0.96  0.99  0.95  0.92  0.96  0.90  0.86  0.98  0.93  0.97  0.96  0.93 ]
[ 0.97  0.98  0.99  0.88  0.97  0.99  0.85  0.93  0.96  0.97  0.93  0.84 ]

10/26 04:52:41 PM | === Batch 5/200
10/26 04:52:42 PM | loss = 0.314 + 3.402 + 0.076 = 3.792
10/26 04:52:42 PM | attribution_score[0:12]: 
[ 0.98  0.98  0.83  0.31  0.31  0.78  0.39  0.39  0.32  0.94  0.38  0.96 ]
[ 0.63  0.73  0.98  0.32  0.96  0.83  0.93  0.61  0.84  0.98  0.97  0.56 ]
[ 0.97  0.98  0.42  0.98  0.72  0.97  0.94  0.96  0.93  0.36  0.79  0.97 ]
[ 0.59  0.82  0.97  0.97  0.61  0.96  0.96  0.92  0.97  0.98  0.98  0.97 ]
[ 0.39  0.97  0.98  0.61  0.87  0.98  0.80  0.79  0.99  0.95  0.94  0.84 ]
[ 0.87  0.98  0.93  0.39  0.60  0.65  0.81  0.97  0.39  0.86  0.42  0.99 ]
[ 0.84  0.96  0.89  0.86  0.97  0.70  0.56  0.84  0.98  0.96  0.99  0.77 ]
[ 0.37  0.42  0.35  0.99  0.34  0.82  0.89  0.41  0.97  0.93  0.32  0.63 ]
[ 0.68  0.61  0.34  0.47  0.86  0.31  0.35  0.42  0.83  0.32  0.63  0.99 ]
[ 0.77  0.41  0.33  0.33  0.91  0.75  0.31  0.32  0.72  0.36  0.35  0.41 ]
[ 0.31  0.95  0.97  0.48  0.83  0.94  0.93  0.85  0.94  0.91  0.98  0.92 ]
[ 0.96  0.99  0.93  0.95  0.96  0.87  0.90  0.98  0.90  0.97  0.96  0.95 ]
[ 0.97  0.98  0.99  0.90  0.97  0.99  0.89  0.96  0.97  0.97  0.96  0.88 ]

10/26 04:52:42 PM | === Batch 6/200
10/26 04:52:42 PM | loss = 0.591 + 3.056 + 0.067 = 3.714
10/26 04:52:43 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.75  0.20  0.21  0.79  0.29  0.27  0.20  0.94  0.26  0.97 ]
[ 0.61  0.67  0.98  0.21  0.96  0.83  0.91  0.56  0.84  0.99  0.97  0.58 ]
[ 0.98  0.98  0.31  0.98  0.64  0.97  0.93  0.97  0.91  0.25  0.74  0.98 ]
[ 0.60  0.84  0.98  0.97  0.60  0.97  0.97  0.92  0.97  0.98  0.98  0.97 ]
[ 0.37  0.97  0.99  0.58  0.91  0.99  0.84  0.80  0.99  0.95  0.96  0.77 ]
[ 0.88  0.98  0.92  0.30  0.59  0.68  0.86  0.97  0.33  0.88  0.32  0.99 ]
[ 0.86  0.96  0.91  0.90  0.97  0.72  0.50  0.78  0.98  0.97  0.99  0.81 ]
[ 0.26  0.30  0.27  0.99  0.24  0.86  0.84  0.33  0.96  0.93  0.21  0.63 ]
[ 0.66  0.52  0.22  0.36  0.78  0.20  0.24  0.33  0.84  0.22  0.59  0.99 ]
[ 0.76  0.30  0.22  0.22  0.89  0.74  0.20  0.21  0.78  0.25  0.29  0.30 ]
[ 0.23  0.97  0.97  0.38  0.85  0.95  0.95  0.89  0.95  0.91  0.99  0.94 ]
[ 0.96  0.99  0.91  0.96  0.96  0.88  0.92  0.99  0.88  0.97  0.96  0.97 ]
[ 0.97  0.99  0.99  0.92  0.97  1.00  0.93  0.97  0.97  0.96  0.97  0.91 ]

10/26 04:52:43 PM | === Batch 7/200
10/26 04:52:43 PM | loss = 0.790 + 2.774 + 0.059 = 3.623
10/26 04:52:43 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.66  0.12  0.16  0.81  0.25  0.18  0.12  0.95  0.18  0.97 ]
[ 0.67  0.57  0.98  0.14  0.96  0.80  0.92  0.48  0.81  0.99  0.98  0.63 ]
[ 0.98  0.98  0.22  0.98  0.58  0.97  0.92  0.97  0.86  0.16  0.67  0.98 ]
[ 0.56  0.86  0.98  0.97  0.54  0.98  0.98  0.93  0.97  0.98  0.98  0.97 ]
[ 0.38  0.97  0.99  0.51  0.93  0.99  0.87  0.79  0.99  0.94  0.97  0.72 ]
[ 0.89  0.98  0.90  0.21  0.51  0.72  0.89  0.97  0.28  0.90  0.25  1.00 ]
[ 0.89  0.97  0.92  0.92  0.96  0.70  0.48  0.69  0.98  0.97  0.99  0.85 ]
[ 0.17  0.22  0.21  0.99  0.17  0.88  0.77  0.25  0.96  0.92  0.14  0.64 ]
[ 0.65  0.50  0.14  0.30  0.79  0.12  0.15  0.29  0.85  0.15  0.56  0.99 ]
[ 0.74  0.21  0.14  0.14  0.88  0.69  0.12  0.13  0.85  0.16  0.26  0.20 ]
[ 0.18  0.97  0.97  0.34  0.87  0.96  0.96  0.91  0.97  0.91  0.99  0.95 ]
[ 0.96  0.99  0.88  0.97  0.96  0.90  0.93  0.99  0.86  0.98  0.97  0.97 ]
[ 0.97  0.99  1.00  0.93  0.97  1.00  0.95  0.98  0.98  0.96  0.98  0.93 ]

10/26 04:52:43 PM | === Batch 8/200
10/26 04:52:43 PM | loss = 0.886 + 2.568 + 0.053 = 3.507
10/26 04:52:44 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.54  0.07  0.12  0.80  0.20  0.11  0.07  0.96  0.13  0.97 ]
[ 0.74  0.50  0.98  0.09  0.96  0.78  0.94  0.40  0.79  0.99  0.98  0.69 ]
[ 0.98  0.99  0.14  0.98  0.49  0.97  0.91  0.98  0.79  0.10  0.58  0.98 ]
[ 0.53  0.87  0.98  0.97  0.45  0.98  0.98  0.93  0.97  0.98  0.99  0.98 ]
[ 0.39  0.96  0.99  0.44  0.94  0.99  0.89  0.78  0.99  0.93  0.98  0.75 ]
[ 0.90  0.97  0.88  0.14  0.41  0.72  0.92  0.98  0.25  0.91  0.20  1.00 ]
[ 0.91  0.97  0.93  0.93  0.96  0.65  0.47  0.58  0.99  0.98  0.99  0.88 ]
[ 0.10  0.17  0.17  0.99  0.11  0.90  0.73  0.17  0.96  0.92  0.09  0.65 ]
[ 0.65  0.55  0.09  0.24  0.83  0.07  0.09  0.30  0.83  0.10  0.54  0.99 ]
[ 0.71  0.13  0.10  0.09  0.89  0.62  0.08  0.08  0.90  0.11  0.28  0.14 ]
[ 0.15  0.98  0.98  0.28  0.85  0.97  0.96  0.93  0.97  0.90  0.99  0.95 ]
[ 0.97  1.00  0.86  0.98  0.97  0.92  0.93  0.99  0.88  0.98  0.97  0.98 ]
[ 0.98  0.99  1.00  0.93  0.97  1.00  0.96  0.98  0.98  0.96  0.98  0.93 ]

10/26 04:52:44 PM | === Batch 9/200
10/26 04:52:44 PM | loss = 0.856 + 2.415 + 0.049 = 3.321
10/26 04:52:44 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.47  0.04  0.09  0.78  0.17  0.07  0.04  0.96  0.09  0.98 ]
[ 0.80  0.41  0.98  0.06  0.95  0.75  0.96  0.32  0.76  0.99  0.98  0.75 ]
[ 0.98  0.99  0.09  0.99  0.41  0.97  0.90  0.98  0.70  0.07  0.47  0.98 ]
[ 0.50  0.87  0.98  0.97  0.38  0.98  0.98  0.93  0.97  0.98  0.99  0.98 ]
[ 0.38  0.96  0.99  0.37  0.94  0.99  0.90  0.76  0.99  0.93  0.98  0.77 ]
[ 0.90  0.97  0.85  0.09  0.32  0.71  0.94  0.98  0.24  0.91  0.16  1.00 ]
[ 0.92  0.97  0.94  0.94  0.96  0.63  0.44  0.47  0.99  0.98  0.99  0.90 ]
[ 0.07  0.14  0.14  0.99  0.07  0.91  0.68  0.12  0.95  0.93  0.07  0.66 ]
[ 0.68  0.63  0.06  0.22  0.87  0.05  0.06  0.35  0.80  0.07  0.58  0.99 ]
[ 0.69  0.09  0.07  0.06  0.92  0.56  0.05  0.05  0.93  0.07  0.32  0.10 ]
[ 0.13  0.98  0.98  0.23  0.83  0.98  0.97  0.95  0.98  0.88  0.99  0.95 ]
[ 0.97  1.00  0.88  0.98  0.97  0.95  0.94  0.99  0.91  0.98  0.98  0.98 ]
[ 0.98  0.99  1.00  0.94  0.98  1.00  0.97  0.98  0.99  0.97  0.99  0.92 ]

10/26 04:52:44 PM | === Batch 10/200
10/26 04:52:44 PM | loss = 0.823 + 2.305 + 0.046 = 3.174
10/26 04:52:45 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.40  0.03  0.06  0.76  0.14  0.04  0.03  0.97  0.06  0.98 ]
[ 0.85  0.36  0.98  0.04  0.95  0.75  0.97  0.24  0.77  0.99  0.98  0.80 ]
[ 0.98  0.99  0.06  0.99  0.32  0.97  0.88  0.98  0.65  0.04  0.36  0.98 ]
[ 0.49  0.88  0.98  0.97  0.37  0.99  0.98  0.91  0.97  0.98  0.99  0.98 ]
[ 0.33  0.96  0.99  0.31  0.95  0.99  0.91  0.71  0.99  0.93  0.98  0.79 ]
[ 0.90  0.97  0.81  0.06  0.26  0.67  0.95  0.98  0.21  0.91  0.12  1.00 ]
[ 0.92  0.97  0.95  0.94  0.96  0.60  0.39  0.39  0.99  0.98  0.99  0.90 ]
[ 0.04  0.13  0.12  1.00  0.05  0.92  0.60  0.08  0.95  0.93  0.06  0.67 ]
[ 0.71  0.72  0.04  0.22  0.90  0.03  0.04  0.43  0.75  0.05  0.66  0.99 ]
[ 0.72  0.06  0.05  0.04  0.93  0.55  0.04  0.04  0.95  0.05  0.34  0.08 ]
[ 0.10  0.98  0.99  0.20  0.83  0.99  0.97  0.96  0.99  0.88  0.99  0.96 ]
[ 0.98  1.00  0.91  0.99  0.98  0.96  0.95  0.99  0.91  0.99  0.98  0.99 ]
[ 0.99  0.99  1.00  0.95  0.98  1.00  0.98  0.99  0.99  0.97  0.99  0.91 ]

10/26 04:52:45 PM | === Batch 11/200
10/26 04:52:45 PM | loss = 1.063 + 2.231 + 0.044 = 3.337
10/26 04:52:46 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.37  0.02  0.05  0.73  0.12  0.02  0.02  0.97  0.04  0.98 ]
[ 0.89  0.32  0.99  0.03  0.94  0.78  0.97  0.19  0.78  0.99  0.98  0.83 ]
[ 0.98  0.99  0.04  0.99  0.25  0.97  0.86  0.98  0.57  0.03  0.26  0.98 ]
[ 0.48  0.88  0.98  0.97  0.38  0.99  0.99  0.89  0.97  0.97  0.99  0.98 ]
[ 0.32  0.96  0.99  0.25  0.95  0.99  0.90  0.65  0.99  0.93  0.99  0.82 ]
[ 0.91  0.97  0.79  0.04  0.20  0.64  0.96  0.97  0.19  0.90  0.09  1.00 ]
[ 0.91  0.97  0.95  0.95  0.97  0.56  0.34  0.36  0.99  0.99  0.99  0.90 ]
[ 0.03  0.13  0.11  1.00  0.04  0.93  0.55  0.05  0.96  0.93  0.05  0.68 ]
[ 0.75  0.80  0.03  0.24  0.93  0.02  0.03  0.50  0.70  0.04  0.72  0.99 ]
[ 0.75  0.05  0.03  0.03  0.94  0.55  0.03  0.02  0.97  0.04  0.37  0.07 ]
[ 0.08  0.98  0.99  0.18  0.82  0.99  0.98  0.97  0.99  0.88  1.00  0.96 ]
[ 0.98  1.00  0.94  0.99  0.98  0.97  0.95  0.99  0.92  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.95  0.98  1.00  0.98  0.99  0.99  0.97  0.99  0.89 ]

10/26 04:52:46 PM | === Batch 12/200
10/26 04:52:46 PM | loss = 0.906 + 2.169 + 0.042 = 3.116
10/26 04:52:46 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.36  0.01  0.04  0.69  0.10  0.02  0.01  0.97  0.03  0.98 ]
[ 0.91  0.30  0.99  0.02  0.93  0.81  0.98  0.15  0.81  0.99  0.98  0.85 ]
[ 0.98  0.99  0.03  0.99  0.18  0.97  0.84  0.98  0.47  0.02  0.18  0.99 ]
[ 0.49  0.87  0.99  0.97  0.39  0.99  0.99  0.88  0.97  0.97  0.99  0.98 ]
[ 0.32  0.96  1.00  0.21  0.96  0.99  0.91  0.60  0.99  0.94  0.99  0.82 ]
[ 0.91  0.97  0.74  0.03  0.16  0.58  0.96  0.97  0.17  0.89  0.07  1.00 ]
[ 0.89  0.97  0.96  0.95  0.97  0.51  0.31  0.39  0.99  0.99  0.99  0.90 ]
[ 0.02  0.13  0.10  1.00  0.03  0.93  0.50  0.04  0.96  0.93  0.05  0.71 ]
[ 0.77  0.86  0.02  0.27  0.94  0.01  0.02  0.56  0.66  0.03  0.78  1.00 ]
[ 0.79  0.04  0.02  0.02  0.95  0.56  0.02  0.02  0.98  0.03  0.34  0.06 ]
[ 0.06  0.99  0.99  0.19  0.84  0.99  0.98  0.98  0.99  0.90  1.00  0.97 ]
[ 0.98  1.00  0.95  0.99  0.99  0.98  0.96  0.99  0.92  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.96  0.99  1.00  0.99  0.99  0.99  0.98  0.99  0.85 ]

10/26 04:52:46 PM | === Batch 13/200
10/26 04:52:46 PM | loss = 0.862 + 2.116 + 0.040 = 3.018
10/26 04:52:47 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.37  0.01  0.03  0.66  0.08  0.01  0.01  0.97  0.02  0.98 ]
[ 0.93  0.29  0.99  0.01  0.91  0.84  0.98  0.12  0.83  0.99  0.97  0.86 ]
[ 0.98  0.99  0.02  0.99  0.13  0.96  0.81  0.98  0.36  0.01  0.13  0.99 ]
[ 0.46  0.86  0.99  0.97  0.40  0.99  0.99  0.89  0.97  0.96  0.99  0.98 ]
[ 0.36  0.96  1.00  0.17  0.96  0.99  0.92  0.55  1.00  0.94  0.99  0.80 ]
[ 0.92  0.97  0.71  0.02  0.13  0.52  0.96  0.97  0.15  0.89  0.06  1.00 ]
[ 0.85  0.97  0.96  0.95  0.97  0.47  0.29  0.45  0.99  0.99  1.00  0.90 ]
[ 0.01  0.12  0.10  1.00  0.02  0.92  0.48  0.03  0.96  0.93  0.04  0.73 ]
[ 0.81  0.91  0.02  0.31  0.95  0.01  0.02  0.60  0.66  0.03  0.83  1.00 ]
[ 0.81  0.03  0.02  0.02  0.96  0.59  0.01  0.01  0.99  0.02  0.31  0.05 ]
[ 0.05  0.99  0.99  0.20  0.86  1.00  0.98  0.98  0.99  0.92  1.00  0.97 ]
[ 0.99  1.00  0.96  0.99  0.99  0.98  0.96  1.00  0.92  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.97  0.99  1.00  0.99  0.99  0.99  0.98  0.99  0.79 ]

10/26 04:52:47 PM | === Batch 14/200
10/26 04:52:47 PM | loss = 0.928 + 2.069 + 0.038 = 3.035
10/26 04:52:47 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.37  0.01  0.02  0.66  0.07  0.01  0.01  0.97  0.01  0.98 ]
[ 0.95  0.29  0.99  0.01  0.88  0.86  0.98  0.10  0.86  0.99  0.97  0.86 ]
[ 0.98  0.99  0.02  0.99  0.09  0.96  0.75  0.98  0.26  0.01  0.09  0.99 ]
[ 0.45  0.85  0.99  0.97  0.44  0.99  0.99  0.90  0.97  0.96  0.99  0.98 ]
[ 0.42  0.96  1.00  0.15  0.96  0.99  0.93  0.49  1.00  0.95  0.99  0.77 ]
[ 0.92  0.97  0.63  0.02  0.11  0.46  0.96  0.97  0.13  0.89  0.04  1.00 ]
[ 0.80  0.97  0.96  0.95  0.97  0.45  0.26  0.51  0.99  0.99  1.00  0.91 ]
[ 0.01  0.10  0.10  1.00  0.01  0.91  0.44  0.02  0.96  0.92  0.04  0.74 ]
[ 0.83  0.94  0.01  0.38  0.96  0.01  0.01  0.65  0.69  0.02  0.88  1.00 ]
[ 0.84  0.02  0.01  0.01  0.96  0.63  0.01  0.01  0.99  0.01  0.26  0.04 ]
[ 0.04  0.99  0.99  0.22  0.89  1.00  0.99  0.98  1.00  0.94  1.00  0.98 ]
[ 0.99  1.00  0.97  0.99  0.99  0.99  0.97  1.00  0.92  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.97  0.99  1.00  0.99  0.99  1.00  0.98  0.99  0.71 ]

10/26 04:52:47 PM | === Batch 15/200
10/26 04:52:48 PM | loss = 0.820 + 2.023 + 0.036 = 2.880
10/26 04:52:48 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.38  0.00  0.02  0.64  0.05  0.00  0.00  0.96  0.01  0.98 ]
[ 0.96  0.26  0.99  0.01  0.85  0.88  0.99  0.08  0.86  0.99  0.97  0.87 ]
[ 0.98  0.99  0.01  0.99  0.06  0.95  0.69  0.98  0.18  0.01  0.07  0.99 ]
[ 0.42  0.81  0.99  0.97  0.48  0.99  0.99  0.91  0.97  0.95  0.99  0.98 ]
[ 0.46  0.96  1.00  0.13  0.96  1.00  0.93  0.41  1.00  0.95  0.99  0.73 ]
[ 0.92  0.96  0.56  0.01  0.10  0.43  0.97  0.96  0.11  0.88  0.04  1.00 ]
[ 0.73  0.97  0.95  0.95  0.97  0.42  0.23  0.59  0.99  0.99  1.00  0.91 ]
[ 0.01  0.09  0.09  1.00  0.01  0.90  0.38  0.02  0.96  0.91  0.04  0.76 ]
[ 0.83  0.95  0.01  0.46  0.97  0.01  0.01  0.71  0.72  0.02  0.91  1.00 ]
[ 0.86  0.02  0.01  0.01  0.97  0.67  0.01  0.01  0.99  0.01  0.21  0.04 ]
[ 0.03  0.99  1.00  0.26  0.92  1.00  0.99  0.98  1.00  0.96  1.00  0.98 ]
[ 0.99  1.00  0.98  0.99  0.99  0.99  0.97  1.00  0.92  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  1.00  1.00  0.98  0.99  0.61 ]

10/26 04:52:48 PM | === Batch 16/200
10/26 04:52:48 PM | loss = 0.878 + 1.985 + 0.035 = 2.897
10/26 04:52:49 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.42  0.00  0.01  0.58  0.05  0.00  0.00  0.96  0.01  0.98 ]
[ 0.97  0.21  0.99  0.01  0.81  0.89  0.99  0.07  0.87  0.99  0.98  0.89 ]
[ 0.98  0.99  0.01  0.99  0.05  0.94  0.62  0.98  0.12  0.00  0.05  0.99 ]
[ 0.40  0.76  0.99  0.97  0.52  0.99  0.99  0.90  0.97  0.93  0.99  0.97 ]
[ 0.48  0.96  1.00  0.11  0.96  1.00  0.93  0.37  1.00  0.95  0.99  0.70 ]
[ 0.92  0.96  0.51  0.01  0.08  0.41  0.97  0.96  0.10  0.86  0.03  1.00 ]
[ 0.69  0.97  0.95  0.96  0.98  0.39  0.20  0.63  0.99  0.99  1.00  0.91 ]
[ 0.01  0.08  0.09  1.00  0.01  0.89  0.35  0.01  0.96  0.89  0.04  0.76 ]
[ 0.83  0.97  0.01  0.53  0.97  0.00  0.01  0.76  0.74  0.01  0.93  1.00 ]
[ 0.88  0.02  0.01  0.01  0.97  0.70  0.01  0.01  0.99  0.01  0.17  0.04 ]
[ 0.02  0.99  1.00  0.30  0.94  1.00  0.99  0.99  1.00  0.97  1.00  0.98 ]
[ 0.99  1.00  0.98  1.00  0.99  0.99  0.98  1.00  0.92  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  1.00  1.00  0.98  0.99  0.52 ]

10/26 04:52:49 PM | === Batch 17/200
10/26 04:52:49 PM | loss = 0.954 + 1.947 + 0.033 = 2.934
10/26 04:52:49 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.47  0.00  0.01  0.54  0.04  0.00  0.00  0.95  0.01  0.98 ]
[ 0.97  0.19  0.99  0.00  0.75  0.88  0.99  0.06  0.87  0.99  0.98  0.91 ]
[ 0.98  0.99  0.01  0.99  0.03  0.92  0.52  0.98  0.09  0.00  0.03  0.99 ]
[ 0.34  0.69  0.99  0.97  0.58  0.99  0.99  0.88  0.97  0.91  0.99  0.96 ]
[ 0.50  0.97  1.00  0.10  0.96  1.00  0.94  0.35  1.00  0.96  0.99  0.66 ]
[ 0.92  0.96  0.49  0.01  0.08  0.40  0.97  0.96  0.10  0.83  0.02  1.00 ]
[ 0.68  0.97  0.95  0.96  0.98  0.41  0.18  0.61  0.99  0.99  1.00  0.91 ]
[ 0.00  0.08  0.09  1.00  0.01  0.88  0.32  0.01  0.96  0.88  0.03  0.78 ]
[ 0.85  0.98  0.01  0.56  0.97  0.00  0.01  0.80  0.77  0.01  0.95  1.00 ]
[ 0.88  0.01  0.01  0.01  0.97  0.72  0.01  0.00  1.00  0.01  0.15  0.03 ]
[ 0.02  0.99  1.00  0.30  0.95  1.00  0.99  0.99  1.00  0.97  1.00  0.98 ]
[ 0.99  1.00  0.98  1.00  1.00  0.99  0.98  1.00  0.91  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.99  0.50 ]

10/26 04:52:49 PM | === Batch 18/200
10/26 04:52:49 PM | loss = 1.099 + 1.913 + 0.032 = 3.044
10/26 04:52:50 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.51  0.00  0.01  0.49  0.04  0.00  0.00  0.95  0.00  0.98 ]
[ 0.98  0.17  0.99  0.00  0.68  0.88  0.99  0.05  0.86  0.99  0.98  0.92 ]
[ 0.98  0.99  0.01  0.99  0.02  0.90  0.47  0.98  0.06  0.00  0.02  0.99 ]
[ 0.28  0.62  0.99  0.96  0.61  0.99  0.99  0.85  0.97  0.88  0.99  0.95 ]
[ 0.51  0.97  1.00  0.10  0.96  1.00  0.94  0.32  1.00  0.96  0.99  0.60 ]
[ 0.92  0.95  0.49  0.01  0.07  0.39  0.97  0.95  0.10  0.83  0.02  1.00 ]
[ 0.71  0.97  0.94  0.96  0.98  0.45  0.15  0.56  0.99  0.99  1.00  0.92 ]
[ 0.00  0.07  0.08  1.00  0.01  0.88  0.29  0.01  0.96  0.87  0.03  0.80 ]
[ 0.87  0.98  0.01  0.57  0.98  0.00  0.01  0.83  0.79  0.01  0.96  1.00 ]
[ 0.87  0.01  0.01  0.01  0.98  0.74  0.01  0.00  1.00  0.01  0.13  0.03 ]
[ 0.02  0.99  1.00  0.26  0.96  1.00  0.99  0.99  1.00  0.98  1.00  0.98 ]
[ 0.99  1.00  0.98  1.00  1.00  0.99  0.98  1.00  0.90  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.99  0.52 ]

10/26 04:52:50 PM | === Batch 19/200
10/26 04:52:50 PM | loss = 1.050 + 1.878 + 0.031 = 2.960
10/26 04:52:50 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.53  0.00  0.01  0.46  0.03  0.00  0.00  0.94  0.00  0.99 ]
[ 0.98  0.15  0.99  0.00  0.70  0.87  0.99  0.04  0.84  0.99  0.98  0.93 ]
[ 0.98  0.99  0.01  0.99  0.02  0.90  0.50  0.97  0.04  0.00  0.02  0.99 ]
[ 0.21  0.54  0.99  0.96  0.66  0.99  0.99  0.79  0.97  0.88  0.99  0.94 ]
[ 0.54  0.98  1.00  0.09  0.96  1.00  0.94  0.28  1.00  0.96  0.99  0.56 ]
[ 0.91  0.95  0.51  0.01  0.06  0.40  0.97  0.95  0.09  0.80  0.02  1.00 ]
[ 0.73  0.97  0.94  0.96  0.98  0.43  0.12  0.49  0.99  0.99  1.00  0.92 ]
[ 0.00  0.06  0.08  1.00  0.00  0.89  0.28  0.01  0.96  0.87  0.03  0.81 ]
[ 0.90  0.99  0.01  0.55  0.98  0.00  0.01  0.84  0.81  0.01  0.97  1.00 ]
[ 0.87  0.01  0.00  0.00  0.98  0.75  0.00  0.00  1.00  0.00  0.12  0.03 ]
[ 0.01  1.00  1.00  0.22  0.96  1.00  0.99  0.99  1.00  0.98  1.00  0.98 ]
[ 0.99  1.00  0.98  1.00  1.00  0.99  0.98  1.00  0.90  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.99  0.58 ]

10/26 04:52:50 PM | === Batch 20/200
10/26 04:52:51 PM | loss = 0.949 + 1.847 + 0.031 = 2.827
10/26 04:52:51 PM | === Batch 21/200
10/26 04:52:51 PM | loss = 1.069 + 1.819 + 0.030 = 2.918
10/26 04:52:52 PM | === Batch 22/200
10/26 04:52:52 PM | loss = 1.045 + 1.795 + 0.029 = 2.868
10/26 04:52:53 PM | === Batch 23/200
10/26 04:52:53 PM | loss = 0.880 + 1.775 + 0.028 = 2.684
10/26 04:52:53 PM | === Batch 24/200
10/26 04:52:54 PM | loss = 1.005 + 1.755 + 0.028 = 2.788
10/26 04:52:54 PM | === Batch 25/200
10/26 04:52:54 PM | loss = 0.983 + 1.737 + 0.027 = 2.747
10/26 04:52:55 PM | === Batch 26/200
10/26 04:52:55 PM | loss = 0.865 + 1.715 + 0.026 = 2.607
10/26 04:52:56 PM | === Batch 27/200
10/26 04:52:56 PM | loss = 0.917 + 1.695 + 0.026 = 2.637
10/26 04:52:56 PM | === Batch 28/200
10/26 04:52:57 PM | loss = 1.109 + 1.671 + 0.025 = 2.805
10/26 04:52:57 PM | === Batch 29/200
10/26 04:52:57 PM | loss = 0.944 + 1.653 + 0.025 = 2.622
10/26 04:52:58 PM | === Batch 30/200
10/26 04:52:58 PM | loss = 0.924 + 1.632 + 0.025 = 2.580
10/26 04:52:58 PM | === Batch 31/200
10/26 04:52:59 PM | loss = 0.986 + 1.612 + 0.024 = 2.623
10/26 04:52:59 PM | === Batch 32/200
10/26 04:52:59 PM | loss = 0.965 + 1.596 + 0.024 = 2.585
10/26 04:53:00 PM | === Batch 33/200
10/26 04:53:00 PM | loss = 1.038 + 1.581 + 0.023 = 2.642
10/26 04:53:01 PM | === Batch 34/200
10/26 04:53:01 PM | loss = 1.042 + 1.565 + 0.023 = 2.630
10/26 04:53:01 PM | === Batch 35/200
10/26 04:53:02 PM | loss = 1.090 + 1.552 + 0.022 = 2.665
10/26 04:53:02 PM | === Batch 36/200
10/26 04:53:02 PM | loss = 0.954 + 1.537 + 0.022 = 2.513
10/26 04:53:03 PM | === Batch 37/200
10/26 04:53:03 PM | loss = 1.056 + 1.526 + 0.021 = 2.604
10/26 04:53:03 PM | === Batch 38/200
10/26 04:53:04 PM | loss = 1.141 + 1.514 + 0.021 = 2.676
10/26 04:53:04 PM | === Batch 39/200
10/26 04:53:04 PM | loss = 0.806 + 1.499 + 0.021 = 2.326
10/26 04:53:05 PM | === Batch 40/200
10/26 04:53:05 PM | loss = 0.984 + 1.483 + 0.021 = 2.489
10/26 04:53:05 PM | === Batch 41/200
10/26 04:53:06 PM | loss = 1.120 + 1.465 + 0.021 = 2.606
10/26 04:53:06 PM | === Batch 42/200
10/26 04:53:06 PM | loss = 0.898 + 1.448 + 0.020 = 2.367
10/26 04:53:07 PM | === Batch 43/200
10/26 04:53:07 PM | loss = 0.899 + 1.432 + 0.020 = 2.351
10/26 04:53:07 PM | === Batch 44/200
10/26 04:53:08 PM | loss = 0.971 + 1.417 + 0.020 = 2.408
10/26 04:53:08 PM | === Batch 45/200
10/26 04:53:08 PM | loss = 1.084 + 1.403 + 0.019 = 2.506
10/26 04:53:09 PM | === Batch 46/200
10/26 04:53:09 PM | loss = 1.057 + 1.388 + 0.019 = 2.464
10/26 04:53:09 PM | === Batch 47/200
10/26 04:53:10 PM | loss = 0.939 + 1.373 + 0.019 = 2.331
10/26 04:53:10 PM | === Batch 48/200
10/26 04:53:10 PM | loss = 1.071 + 1.359 + 0.018 = 2.448
10/26 04:53:11 PM | === Batch 49/200
10/26 04:53:11 PM | loss = 1.028 + 1.345 + 0.018 = 2.392
10/26 04:53:11 PM | === Batch 50/200
10/26 04:53:11 PM | loss = 1.138 + 1.333 + 0.018 = 2.488
10/26 04:53:12 PM | === Batch 51/200
10/26 04:53:12 PM | loss = 0.987 + 1.320 + 0.018 = 2.325
10/26 04:53:12 PM | === Batch 52/200
10/26 04:53:13 PM | loss = 1.113 + 1.308 + 0.018 = 2.438
10/26 04:53:13 PM | === Batch 53/200
10/26 04:53:13 PM | loss = 1.072 + 1.296 + 0.018 = 2.385
10/26 04:53:14 PM | === Batch 54/200
10/26 04:53:14 PM | loss = 1.190 + 1.283 + 0.017 = 2.491
10/26 04:53:14 PM | === Batch 55/200
10/26 04:53:15 PM | loss = 1.216 + 1.270 + 0.017 = 2.502
10/26 04:53:15 PM | === Batch 56/200
10/26 04:53:15 PM | loss = 1.082 + 1.257 + 0.017 = 2.356
10/26 04:53:16 PM | === Batch 57/200
10/26 04:53:16 PM | loss = 0.977 + 1.246 + 0.016 = 2.239
10/26 04:53:16 PM | === Batch 58/200
10/26 04:53:17 PM | loss = 1.214 + 1.236 + 0.016 = 2.467
10/26 04:53:17 PM | === Batch 59/200
10/26 04:53:17 PM | loss = 1.163 + 1.228 + 0.016 = 2.407
10/26 04:53:18 PM | === Batch 60/200
10/26 04:53:18 PM | loss = 1.175 + 1.221 + 0.015 = 2.412
10/26 04:53:18 PM | === Batch 61/200
10/26 04:53:19 PM | loss = 1.251 + 0.803 + 0.015 = 2.069
10/26 04:53:19 PM | === Batch 62/200
10/26 04:53:19 PM | loss = 1.415 + 0.795 + 0.015 = 2.225
10/26 04:53:20 PM | === Batch 63/200
10/26 04:53:20 PM | loss = 1.402 + 0.786 + 0.015 = 2.203
10/26 04:53:20 PM | === Batch 64/200
10/26 04:53:20 PM | loss = 1.167 + 0.780 + 0.015 = 1.961
10/26 04:53:21 PM | === Batch 65/200
10/26 04:53:21 PM | loss = 1.242 + 0.773 + 0.015 = 2.029
10/26 04:53:22 PM | === Batch 66/200
10/26 04:53:22 PM | loss = 1.059 + 0.767 + 0.014 = 1.841
10/26 04:53:22 PM | === Batch 67/200
10/26 04:53:23 PM | loss = 1.215 + 0.761 + 0.014 = 1.989
10/26 04:53:23 PM | === Batch 68/200
10/26 04:53:23 PM | loss = 1.076 + 0.755 + 0.014 = 1.845
10/26 04:53:24 PM | === Batch 69/200
10/26 04:53:24 PM | loss = 1.207 + 0.749 + 0.014 = 1.969
10/26 04:53:24 PM | === Batch 70/200
10/26 04:53:24 PM | loss = 1.285 + 0.741 + 0.013 = 2.039
10/26 04:53:25 PM | === Batch 71/200
10/26 04:53:25 PM | loss = 1.106 + 0.731 + 0.013 = 1.850
10/26 04:53:26 PM | === Batch 72/200
10/26 04:53:26 PM | loss = 1.237 + 0.720 + 0.013 = 1.970
10/26 04:53:26 PM | === Batch 73/200
10/26 04:53:26 PM | loss = 1.100 + 0.708 + 0.013 = 1.821
10/26 04:53:27 PM | === Batch 74/200
10/26 04:53:27 PM | loss = 1.069 + 0.698 + 0.013 = 1.780
10/26 04:53:27 PM | === Batch 75/200
10/26 04:53:28 PM | loss = 1.210 + 0.689 + 0.013 = 1.912
10/26 04:53:28 PM | === Batch 76/200
10/26 04:53:29 PM | loss = 1.358 + 0.682 + 0.013 = 2.053
10/26 04:53:29 PM | === Batch 77/200
10/26 04:53:29 PM | loss = 1.266 + 0.677 + 0.013 = 1.956
10/26 04:53:30 PM | === Batch 78/200
10/26 04:53:30 PM | loss = 1.209 + 0.672 + 0.012 = 1.894
10/26 04:53:30 PM | === Batch 79/200
10/26 04:53:31 PM | loss = 1.271 + 0.669 + 0.012 = 1.952
10/26 04:53:31 PM | === Batch 80/200
10/26 04:53:32 PM | loss = 1.111 + 0.664 + 0.012 = 1.787
10/26 04:53:32 PM | === Batch 81/200
10/26 04:53:32 PM | loss = 1.067 + 0.336 + 0.012 = 1.415
10/26 04:53:33 PM | === Batch 82/200
10/26 04:53:33 PM | loss = 0.991 + 0.330 + 0.012 = 1.333
10/26 04:53:33 PM | === Batch 83/200
10/26 04:53:33 PM | loss = 1.117 + 0.323 + 0.012 = 1.452
10/26 04:53:34 PM | === Batch 84/200
10/26 04:53:34 PM | loss = 1.171 + 0.314 + 0.012 = 1.497
10/26 04:53:35 PM | === Batch 85/200
10/26 04:53:35 PM | loss = 1.341 + 0.304 + 0.012 = 1.657
10/26 04:53:35 PM | === Batch 86/200
10/26 04:53:35 PM | loss = 0.971 + 0.292 + 0.012 = 1.274
10/26 04:53:36 PM | === Batch 87/200
10/26 04:53:36 PM | loss = 1.285 + 0.281 + 0.012 = 1.578
10/26 04:53:37 PM | === Batch 88/200
10/26 04:53:37 PM | loss = 1.176 + 0.272 + 0.012 = 1.459
10/26 04:53:37 PM | === Batch 89/200
10/26 04:53:38 PM | loss = 1.305 + 0.263 + 0.012 = 1.580
10/26 04:53:38 PM | === Batch 90/200
10/26 04:53:38 PM | loss = 1.288 + 0.255 + 0.011 = 1.555
10/26 04:53:39 PM | === Batch 91/200
10/26 04:53:39 PM | loss = 1.122 + 0.247 + 0.011 = 1.380
10/26 04:53:39 PM | === Batch 92/200
10/26 04:53:40 PM | loss = 1.183 + 0.239 + 0.011 = 1.433
10/26 04:53:40 PM | === Batch 93/200
10/26 04:53:40 PM | loss = 1.184 + 0.231 + 0.011 = 1.427
10/26 04:53:41 PM | === Batch 94/200
10/26 04:53:41 PM | loss = 1.162 + 0.226 + 0.011 = 1.399
10/26 04:53:42 PM | === Batch 95/200
10/26 04:53:42 PM | loss = 1.357 + 0.219 + 0.011 = 1.587
10/26 04:53:42 PM | === Batch 96/200
10/26 04:53:43 PM | loss = 1.473 + 0.213 + 0.011 = 1.697
10/26 04:53:43 PM | === Batch 97/200
10/26 04:53:43 PM | loss = 1.337 + 0.204 + 0.011 = 1.552
10/26 04:53:44 PM | === Batch 98/200
10/26 04:53:44 PM | loss = 1.331 + 0.194 + 0.011 = 1.536
10/26 04:53:44 PM | === Batch 99/200
10/26 04:53:45 PM | loss = 1.324 + 0.184 + 0.011 = 1.518
10/26 04:53:45 PM | === Batch 100/200
10/26 04:53:45 PM | loss = 1.338 + 0.176 + 0.011 = 1.525
10/26 04:53:46 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.01  0.00  0.00  0.12  0.00  0.00  0.00  0.99  0.00  0.87 ]
[ 1.00  0.00  1.00  0.00  0.95  0.82  1.00  0.00  0.97  1.00  0.99  0.03 ]
[ 1.00  0.98  0.00  1.00  0.00  1.00  0.01  0.99  0.00  0.00  0.00  0.98 ]
[ 0.01  0.01  0.99  0.98  0.97  0.99  0.98  0.73  0.10  0.01  1.00  0.93 ]
[ 0.07  0.04  1.00  0.01  0.03  0.82  0.00  0.00  1.00  0.90  0.99  0.97 ]
[ 0.99  1.00  0.00  0.00  0.01  0.09  0.01  0.00  0.01  0.03  0.00  0.98 ]
[ 0.97  0.93  0.00  0.99  1.00  0.03  0.00  0.01  0.99  1.00  1.00  0.99 ]
[ 0.00  0.02  0.00  1.00  0.00  0.99  0.99  0.00  0.99  0.98  0.00  0.43 ]
[ 1.00  1.00  0.00  0.08  0.99  0.00  0.00  0.01  0.98  0.00  1.00  1.00 ]
[ 0.01  0.00  0.00  0.00  1.00  0.98  0.00  0.00  1.00  0.00  0.01  0.00 ]
[ 0.00  1.00  1.00  0.00  0.92  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.99  1.00  1.00  0.99  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.01 ]

10/26 04:53:46 PM | === Batch 101/200
10/26 04:53:46 PM | loss = 1.423 + 0.174 + 0.010 = 1.608
10/26 04:53:46 PM | === Batch 102/200
10/26 04:53:47 PM | loss = 1.363 + 0.174 + 0.010 = 1.547
10/26 04:53:47 PM | === Batch 103/200
10/26 04:53:47 PM | loss = 1.274 + 0.175 + 0.010 = 1.459
10/26 04:53:48 PM | === Batch 104/200
10/26 04:53:48 PM | loss = 1.310 + 0.174 + 0.010 = 1.494
10/26 04:53:48 PM | === Batch 105/200
10/26 04:53:49 PM | loss = 1.308 + 0.173 + 0.010 = 1.491
10/26 04:53:49 PM | === Batch 106/200
10/26 04:53:49 PM | loss = 1.274 + 0.173 + 0.010 = 1.457
10/26 04:53:50 PM | === Batch 107/200
10/26 04:53:50 PM | loss = 1.400 + 0.174 + 0.010 = 1.583
10/26 04:53:50 PM | === Batch 108/200
10/26 04:53:50 PM | loss = 1.308 + 0.173 + 0.009 = 1.490
10/26 04:53:51 PM | === Batch 109/200
10/26 04:53:51 PM | loss = 1.233 + 0.172 + 0.009 = 1.414
10/26 04:53:52 PM | === Batch 110/200
10/26 04:53:52 PM | loss = 1.060 + 0.171 + 0.009 = 1.241
10/26 04:53:52 PM | === Batch 111/200
10/26 04:53:53 PM | loss = 1.336 + 0.170 + 0.009 = 1.515
10/26 04:53:53 PM | === Batch 112/200
10/26 04:53:53 PM | loss = 1.070 + 0.169 + 0.008 = 1.247
10/26 04:53:54 PM | === Batch 113/200
10/26 04:53:54 PM | loss = 1.168 + 0.166 + 0.008 = 1.342
10/26 04:53:54 PM | === Batch 114/200
10/26 04:53:55 PM | loss = 1.215 + 0.162 + 0.008 = 1.385
10/26 04:53:55 PM | === Batch 115/200
10/26 04:53:55 PM | loss = 1.294 + 0.158 + 0.008 = 1.460
10/26 04:53:56 PM | === Batch 116/200
10/26 04:53:56 PM | loss = 1.235 + 0.156 + 0.008 = 1.399
10/26 04:53:56 PM | === Batch 117/200
10/26 04:53:56 PM | loss = 1.249 + 0.154 + 0.008 = 1.411
10/26 04:53:57 PM | === Batch 118/200
10/26 04:53:57 PM | loss = 1.317 + 0.153 + 0.008 = 1.478
10/26 04:53:57 PM | === Batch 119/200
10/26 04:53:58 PM | loss = 1.203 + 0.153 + 0.008 = 1.363
10/26 04:53:58 PM | === Batch 120/200
10/26 04:53:58 PM | loss = 1.156 + 0.151 + 0.008 = 1.316
10/26 04:53:59 PM | === Batch 121/200
10/26 04:53:59 PM | loss = 1.327 + 0.149 + 0.008 = 1.483
10/26 04:53:59 PM | === Batch 122/200
10/26 04:53:59 PM | loss = 1.245 + 0.148 + 0.008 = 1.401
10/26 04:54:00 PM | === Batch 123/200
10/26 04:54:00 PM | loss = 1.174 + 0.147 + 0.008 = 1.328
10/26 04:54:00 PM | === Batch 124/200
10/26 04:54:01 PM | loss = 1.219 + 0.145 + 0.007 = 1.372
10/26 04:54:01 PM | === Batch 125/200
10/26 04:54:01 PM | loss = 1.200 + 0.145 + 0.007 = 1.352
10/26 04:54:02 PM | === Batch 126/200
10/26 04:54:02 PM | loss = 1.325 + 0.143 + 0.007 = 1.475
10/26 04:54:02 PM | === Batch 127/200
10/26 04:54:03 PM | loss = 1.088 + 0.141 + 0.007 = 1.236
10/26 04:54:03 PM | === Batch 128/200
10/26 04:54:03 PM | loss = 1.292 + 0.140 + 0.007 = 1.439
10/26 04:54:04 PM | === Batch 129/200
10/26 04:54:04 PM | loss = 1.180 + 0.139 + 0.007 = 1.325
10/26 04:54:04 PM | === Batch 130/200
10/26 04:54:04 PM | loss = 1.218 + 0.136 + 0.007 = 1.361
10/26 04:54:05 PM | === Batch 131/200
10/26 04:54:05 PM | loss = 1.194 + 0.134 + 0.007 = 1.334
10/26 04:54:05 PM | === Batch 132/200
10/26 04:54:06 PM | loss = 1.205 + 0.131 + 0.007 = 1.343
10/26 04:54:06 PM | === Batch 133/200
10/26 04:54:06 PM | loss = 1.326 + 0.128 + 0.007 = 1.460
10/26 04:54:06 PM | === Batch 134/200
10/26 04:54:07 PM | loss = 1.315 + 0.124 + 0.007 = 1.445
10/26 04:54:07 PM | === Batch 135/200
10/26 04:54:07 PM | loss = 1.107 + 0.119 + 0.007 = 1.232
10/26 04:54:07 PM | === Batch 136/200
10/26 04:54:08 PM | loss = 1.261 + 0.114 + 0.007 = 1.382
10/26 04:54:08 PM | === Batch 137/200
10/26 04:54:08 PM | loss = 1.338 + 0.112 + 0.007 = 1.457
10/26 04:54:08 PM | === Batch 138/200
10/26 04:54:09 PM | loss = 1.283 + 0.109 + 0.007 = 1.399
10/26 04:54:09 PM | === Batch 139/200
10/26 04:54:09 PM | loss = 1.369 + 0.107 + 0.007 = 1.482
10/26 04:54:10 PM | === Batch 140/200
10/26 04:54:10 PM | loss = 1.052 + 0.105 + 0.007 = 1.163
10/26 04:54:10 PM | === Batch 141/200
10/26 04:54:10 PM | loss = 1.244 + 0.103 + 0.007 = 1.354
10/26 04:54:11 PM | === Batch 142/200
10/26 04:54:11 PM | loss = 1.277 + 0.101 + 0.007 = 1.385
10/26 04:54:11 PM | === Batch 143/200
10/26 04:54:11 PM | loss = 1.389 + 0.097 + 0.007 = 1.493
10/26 04:54:12 PM | === Batch 144/200
10/26 04:54:12 PM | loss = 1.231 + 0.093 + 0.006 = 1.330
10/26 04:54:12 PM | === Batch 145/200
10/26 04:54:13 PM | loss = 1.097 + 0.089 + 0.006 = 1.193
10/26 04:54:13 PM | === Batch 146/200
10/26 04:54:13 PM | loss = 1.284 + 0.085 + 0.006 = 1.375
10/26 04:54:14 PM | === Batch 147/200
10/26 04:54:14 PM | loss = 1.354 + 0.079 + 0.006 = 1.440
10/26 04:54:15 PM | === Batch 148/200
10/26 04:54:15 PM | loss = 1.096 + 0.074 + 0.006 = 1.177
10/26 04:54:15 PM | === Batch 149/200
10/26 04:54:16 PM | loss = 1.345 + 0.071 + 0.006 = 1.421
10/26 04:54:16 PM | === Batch 150/200
10/26 04:54:16 PM | loss = 1.168 + 0.068 + 0.006 = 1.242
10/26 04:54:17 PM | === Batch 151/200
10/26 04:54:17 PM | loss = 1.424 + 0.066 + 0.006 = 1.496
10/26 04:54:17 PM | === Batch 152/200
10/26 04:54:17 PM | loss = 1.038 + 0.063 + 0.006 = 1.107
10/26 04:54:18 PM | === Batch 153/200
10/26 04:54:18 PM | loss = 1.234 + 0.058 + 0.006 = 1.298
10/26 04:54:18 PM | === Batch 154/200
10/26 04:54:19 PM | loss = 1.178 + 0.054 + 0.006 = 1.238
10/26 04:54:19 PM | === Batch 155/200
10/26 04:54:19 PM | loss = 1.291 + 0.051 + 0.006 = 1.349
10/26 04:54:19 PM | === Batch 156/200
10/26 04:54:20 PM | loss = 1.217 + 0.047 + 0.006 = 1.270
10/26 04:54:20 PM | === Batch 157/200
10/26 04:54:20 PM | loss = 1.244 + 0.045 + 0.006 = 1.295
10/26 04:54:20 PM | === Batch 158/200
10/26 04:54:21 PM | loss = 1.237 + 0.040 + 0.006 = 1.284
10/26 04:54:21 PM | === Batch 159/200
10/26 04:54:21 PM | loss = 1.192 + 0.036 + 0.006 = 1.234
10/26 04:54:21 PM | === Batch 160/200
10/26 04:54:22 PM | loss = 1.222 + 0.031 + 0.006 = 1.259
10/26 04:54:22 PM | === Batch 161/200
10/26 04:54:22 PM | loss = 1.368 + 0.026 + 0.006 = 1.400
10/26 04:54:23 PM | === Batch 162/200
10/26 04:54:23 PM | loss = 1.104 + 0.022 + 0.006 = 1.132
10/26 04:54:23 PM | === Batch 163/200
10/26 04:54:23 PM | loss = 1.319 + 0.018 + 0.006 = 1.343
10/26 04:54:24 PM | === Batch 164/200
10/26 04:54:24 PM | loss = 1.263 + 0.014 + 0.006 = 1.283
10/26 04:54:24 PM | === Batch 165/200
10/26 04:54:25 PM | loss = 1.063 + 0.011 + 0.006 = 1.080
10/26 04:54:25 PM | === Batch 166/200
10/26 04:54:25 PM | loss = 1.251 + 0.009 + 0.006 = 1.267
10/26 04:54:25 PM | === Batch 167/200
10/26 04:54:26 PM | loss = 1.406 + 0.007 + 0.006 = 1.419
10/26 04:54:26 PM | === Batch 168/200
10/26 04:54:26 PM | loss = 1.252 + 0.005 + 0.006 = 1.264
10/26 04:54:27 PM | === Batch 169/200
10/26 04:54:27 PM | loss = 1.392 + 0.004 + 0.006 = 1.402
10/26 04:54:28 PM | === Batch 170/200
10/26 04:54:28 PM | loss = 1.211 + 0.002 + 0.006 = 1.219
10/26 04:54:28 PM | === Batch 171/200
10/26 04:54:29 PM | loss = 1.400 + 0.000 + 0.006 = 1.407
10/26 04:54:29 PM | === Batch 172/200
10/26 04:54:29 PM | loss = 1.222 + 0.008 + 0.006 = 1.236
10/26 04:54:30 PM | === Batch 173/200
10/26 04:54:30 PM | loss = 1.177 + 0.003 + 0.006 = 1.186
10/26 04:54:30 PM | === Batch 174/200
10/26 04:54:31 PM | loss = 1.361 + 0.009 + 0.006 = 1.375
10/26 04:54:31 PM | === Batch 175/200
10/26 04:54:31 PM | loss = 1.211 + 0.012 + 0.006 = 1.228
10/26 04:54:32 PM | === Batch 176/200
10/26 04:54:32 PM | loss = 1.046 + 0.014 + 0.005 = 1.066
10/26 04:54:32 PM | === Batch 177/200
10/26 04:54:33 PM | loss = 1.250 + 0.016 + 0.005 = 1.271
10/26 04:54:33 PM | === Batch 178/200
10/26 04:54:33 PM | loss = 1.125 + 0.017 + 0.005 = 1.148
10/26 04:54:34 PM | === Batch 179/200
10/26 04:54:34 PM | loss = 1.183 + 0.020 + 0.005 = 1.208
10/26 04:54:34 PM | === Batch 180/200
10/26 04:54:34 PM | loss = 1.216 + 0.022 + 0.005 = 1.243
10/26 04:54:35 PM | === Batch 181/200
10/26 04:54:35 PM | loss = 1.300 + 0.023 + 0.005 = 1.328
10/26 04:54:35 PM | === Batch 182/200
10/26 04:54:36 PM | loss = 1.178 + 0.023 + 0.005 = 1.207
10/26 04:54:36 PM | === Batch 183/200
10/26 04:54:36 PM | loss = 1.162 + 0.024 + 0.005 = 1.192
10/26 04:54:37 PM | === Batch 184/200
10/26 04:54:37 PM | loss = 1.273 + 0.024 + 0.005 = 1.303
10/26 04:54:37 PM | === Batch 185/200
10/26 04:54:37 PM | loss = 1.243 + 0.023 + 0.005 = 1.271
10/26 04:54:38 PM | === Batch 186/200
10/26 04:54:38 PM | loss = 1.308 + 0.020 + 0.005 = 1.333
10/26 04:54:38 PM | === Batch 187/200
10/26 04:54:38 PM | loss = 1.310 + 0.016 + 0.006 = 1.332
10/26 04:54:39 PM | === Batch 188/200
10/26 04:54:39 PM | loss = 1.109 + 0.012 + 0.006 = 1.126
10/26 04:54:39 PM | === Batch 189/200
10/26 04:54:40 PM | loss = 1.233 + 0.006 + 0.006 = 1.244
10/26 04:54:40 PM | === Batch 190/200
10/26 04:54:41 PM | loss = 1.366 + 0.002 + 0.006 = 1.375
10/26 04:54:41 PM | === Batch 191/200
10/26 04:54:41 PM | loss = 1.174 + 0.000 + 0.006 = 1.180
10/26 04:54:42 PM | === Batch 192/200
10/26 04:54:42 PM | loss = 1.290 + 0.007 + 0.006 = 1.303
10/26 04:54:42 PM | === Batch 193/200
10/26 04:54:43 PM | loss = 1.232 + 0.014 + 0.006 = 1.251
10/26 04:54:43 PM | === Batch 194/200
10/26 04:54:43 PM | loss = 1.132 + 0.019 + 0.006 = 1.157
10/26 04:54:43 PM | === Batch 195/200
10/26 04:54:44 PM | loss = 1.252 + 0.023 + 0.006 = 1.280
10/26 04:54:44 PM | === Batch 196/200
10/26 04:54:44 PM | loss = 1.010 + 0.026 + 0.006 = 1.042
10/26 04:54:45 PM | === Batch 197/200
10/26 04:54:45 PM | loss = 1.227 + 0.029 + 0.006 = 1.262
10/26 04:54:45 PM | === Batch 198/200
10/26 04:54:45 PM | loss = 1.056 + 0.032 + 0.006 = 1.093
10/26 04:54:46 PM | === Batch 199/200
10/26 04:54:46 PM | loss = 1.157 + 0.034 + 0.006 = 1.198
10/26 04:54:46 PM | === Batch 200/200
10/26 04:54:47 PM | loss = 1.361 + 0.036 + 0.006 = 1.403
10/26 04:54:47 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.01  0.00  0.00  0.37  0.00  0.00  0.00  1.00  0.00  0.98 ]
[ 1.00  0.00  1.00  0.00  0.98  0.96  1.00  0.00  0.99  1.00  1.00  0.01 ]
[ 1.00  1.00  0.00  1.00  0.00  1.00  0.00  0.99  0.00  0.00  0.00  0.97 ]
[ 0.00  0.00  1.00  0.99  0.99  1.00  0.97  0.91  0.60  0.00  1.00  0.97 ]
[ 0.99  0.00  1.00  0.00  0.02  1.00  0.00  0.00  1.00  0.98  0.99  0.26 ]
[ 1.00  1.00  0.00  0.00  0.00  0.99  0.00  0.00  0.01  0.01  0.00  1.00 ]
[ 0.99  0.03  0.00  0.99  1.00  0.01  0.00  0.01  1.00  1.00  1.00  1.00 ]
[ 0.00  0.01  0.00  0.01  0.00  1.00  0.97  0.00  0.99  0.00  0.00  0.00 ]
[ 1.00  1.00  0.00  0.44  1.00  0.00  0.00  0.00  0.99  0.00  1.00  1.00 ]
[ 0.00  0.00  0.00  0.00  1.00  0.99  0.00  0.00  1.00  0.00  0.01  0.00 ]
[ 0.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.98  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00 ]

10/26 04:54:47 PM | ===
Best loss was 1.04 at iteration 196

10/26 04:54:47 PM | Looking for optimal threshold...
10/26 04:54:47 PM | Testing threshold 0.5
10/26 04:54:47 PM | Distance to target: 1,175,432
10/26 04:54:47 PM | Testing threshold 0.75
10/26 04:54:47 PM | Distance to target: 1,503,000
10/26 04:54:47 PM | Testing threshold 0.625
10/26 04:54:47 PM | Distance to target: 173,928
10/26 04:54:47 PM | Testing threshold 0.6875
10/26 04:54:47 PM | Distance to target: 561,048
10/26 04:54:47 PM | Testing threshold 0.65625
10/26 04:54:47 PM | Distance to target: 123,672
10/26 04:54:47 PM | Testing threshold 0.640625
10/26 04:54:47 PM | Distance to target: 74,728
10/26 04:54:47 PM | Testing threshold 0.6484375
10/26 04:54:47 PM | Distance to target: 24,472
10/26 04:54:49 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  1.00  0.00  1.00 ]
[ 1.00  0.00  1.00  0.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00  0.00 ]
[ 1.00  1.00  0.00  1.00  0.00  1.00  0.00  1.00  0.00  0.00  0.00  1.00 ]
[ 0.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00 ]
[ 1.00  0.00  1.00  0.00  0.00  1.00  0.00  0.00  1.00  1.00  1.00  0.00 ]
[ 1.00  1.00  0.00  0.00  0.00  1.00  0.00  0.00  0.00  0.00  0.00  1.00 ]
[ 1.00  0.00  0.00  1.00  1.00  0.00  0.00  0.00  1.00  1.00  1.00  1.00 ]
[ 0.00  0.00  0.00  0.00  0.00  1.00  1.00  0.00  1.00  0.00  0.00  0.00 ]
[ 1.00  1.00  0.00  0.00  1.00  0.00  0.00  0.00  1.00  0.00  1.00  1.00 ]
[ 0.00  0.00  0.00  0.00  1.00  1.00  0.00  0.00  1.00  0.00  0.00  0.00 ]
[ 0.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00 ]

10/26 04:54:49 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(20, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(42, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(56, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(76, 117, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(117, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(117, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(96, 130, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(130, 188, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(188, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(188, 161, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(161, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(161, 242, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(242, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(242, 356, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(356, 424, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(424, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(424, 499, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(499, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=499, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/26 04:54:50 PM | ----------------------------------------
10/26 04:54:50 PM | Performances input model:
10/26 04:54:50 PM |  - flops: 314,294,272
10/26 04:54:50 PM |  - params: 14,991,946
10/26 04:54:50 PM |  - accuracy: 93.95999908447266
10/26 04:54:50 PM | ----------------------------------------
10/26 04:54:50 PM | Performances pseudo-pruned model:
10/26 04:54:50 PM |  - flops: 314,294,272
10/26 04:54:50 PM |  - params: 14,991,946
10/26 04:54:50 PM |  - accuracy: 71.23999786376953
10/26 04:54:50 PM | ----------------------------------------
10/26 04:54:50 PM | Performances pruned model:
10/26 04:54:50 PM |  - flops: 72,602,852
10/26 04:54:50 PM |  - params: 5,511,461
10/26 04:54:50 PM |  - accuracy: 71.23999786376953
