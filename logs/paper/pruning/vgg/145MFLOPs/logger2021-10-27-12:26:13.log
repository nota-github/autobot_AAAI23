10/27 12:26:13 PM | args = Namespace(output_dir='result/test/test3', loaded_model_path='./checkpoints/cifar10/vgg_16_bn.pt', resume=False, test_only=False, mode='prune', batch_size=64, nb_batches=200, Mflops_target=145.61, lr=0.5, momentum=0.9, beta=6.0, gamma=0.2, gpu='3', num_workers=4, dataset='cifar10', arch='vgg_16_bn', save_plot=False, seed=1, lr_finetuning=0.03, epoch_finetuning=200, wd=0.002, data_dir='./data/cifar10/', print_freq=200, num_classes=10, device_ids=[3], device=device(type='cuda', index=0), name_base='')
10/27 12:26:23 PM | ----------------------------------------
10/27 12:26:23 PM | ==> Building model...
10/27 12:26:23 PM | ----------------------------------------
10/27 12:26:23 PM | ==> Loading weights into the model...
10/27 12:26:23 PM | ----------------------------------------
10/27 12:26:23 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=512, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/27 12:26:25 PM | Save modules info...
10/27 12:26:25 PM | FLOPS pruning ratio is 0.54
10/27 12:26:25 PM | Pruning with information flow
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: None
10/27 12:26:25 PM | cout: [0]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [0]
10/27 12:26:25 PM | cout: [0]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [0]
10/27 12:26:25 PM | cout: [0]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [0]
10/27 12:26:25 PM | cout: [1]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [1]
10/27 12:26:25 PM | cout: [1]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [1]
10/27 12:26:25 PM | cout: [1]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:26:25 PM | cin: [1]
10/27 12:26:25 PM | cout: [1]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [1]
10/27 12:26:25 PM | cout: [2]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [2]
10/27 12:26:25 PM | cout: [2]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [2]
10/27 12:26:25 PM | cout: [2]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [2]
10/27 12:26:25 PM | cout: [3]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [3]
10/27 12:26:25 PM | cout: [3]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [3]
10/27 12:26:25 PM | cout: [3]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:26:25 PM | cin: [3]
10/27 12:26:25 PM | cout: [3]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [3]
10/27 12:26:25 PM | cout: [4]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [4]
10/27 12:26:25 PM | cout: [4]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [4]
10/27 12:26:25 PM | cout: [4]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [4]
10/27 12:26:25 PM | cout: [5]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [5]
10/27 12:26:25 PM | cout: [5]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [5]
10/27 12:26:25 PM | cout: [5]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [5]
10/27 12:26:25 PM | cout: [6]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [6]
10/27 12:26:25 PM | cout: [6]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [6]
10/27 12:26:25 PM | cout: [6]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:26:25 PM | cin: [6]
10/27 12:26:25 PM | cout: [6]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [6]
10/27 12:26:25 PM | cout: [7]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [7]
10/27 12:26:25 PM | cout: [7]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [7]
10/27 12:26:25 PM | cout: [7]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [7]
10/27 12:26:25 PM | cout: [8]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [8]
10/27 12:26:25 PM | cout: [8]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [8]
10/27 12:26:25 PM | cout: [8]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [8]
10/27 12:26:25 PM | cout: [9]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [9]
10/27 12:26:25 PM | cout: [9]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [9]
10/27 12:26:25 PM | cout: [9]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:26:25 PM | cin: [9]
10/27 12:26:25 PM | cout: [9]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [9]
10/27 12:26:25 PM | cout: [10]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [10]
10/27 12:26:25 PM | cout: [10]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [10]
10/27 12:26:25 PM | cout: [10]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [10]
10/27 12:26:25 PM | cout: [11]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [11]
10/27 12:26:25 PM | cout: [11]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [11]
10/27 12:26:25 PM | cout: [11]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:26:25 PM | cin: [11]
10/27 12:26:25 PM | cout: [12]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:26:25 PM | cin: [12]
10/27 12:26:25 PM | cout: [12]
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: [12]
10/27 12:26:25 PM | cout: [12]
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Linear(in_features=512, out_features=512, bias=True)
10/27 12:26:25 PM | cin: [12]
10/27 12:26:25 PM | cout: None
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: ReLU(inplace=True)
10/27 12:26:25 PM | cin: None
10/27 12:26:25 PM | cout: None
10/27 12:26:25 PM | active
10/27 12:26:25 PM | -----
10/27 12:26:25 PM | module: Linear(in_features=512, out_features=10, bias=True)
10/27 12:26:25 PM | cin: None
10/27 12:26:25 PM | cout: None
10/27 12:26:25 PM | inactive
10/27 12:26:25 PM | Used masks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10/27 12:26:25 PM | 13 unique masks in total
10/27 12:26:25 PM | === Batch 1/200
10/27 12:26:26 PM | loss = 0.494 + 3.886 + 0.040 = 4.420
10/27 12:26:26 PM | attribution_score[0:12]: 
[ 0.94  0.94  0.94  0.85  0.85  0.85  0.85  0.85  0.85  0.94  0.85  0.94 ]
[ 0.85  0.85  0.94  0.85  0.94  0.85  0.85  0.85  0.85  0.94  0.94  0.85 ]
[ 0.94  0.94  0.85  0.94  0.85  0.94  0.94  0.94  0.85  0.85  0.85  0.94 ]
[ 0.85  0.85  0.94  0.94  0.85  0.85  0.94  0.85  0.94  0.94  0.94  0.94 ]
[ 0.85  0.94  0.85  0.85  0.85  0.94  0.85  0.85  0.94  0.94  0.85  0.85 ]
[ 0.85  0.94  0.94  0.85  0.85  0.85  0.85  0.94  0.85  0.85  0.85  0.85 ]
[ 0.94  0.94  0.85  0.85  0.94  0.85  0.85  0.85  0.94  0.94  0.94  0.85 ]
[ 0.85  0.85  0.85  0.94  0.85  0.85  0.94  0.85  0.94  0.94  0.85  0.85 ]
[ 0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.94 ]
[ 0.85  0.85  0.85  0.85  0.94  0.85  0.85  0.85  0.85  0.85  0.85  0.85 ]
[ 0.85  0.85  0.94  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.94  0.85 ]
[ 0.94  0.94  0.94  0.85  0.94  0.85  0.85  0.94  0.94  0.94  0.94  0.85 ]
[ 0.94  0.94  0.85  0.85  0.94  0.94  0.85  0.85  0.94  0.94  0.85  0.85 ]

10/27 12:26:26 PM | === Batch 2/200
10/27 12:26:26 PM | loss = 0.143 + 3.525 + 0.047 = 3.715
10/27 12:26:27 PM | attribution_score[0:12]: 
[ 0.96  0.95  0.91  0.77  0.77  0.79  0.77  0.77  0.77  0.92  0.77  0.95 ]
[ 0.82  0.77  0.92  0.77  0.95  0.80  0.79  0.77  0.79  0.95  0.95  0.78 ]
[ 0.96  0.95  0.77  0.96  0.78  0.95  0.94  0.95  0.79  0.77  0.77  0.95 ]
[ 0.77  0.78  0.95  0.95  0.78  0.82  0.94  0.84  0.95  0.96  0.95  0.96 ]
[ 0.77  0.94  0.78  0.77  0.77  0.95  0.77  0.77  0.95  0.92  0.86  0.79 ]
[ 0.77  0.95  0.92  0.77  0.77  0.77  0.78  0.95  0.78  0.77  0.78  0.77 ]
[ 0.91  0.94  0.77  0.77  0.95  0.77  0.78  0.79  0.95  0.91  0.96  0.77 ]
[ 0.77  0.77  0.77  0.95  0.77  0.77  0.94  0.77  0.95  0.93  0.77  0.77 ]
[ 0.77  0.77  0.77  0.77  0.77  0.77  0.77  0.77  0.77  0.77  0.77  0.92 ]
[ 0.77  0.77  0.77  0.77  0.93  0.77  0.77  0.77  0.77  0.77  0.77  0.77 ]
[ 0.77  0.84  0.95  0.78  0.78  0.78  0.82  0.77  0.83  0.78  0.96  0.82 ]
[ 0.95  0.96  0.92  0.84  0.96  0.78  0.77  0.96  0.93  0.96  0.95  0.84 ]
[ 0.96  0.96  0.89  0.80  0.96  0.96  0.79  0.86  0.95  0.96  0.84  0.78 ]

10/27 12:26:27 PM | === Batch 3/200
10/27 12:26:27 PM | loss = 0.226 + 2.823 + 0.060 = 3.109
10/27 12:26:27 PM | attribution_score[0:12]: 
[ 0.97  0.96  0.88  0.67  0.67  0.73  0.67  0.67  0.67  0.89  0.68  0.96 ]
[ 0.77  0.68  0.93  0.67  0.95  0.78  0.72  0.68  0.77  0.96  0.96  0.69 ]
[ 0.96  0.96  0.68  0.97  0.69  0.95  0.93  0.96  0.71  0.68  0.68  0.96 ]
[ 0.67  0.75  0.95  0.96  0.69  0.86  0.94  0.82  0.96  0.96  0.96  0.96 ]
[ 0.68  0.95  0.80  0.67  0.75  0.95  0.72  0.68  0.96  0.91  0.82  0.71 ]
[ 0.74  0.95  0.89  0.67  0.68  0.68  0.68  0.95  0.69  0.76  0.69  0.68 ]
[ 0.88  0.93  0.70  0.69  0.95  0.69  0.68  0.70  0.96  0.91  0.97  0.70 ]
[ 0.67  0.68  0.67  0.96  0.67  0.70  0.92  0.67  0.96  0.91  0.67  0.67 ]
[ 0.68  0.69  0.67  0.68  0.68  0.67  0.67  0.67  0.72  0.67  0.68  0.93 ]
[ 0.70  0.67  0.67  0.67  0.91  0.68  0.67  0.67  0.68  0.67  0.67  0.67 ]
[ 0.67  0.87  0.95  0.69  0.71  0.71  0.84  0.73  0.85  0.76  0.97  0.83 ]
[ 0.95  0.97  0.88  0.87  0.96  0.69  0.80  0.97  0.91  0.96  0.95  0.86 ]
[ 0.97  0.97  0.92  0.81  0.96  0.98  0.82  0.90  0.95  0.97  0.87  0.76 ]

10/27 12:26:27 PM | === Batch 4/200
10/27 12:26:27 PM | loss = 0.701 + 2.002 + 0.076 = 2.778
10/27 12:26:28 PM | attribution_score[0:12]: 
[ 0.98  0.96  0.82  0.55  0.55  0.72  0.57  0.55  0.55  0.85  0.57  0.96 ]
[ 0.70  0.57  0.92  0.55  0.95  0.74  0.63  0.58  0.76  0.96  0.97  0.59 ]
[ 0.97  0.97  0.56  0.97  0.58  0.95  0.91  0.96  0.61  0.57  0.58  0.97 ]
[ 0.59  0.69  0.96  0.96  0.68  0.89  0.94  0.77  0.96  0.95  0.97  0.96 ]
[ 0.58  0.95  0.82  0.57  0.76  0.96  0.68  0.59  0.97  0.89  0.84  0.61 ]
[ 0.76  0.96  0.85  0.58  0.57  0.62  0.57  0.95  0.58  0.75  0.58  0.56 ]
[ 0.82  0.92  0.67  0.64  0.95  0.61  0.59  0.60  0.97  0.93  0.98  0.62 ]
[ 0.55  0.59  0.56  0.97  0.56  0.63  0.88  0.55  0.96  0.87  0.55  0.63 ]
[ 0.58  0.58  0.56  0.58  0.59  0.55  0.56  0.57  0.72  0.55  0.65  0.95 ]
[ 0.66  0.57  0.56  0.56  0.88  0.65  0.55  0.55  0.65  0.56  0.55  0.56 ]
[ 0.55  0.91  0.95  0.59  0.67  0.61  0.88  0.77  0.88  0.80  0.98  0.87 ]
[ 0.95  0.98  0.85  0.91  0.96  0.66  0.84  0.98  0.88  0.97  0.95  0.90 ]
[ 0.97  0.98  0.95  0.84  0.97  0.99  0.86  0.93  0.96  0.97  0.91  0.77 ]

10/27 12:26:28 PM | === Batch 5/200
10/27 12:26:28 PM | loss = 0.673 + 1.230 + 0.091 = 1.993
10/27 12:26:28 PM | attribution_score[0:12]: 
[ 0.98  0.96  0.75  0.42  0.43  0.72  0.47  0.43  0.43  0.79  0.45  0.96 ]
[ 0.62  0.45  0.92  0.42  0.94  0.72  0.55  0.47  0.79  0.96  0.96  0.48 ]
[ 0.97  0.97  0.45  0.97  0.46  0.96  0.87  0.95  0.50  0.45  0.46  0.97 ]
[ 0.52  0.65  0.95  0.96  0.68  0.92  0.92  0.74  0.96  0.93  0.97  0.94 ]
[ 0.47  0.95  0.86  0.47  0.81  0.96  0.66  0.49  0.98  0.89  0.86  0.51 ]
[ 0.79  0.96  0.79  0.47  0.46  0.55  0.45  0.94  0.46  0.73  0.46  0.59 ]
[ 0.80  0.89  0.71  0.68  0.94  0.56  0.48  0.48  0.97  0.94  0.98  0.60 ]
[ 0.43  0.48  0.44  0.97  0.45  0.59  0.83  0.43  0.96  0.81  0.42  0.63 ]
[ 0.51  0.58  0.44  0.47  0.48  0.42  0.44  0.47  0.75  0.42  0.60  0.96 ]
[ 0.63  0.47  0.43  0.43  0.83  0.65  0.42  0.43  0.66  0.44  0.43  0.46 ]
[ 0.43  0.94  0.96  0.47  0.66  0.54  0.91  0.81  0.91  0.84  0.98  0.90 ]
[ 0.95  0.99  0.80  0.94  0.97  0.63  0.89  0.98  0.83  0.97  0.96  0.93 ]
[ 0.97  0.98  0.97  0.88  0.97  0.99  0.90  0.95  0.97  0.97  0.94  0.79 ]

10/27 12:26:28 PM | === Batch 6/200
10/27 12:26:28 PM | loss = 0.939 + 0.520 + 0.094 = 1.554
10/27 12:26:29 PM | attribution_score[0:12]: 
[ 0.98  0.97  0.66  0.31  0.32  0.70  0.36  0.32  0.31  0.72  0.34  0.95 ]
[ 0.59  0.33  0.92  0.31  0.93  0.72  0.49  0.36  0.81  0.96  0.96  0.46 ]
[ 0.97  0.97  0.34  0.98  0.35  0.96  0.82  0.94  0.39  0.34  0.35  0.97 ]
[ 0.46  0.62  0.96  0.96  0.69  0.93  0.91  0.72  0.96  0.90  0.97  0.93 ]
[ 0.42  0.95  0.86  0.38  0.85  0.96  0.66  0.39  0.98  0.87  0.88  0.41 ]
[ 0.81  0.97  0.70  0.36  0.34  0.56  0.43  0.93  0.36  0.73  0.35  0.57 ]
[ 0.81  0.86  0.75  0.72  0.93  0.54  0.38  0.36  0.98  0.96  0.99  0.57 ]
[ 0.31  0.38  0.33  0.98  0.34  0.56  0.76  0.32  0.96  0.74  0.32  0.59 ]
[ 0.48  0.53  0.32  0.37  0.37  0.31  0.32  0.38  0.79  0.34  0.52  0.97 ]
[ 0.61  0.39  0.32  0.31  0.77  0.67  0.31  0.31  0.71  0.32  0.32  0.36 ]
[ 0.33  0.95  0.96  0.35  0.66  0.58  0.94  0.86  0.93  0.85  0.99  0.93 ]
[ 0.95  0.99  0.76  0.95  0.96  0.64  0.92  0.99  0.77  0.97  0.96  0.95 ]
[ 0.98  0.99  0.98  0.91  0.97  0.99  0.93  0.96  0.97  0.96  0.95  0.79 ]

10/27 12:26:29 PM | === Batch 7/200
10/27 12:26:29 PM | loss = 1.177 + 0.130 + 0.083 = 1.390
10/27 12:26:29 PM | attribution_score[0:12]: 
[ 0.99  0.97  0.64  0.25  0.29  0.72  0.35  0.27  0.25  0.74  0.30  0.96 ]
[ 0.62  0.26  0.93  0.26  0.92  0.73  0.51  0.30  0.83  0.97  0.96  0.49 ]
[ 0.97  0.97  0.31  0.98  0.33  0.97  0.82  0.95  0.37  0.29  0.31  0.97 ]
[ 0.40  0.64  0.96  0.96  0.68  0.95  0.91  0.76  0.96  0.91  0.98  0.94 ]
[ 0.44  0.95  0.88  0.34  0.88  0.97  0.70  0.33  0.99  0.85  0.91  0.37 ]
[ 0.83  0.97  0.68  0.28  0.28  0.62  0.46  0.93  0.31  0.76  0.30  0.61 ]
[ 0.85  0.86  0.79  0.75  0.92  0.55  0.34  0.33  0.98  0.96  0.99  0.60 ]
[ 0.25  0.36  0.28  0.98  0.28  0.59  0.67  0.27  0.95  0.73  0.30  0.59 ]
[ 0.51  0.54  0.26  0.36  0.36  0.26  0.28  0.40  0.83  0.33  0.52  0.98 ]
[ 0.66  0.37  0.26  0.25  0.78  0.69  0.25  0.26  0.78  0.29  0.29  0.33 ]
[ 0.30  0.96  0.97  0.34  0.71  0.65  0.95  0.90  0.95  0.87  0.99  0.95 ]
[ 0.95  1.00  0.79  0.97  0.97  0.70  0.93  0.99  0.78  0.98  0.97  0.97 ]
[ 0.98  0.99  0.99  0.93  0.97  1.00  0.95  0.97  0.98  0.96  0.96  0.82 ]

10/27 12:26:29 PM | === Batch 8/200
10/27 12:26:29 PM | loss = 1.306 + 0.254 + 0.077 = 1.636
10/27 12:26:30 PM | attribution_score[0:12]: 
[ 0.99  0.97  0.66  0.22  0.29  0.76  0.36  0.25  0.23  0.78  0.29  0.96 ]
[ 0.68  0.24  0.94  0.25  0.93  0.75  0.57  0.27  0.84  0.98  0.96  0.55 ]
[ 0.97  0.97  0.31  0.98  0.33  0.98  0.84  0.96  0.39  0.28  0.30  0.97 ]
[ 0.36  0.69  0.96  0.96  0.66  0.96  0.93  0.80  0.96  0.93  0.98  0.95 ]
[ 0.49  0.96  0.91  0.34  0.90  0.97  0.75  0.34  0.99  0.85  0.93  0.37 ]
[ 0.85  0.97  0.71  0.23  0.24  0.68  0.52  0.94  0.30  0.79  0.29  0.67 ]
[ 0.88  0.87  0.83  0.77  0.93  0.58  0.33  0.28  0.98  0.97  0.99  0.67 ]
[ 0.22  0.38  0.25  0.98  0.24  0.65  0.65  0.25  0.95  0.75  0.30  0.62 ]
[ 0.57  0.59  0.24  0.37  0.39  0.24  0.25  0.44  0.83  0.32  0.55  0.98 ]
[ 0.70  0.36  0.24  0.23  0.82  0.68  0.24  0.24  0.83  0.30  0.33  0.34 ]
[ 0.32  0.97  0.98  0.33  0.72  0.73  0.96  0.93  0.97  0.88  0.99  0.96 ]
[ 0.96  1.00  0.84  0.97  0.97  0.77  0.94  0.99  0.82  0.98  0.98  0.98 ]
[ 0.98  0.99  0.99  0.94  0.98  1.00  0.96  0.98  0.99  0.96  0.97  0.84 ]

10/27 12:26:30 PM | === Batch 9/200
10/27 12:26:30 PM | loss = 1.008 + 0.123 + 0.073 = 1.203
10/27 12:26:30 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.71  0.21  0.31  0.79  0.39  0.25  0.22  0.82  0.31  0.97 ]
[ 0.74  0.23  0.95  0.25  0.93  0.75  0.64  0.26  0.85  0.98  0.96  0.63 ]
[ 0.98  0.97  0.33  0.98  0.36  0.98  0.88  0.97  0.43  0.29  0.30  0.98 ]
[ 0.36  0.74  0.97  0.97  0.66  0.97  0.95  0.83  0.96  0.94  0.98  0.96 ]
[ 0.56  0.97  0.93  0.35  0.91  0.98  0.80  0.38  0.99  0.88  0.95  0.40 ]
[ 0.87  0.97  0.75  0.22  0.23  0.75  0.58  0.95  0.32  0.81  0.30  0.72 ]
[ 0.91  0.89  0.87  0.81  0.94  0.63  0.34  0.24  0.98  0.98  1.00  0.72 ]
[ 0.21  0.43  0.25  0.99  0.23  0.72  0.65  0.24  0.95  0.79  0.34  0.66 ]
[ 0.64  0.64  0.24  0.41  0.44  0.25  0.25  0.51  0.82  0.32  0.61  0.99 ]
[ 0.74  0.38  0.25  0.22  0.86  0.69  0.25  0.24  0.87  0.33  0.37  0.38 ]
[ 0.36  0.97  0.98  0.34  0.73  0.80  0.96  0.95  0.97  0.88  1.00  0.96 ]
[ 0.97  1.00  0.88  0.98  0.98  0.82  0.95  0.99  0.86  0.99  0.98  0.98 ]
[ 0.99  0.99  0.99  0.95  0.98  1.00  0.97  0.98  0.99  0.97  0.97  0.88 ]

10/27 12:26:30 PM | === Batch 10/200
10/27 12:26:31 PM | loss = 0.628 + 0.146 + 0.070 = 0.844
10/27 12:26:31 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.74  0.20  0.32  0.81  0.42  0.23  0.20  0.85  0.31  0.98 ]
[ 0.79  0.23  0.96  0.24  0.94  0.75  0.70  0.24  0.85  0.98  0.97  0.70 ]
[ 0.98  0.97  0.33  0.98  0.37  0.98  0.89  0.97  0.46  0.28  0.28  0.98 ]
[ 0.35  0.77  0.97  0.97  0.67  0.97  0.96  0.83  0.97  0.95  0.99  0.97 ]
[ 0.59  0.97  0.94  0.33  0.91  0.98  0.84  0.38  0.99  0.90  0.96  0.42 ]
[ 0.89  0.97  0.79  0.19  0.22  0.79  0.62  0.95  0.33  0.81  0.30  0.75 ]
[ 0.93  0.91  0.89  0.82  0.95  0.65  0.34  0.20  0.98  0.98  1.00  0.75 ]
[ 0.19  0.47  0.24  0.99  0.21  0.75  0.62  0.23  0.95  0.82  0.38  0.70 ]
[ 0.68  0.68  0.23  0.44  0.48  0.24  0.24  0.57  0.79  0.30  0.67  0.99 ]
[ 0.75  0.37  0.24  0.20  0.89  0.66  0.25  0.23  0.90  0.34  0.42  0.41 ]
[ 0.38  0.98  0.99  0.34  0.74  0.84  0.97  0.96  0.98  0.88  1.00  0.96 ]
[ 0.98  1.00  0.90  0.98  0.98  0.85  0.95  1.00  0.88  0.99  0.99  0.99 ]
[ 0.99  0.99  0.99  0.95  0.99  1.00  0.98  0.98  0.99  0.97  0.98  0.90 ]

10/27 12:26:31 PM | === Batch 11/200
10/27 12:26:31 PM | loss = 0.820 + 0.280 + 0.068 = 1.168
10/27 12:26:32 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.77  0.17  0.32  0.81  0.43  0.20  0.18  0.87  0.30  0.98 ]
[ 0.84  0.23  0.97  0.22  0.94  0.76  0.75  0.21  0.86  0.99  0.97  0.75 ]
[ 0.98  0.97  0.31  0.99  0.37  0.99  0.90  0.98  0.46  0.27  0.25  0.98 ]
[ 0.35  0.78  0.98  0.97  0.69  0.97  0.97  0.83  0.97  0.95  0.99  0.98 ]
[ 0.61  0.98  0.95  0.31  0.91  0.98  0.86  0.38  0.99  0.91  0.97  0.45 ]
[ 0.90  0.97  0.81  0.16  0.19  0.81  0.63  0.96  0.35  0.80  0.29  0.76 ]
[ 0.94  0.92  0.89  0.83  0.96  0.66  0.34  0.17  0.98  0.99  1.00  0.77 ]
[ 0.17  0.52  0.25  0.99  0.19  0.78  0.59  0.22  0.94  0.84  0.41  0.72 ]
[ 0.70  0.72  0.21  0.45  0.52  0.22  0.22  0.60  0.75  0.27  0.69  0.99 ]
[ 0.75  0.33  0.22  0.18  0.91  0.62  0.25  0.21  0.92  0.35  0.46  0.41 ]
[ 0.39  0.98  0.99  0.34  0.73  0.88  0.97  0.97  0.98  0.88  1.00  0.97 ]
[ 0.98  1.00  0.92  0.98  0.99  0.87  0.95  1.00  0.90  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.96  0.99  1.00  0.98  0.99  0.99  0.98  0.98  0.90 ]

10/27 12:26:32 PM | === Batch 12/200
10/27 12:26:32 PM | loss = 0.770 + 0.320 + 0.065 = 1.154
10/27 12:26:32 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.79  0.15  0.31  0.80  0.44  0.17  0.15  0.88  0.28  0.98 ]
[ 0.87  0.23  0.97  0.20  0.94  0.78  0.79  0.19  0.87  0.99  0.96  0.77 ]
[ 0.98  0.97  0.29  0.99  0.35  0.99  0.91  0.98  0.43  0.25  0.22  0.98 ]
[ 0.34  0.79  0.98  0.97  0.69  0.98  0.97  0.83  0.97  0.95  0.99  0.98 ]
[ 0.62  0.98  0.95  0.29  0.91  0.98  0.89  0.37  0.99  0.93  0.97  0.46 ]
[ 0.91  0.97  0.81  0.14  0.17  0.81  0.62  0.96  0.37  0.81  0.28  0.75 ]
[ 0.94  0.93  0.89  0.83  0.96  0.65  0.34  0.16  0.99  0.99  1.00  0.77 ]
[ 0.15  0.54  0.26  0.99  0.16  0.76  0.55  0.20  0.94  0.85  0.44  0.75 ]
[ 0.69  0.76  0.19  0.46  0.54  0.19  0.20  0.61  0.71  0.22  0.72  0.99 ]
[ 0.73  0.29  0.20  0.16  0.92  0.56  0.23  0.18  0.93  0.33  0.45  0.41 ]
[ 0.41  0.98  0.99  0.37  0.76  0.90  0.97  0.98  0.99  0.88  1.00  0.97 ]
[ 0.99  1.00  0.93  0.99  0.99  0.88  0.96  1.00  0.91  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.96  0.99  1.00  0.98  0.99  0.99  0.98  0.98  0.89 ]

10/27 12:26:32 PM | === Batch 13/200
10/27 12:26:32 PM | loss = 0.672 + 0.296 + 0.062 = 1.030
10/27 12:26:33 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.81  0.12  0.30  0.80  0.43  0.14  0.13  0.89  0.25  0.98 ]
[ 0.89  0.25  0.97  0.17  0.93  0.81  0.82  0.17  0.89  0.99  0.96  0.78 ]
[ 0.98  0.97  0.28  0.99  0.31  0.99  0.91  0.98  0.40  0.22  0.20  0.98 ]
[ 0.32  0.78  0.98  0.97  0.68  0.98  0.97  0.84  0.97  0.95  0.99  0.98 ]
[ 0.64  0.98  0.95  0.28  0.91  0.98  0.92  0.36  0.99  0.94  0.97  0.46 ]
[ 0.91  0.96  0.82  0.13  0.14  0.81  0.62  0.96  0.38  0.83  0.26  0.73 ]
[ 0.94  0.94  0.88  0.83  0.96  0.65  0.33  0.16  0.99  0.99  1.00  0.76 ]
[ 0.13  0.52  0.26  0.99  0.14  0.73  0.52  0.17  0.93  0.85  0.44  0.77 ]
[ 0.67  0.79  0.17  0.47  0.55  0.17  0.19  0.61  0.68  0.17  0.75  1.00 ]
[ 0.71  0.23  0.18  0.14  0.92  0.50  0.21  0.16  0.94  0.29  0.41  0.39 ]
[ 0.41  0.98  0.99  0.41  0.79  0.91  0.97  0.98  0.99  0.89  1.00  0.97 ]
[ 0.99  1.00  0.93  0.99  0.99  0.89  0.96  1.00  0.91  0.99  0.99  0.99 ]
[ 0.99  1.00  1.00  0.96  0.99  1.00  0.99  0.99  1.00  0.98  0.98  0.87 ]

10/27 12:26:33 PM | === Batch 14/200
10/27 12:26:33 PM | loss = 0.803 + 0.225 + 0.059 = 1.087
10/27 12:26:33 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.82  0.10  0.28  0.81  0.42  0.12  0.10  0.89  0.22  0.99 ]
[ 0.90  0.28  0.97  0.15  0.92  0.84  0.84  0.16  0.91  0.99  0.96  0.77 ]
[ 0.98  0.97  0.28  0.99  0.28  0.99  0.91  0.98  0.36  0.19  0.18  0.98 ]
[ 0.31  0.76  0.98  0.97  0.69  0.98  0.98  0.86  0.97  0.95  0.99  0.98 ]
[ 0.67  0.98  0.94  0.28  0.91  0.98  0.94  0.32  0.99  0.95  0.98  0.45 ]
[ 0.92  0.96  0.80  0.14  0.12  0.80  0.62  0.95  0.37  0.85  0.24  0.69 ]
[ 0.94  0.95  0.87  0.82  0.96  0.65  0.31  0.16  0.99  0.99  1.00  0.76 ]
[ 0.11  0.49  0.27  0.99  0.12  0.68  0.49  0.14  0.92  0.84  0.44  0.79 ]
[ 0.63  0.81  0.14  0.49  0.54  0.14  0.18  0.61  0.66  0.13  0.79  1.00 ]
[ 0.70  0.19  0.15  0.12  0.92  0.44  0.18  0.13  0.94  0.25  0.36  0.37 ]
[ 0.41  0.98  0.99  0.46  0.83  0.92  0.98  0.98  0.99  0.91  1.00  0.97 ]
[ 0.99  1.00  0.93  0.99  0.99  0.89  0.96  1.00  0.91  0.99  0.99  0.99 ]
[ 0.99  1.00  1.00  0.96  0.99  1.00  0.99  0.99  1.00  0.98  0.98  0.84 ]

10/27 12:26:33 PM | === Batch 15/200
10/27 12:26:34 PM | loss = 0.613 + 0.128 + 0.057 = 0.798
10/27 12:26:34 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.82  0.09  0.25  0.82  0.40  0.10  0.08  0.90  0.19  0.99 ]
[ 0.91  0.29  0.97  0.13  0.90  0.87  0.84  0.14  0.92  0.99  0.96  0.77 ]
[ 0.98  0.97  0.27  0.99  0.25  0.99  0.90  0.99  0.32  0.16  0.15  0.98 ]
[ 0.29  0.73  0.98  0.97  0.70  0.98  0.98  0.88  0.97  0.94  0.99  0.98 ]
[ 0.68  0.98  0.94  0.29  0.92  0.98  0.95  0.27  0.99  0.96  0.98  0.44 ]
[ 0.92  0.95  0.78  0.15  0.10  0.79  0.62  0.94  0.35  0.87  0.22  0.65 ]
[ 0.93  0.95  0.84  0.80  0.96  0.63  0.29  0.17  0.99  0.99  1.00  0.75 ]
[ 0.09  0.48  0.25  0.99  0.10  0.61  0.44  0.12  0.92  0.81  0.43  0.79 ]
[ 0.57  0.83  0.12  0.52  0.51  0.12  0.17  0.61  0.64  0.10  0.82  1.00 ]
[ 0.68  0.15  0.13  0.10  0.92  0.39  0.15  0.11  0.95  0.21  0.31  0.35 ]
[ 0.39  0.99  0.99  0.53  0.87  0.92  0.98  0.99  0.99  0.92  1.00  0.98 ]
[ 0.99  1.00  0.93  0.99  0.99  0.89  0.97  1.00  0.91  0.99  1.00  0.99 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  0.99  1.00  0.98  0.97  0.79 ]

10/27 12:26:34 PM | === Batch 16/200
10/27 12:26:34 PM | loss = 0.625 + 0.014 + 0.054 = 0.694
10/27 12:26:35 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.82  0.07  0.23  0.82  0.38  0.08  0.07  0.90  0.16  0.99 ]
[ 0.92  0.28  0.97  0.11  0.88  0.88  0.85  0.13  0.93  0.99  0.96  0.76 ]
[ 0.98  0.97  0.26  0.99  0.21  0.99  0.89  0.99  0.28  0.13  0.13  0.98 ]
[ 0.27  0.69  0.98  0.97  0.72  0.98  0.98  0.89  0.97  0.94  0.99  0.98 ]
[ 0.67  0.98  0.94  0.29  0.92  0.98  0.96  0.23  1.00  0.96  0.98  0.43 ]
[ 0.92  0.94  0.76  0.15  0.09  0.78  0.62  0.93  0.33  0.87  0.19  0.60 ]
[ 0.92  0.96  0.82  0.79  0.96  0.61  0.26  0.18  0.99  0.99  1.00  0.73 ]
[ 0.08  0.46  0.24  0.98  0.08  0.55  0.41  0.10  0.92  0.77  0.41  0.79 ]
[ 0.52  0.85  0.10  0.54  0.48  0.10  0.16  0.61  0.63  0.07  0.84  1.00 ]
[ 0.67  0.12  0.11  0.08  0.92  0.35  0.13  0.09  0.95  0.17  0.25  0.33 ]
[ 0.36  0.99  0.99  0.60  0.90  0.93  0.98  0.99  0.99  0.94  1.00  0.98 ]
[ 0.99  1.00  0.93  0.99  0.99  0.89  0.97  1.00  0.91  0.99  1.00  0.99 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  0.99  1.00  0.97  0.97  0.73 ]

10/27 12:26:35 PM | === Batch 17/200
10/27 12:26:35 PM | loss = 0.741 + 0.131 + 0.052 = 0.924
10/27 12:26:35 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.83  0.06  0.21  0.83  0.38  0.07  0.06  0.91  0.14  0.99 ]
[ 0.93  0.29  0.97  0.09  0.87  0.90  0.86  0.12  0.94  0.99  0.97  0.77 ]
[ 0.98  0.97  0.27  0.99  0.19  0.99  0.89  0.99  0.28  0.12  0.12  0.98 ]
[ 0.25  0.66  0.98  0.97  0.75  0.98  0.98  0.90  0.97  0.93  0.99  0.98 ]
[ 0.68  0.98  0.93  0.32  0.92  0.98  0.97  0.21  1.00  0.97  0.98  0.44 ]
[ 0.92  0.94  0.76  0.15  0.08  0.78  0.65  0.92  0.33  0.88  0.17  0.58 ]
[ 0.91  0.96  0.82  0.80  0.96  0.64  0.24  0.20  0.99  0.99  1.00  0.73 ]
[ 0.07  0.47  0.24  0.98  0.07  0.51  0.41  0.08  0.92  0.76  0.41  0.81 ]
[ 0.53  0.88  0.09  0.58  0.49  0.09  0.16  0.64  0.64  0.06  0.87  1.00 ]
[ 0.68  0.10  0.09  0.07  0.92  0.35  0.12  0.08  0.96  0.15  0.22  0.33 ]
[ 0.34  0.99  0.99  0.65  0.92  0.93  0.98  0.99  1.00  0.95  1.00  0.98 ]
[ 0.99  1.00  0.94  0.99  0.99  0.89  0.97  1.00  0.91  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  0.99  1.00  0.97  0.97  0.72 ]

10/27 12:26:35 PM | === Batch 18/200
10/27 12:26:35 PM | loss = 0.878 + 0.138 + 0.050 = 1.065
10/27 12:26:36 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.85  0.06  0.21  0.85  0.39  0.06  0.05  0.91  0.13  0.99 ]
[ 0.94  0.32  0.97  0.09  0.86  0.91  0.87  0.12  0.94  0.99  0.97  0.78 ]
[ 0.98  0.97  0.29  0.99  0.18  0.99  0.89  0.99  0.28  0.11  0.11  0.98 ]
[ 0.26  0.66  0.98  0.98  0.77  0.99  0.98  0.91  0.98  0.94  0.99  0.98 ]
[ 0.70  0.98  0.94  0.38  0.92  0.98  0.97  0.21  1.00  0.97  0.98  0.45 ]
[ 0.92  0.94  0.78  0.15  0.08  0.79  0.68  0.92  0.34  0.89  0.17  0.58 ]
[ 0.91  0.96  0.83  0.82  0.96  0.68  0.23  0.20  0.99  0.99  1.00  0.75 ]
[ 0.06  0.49  0.25  0.99  0.07  0.51  0.42  0.07  0.92  0.76  0.41  0.83 ]
[ 0.56  0.90  0.08  0.60  0.52  0.08  0.16  0.68  0.68  0.05  0.90  1.00 ]
[ 0.71  0.09  0.09  0.06  0.92  0.39  0.11  0.07  0.96  0.13  0.20  0.35 ]
[ 0.34  0.99  1.00  0.67  0.93  0.93  0.99  0.99  1.00  0.96  1.00  0.98 ]
[ 0.99  1.00  0.94  0.99  0.99  0.90  0.97  1.00  0.91  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.97  0.99  1.00  0.99  0.99  1.00  0.97  0.97  0.75 ]

10/27 12:26:36 PM | === Batch 19/200
10/27 12:26:36 PM | loss = 0.744 + 0.040 + 0.048 = 0.832
10/27 12:26:36 PM | attribution_score[0:12]: 
[ 1.00  0.99  0.87  0.05  0.22  0.87  0.42  0.05  0.04  0.92  0.12  0.99 ]
[ 0.94  0.35  0.97  0.08  0.88  0.92  0.89  0.13  0.94  0.99  0.98  0.80 ]
[ 0.98  0.97  0.33  0.99  0.19  0.99  0.90  0.99  0.30  0.10  0.11  0.98 ]
[ 0.27  0.67  0.99  0.98  0.81  0.99  0.98  0.91  0.98  0.94  0.99  0.98 ]
[ 0.73  0.99  0.94  0.44  0.92  0.99  0.98  0.22  1.00  0.97  0.98  0.47 ]
[ 0.93  0.95  0.80  0.15  0.09  0.82  0.72  0.93  0.35  0.90  0.18  0.61 ]
[ 0.91  0.97  0.85  0.83  0.96  0.73  0.23  0.20  0.99  0.99  1.00  0.78 ]
[ 0.06  0.52  0.26  0.99  0.07  0.53  0.45  0.07  0.92  0.78  0.44  0.86 ]
[ 0.62  0.92  0.07  0.62  0.57  0.08  0.17  0.72  0.73  0.04  0.92  1.00 ]
[ 0.74  0.08  0.08  0.06  0.93  0.43  0.11  0.06  0.97  0.13  0.20  0.40 ]
[ 0.34  0.99  1.00  0.68  0.94  0.94  0.99  0.99  1.00  0.96  1.00  0.98 ]
[ 0.99  1.00  0.95  0.99  0.99  0.91  0.98  1.00  0.92  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  0.99  1.00  0.97  0.97  0.79 ]

10/27 12:26:36 PM | === Batch 20/200
10/27 12:26:36 PM | loss = 0.725 + 0.117 + 0.046 = 0.887
10/27 12:26:37 PM | === Batch 21/200
10/27 12:26:37 PM | loss = 0.560 + 0.220 + 0.044 = 0.824
10/27 12:26:37 PM | === Batch 22/200
10/27 12:26:37 PM | loss = 0.477 + 0.280 + 0.042 = 0.800
10/27 12:26:38 PM | === Batch 23/200
10/27 12:26:38 PM | loss = 0.306 + 0.304 + 0.041 = 0.651
10/27 12:26:38 PM | === Batch 24/200
10/27 12:26:39 PM | loss = 0.445 + 0.292 + 0.040 = 0.777
10/27 12:26:39 PM | === Batch 25/200
10/27 12:26:39 PM | loss = 0.522 + 0.256 + 0.039 = 0.816
10/27 12:26:39 PM | === Batch 26/200
10/27 12:26:40 PM | loss = 0.363 + 0.196 + 0.038 = 0.596
10/27 12:26:40 PM | === Batch 27/200
10/27 12:26:40 PM | loss = 0.548 + 0.121 + 0.037 = 0.706
10/27 12:26:40 PM | === Batch 28/200
10/27 12:26:41 PM | loss = 0.625 + 0.032 + 0.036 = 0.693
10/27 12:26:41 PM | === Batch 29/200
10/27 12:26:41 PM | loss = 0.615 + 0.073 + 0.036 = 0.724
10/27 12:26:41 PM | === Batch 30/200
10/27 12:26:42 PM | loss = 0.505 + 0.104 + 0.035 = 0.645
10/27 12:26:42 PM | === Batch 31/200
10/27 12:26:42 PM | loss = 0.473 + 0.074 + 0.034 = 0.581
10/27 12:26:42 PM | === Batch 32/200
10/27 12:26:43 PM | loss = 0.452 + 0.006 + 0.033 = 0.491
10/27 12:26:43 PM | === Batch 33/200
10/27 12:26:43 PM | loss = 0.458 + 0.045 + 0.032 = 0.535
10/27 12:26:43 PM | === Batch 34/200
10/27 12:26:44 PM | loss = 0.409 + 0.059 + 0.032 = 0.500
10/27 12:26:44 PM | === Batch 35/200
10/27 12:26:44 PM | loss = 0.553 + 0.048 + 0.031 = 0.633
10/27 12:26:44 PM | === Batch 36/200
10/27 12:26:45 PM | loss = 0.349 + 0.017 + 0.031 = 0.397
10/27 12:26:45 PM | === Batch 37/200
10/27 12:26:45 PM | loss = 0.461 + 0.039 + 0.030 = 0.530
10/27 12:26:45 PM | === Batch 38/200
10/27 12:26:46 PM | loss = 0.608 + 0.039 + 0.030 = 0.677
10/27 12:26:46 PM | === Batch 39/200
10/27 12:26:46 PM | loss = 0.315 + 0.007 + 0.030 = 0.352
10/27 12:26:46 PM | === Batch 40/200
10/27 12:26:47 PM | loss = 0.400 + 0.021 + 0.029 = 0.451
10/27 12:26:47 PM | === Batch 41/200
10/27 12:26:47 PM | loss = 0.417 + 0.014 + 0.029 = 0.460
10/27 12:26:47 PM | === Batch 42/200
10/27 12:26:48 PM | loss = 0.342 + 0.014 + 0.029 = 0.385
10/27 12:26:48 PM | === Batch 43/200
10/27 12:26:48 PM | loss = 0.348 + 0.007 + 0.028 = 0.383
10/27 12:26:48 PM | === Batch 44/200
10/27 12:26:49 PM | loss = 0.328 + 0.006 + 0.028 = 0.362
10/27 12:26:49 PM | === Batch 45/200
10/27 12:26:49 PM | loss = 0.518 + 0.018 + 0.028 = 0.564
10/27 12:26:49 PM | === Batch 46/200
10/27 12:26:50 PM | loss = 0.404 + 0.009 + 0.028 = 0.440
10/27 12:26:50 PM | === Batch 47/200
10/27 12:26:50 PM | loss = 0.357 + 0.008 + 0.027 = 0.392
10/27 12:26:51 PM | === Batch 48/200
10/27 12:26:51 PM | loss = 0.245 + 0.019 + 0.027 = 0.291
10/27 12:26:51 PM | === Batch 49/200
10/27 12:26:51 PM | loss = 0.347 + 0.004 + 0.027 = 0.378
10/27 12:26:52 PM | === Batch 50/200
10/27 12:26:52 PM | loss = 0.389 + 0.004 + 0.027 = 0.420
10/27 12:26:52 PM | === Batch 51/200
10/27 12:26:52 PM | loss = 0.314 + 0.019 + 0.027 = 0.359
10/27 12:26:53 PM | === Batch 52/200
10/27 12:26:53 PM | loss = 0.425 + 0.006 + 0.026 = 0.457
10/27 12:26:53 PM | === Batch 53/200
10/27 12:26:53 PM | loss = 0.300 + 0.007 + 0.026 = 0.332
10/27 12:26:54 PM | === Batch 54/200
10/27 12:26:54 PM | loss = 0.434 + 0.016 + 0.026 = 0.476
10/27 12:26:54 PM | === Batch 55/200
10/27 12:26:54 PM | loss = 0.441 + 0.009 + 0.026 = 0.476
10/27 12:26:55 PM | === Batch 56/200
10/27 12:26:55 PM | loss = 0.333 + 0.010 + 0.026 = 0.370
10/27 12:26:55 PM | === Batch 57/200
10/27 12:26:55 PM | loss = 0.268 + 0.008 + 0.026 = 0.303
10/27 12:26:56 PM | === Batch 58/200
10/27 12:26:56 PM | loss = 0.360 + 0.019 + 0.026 = 0.405
10/27 12:26:56 PM | === Batch 59/200
10/27 12:26:56 PM | loss = 0.246 + 0.024 + 0.025 = 0.296
10/27 12:26:57 PM | === Batch 60/200
10/27 12:26:57 PM | loss = 0.306 + 0.007 + 0.025 = 0.338
10/27 12:26:57 PM | === Batch 61/200
10/27 12:26:57 PM | loss = 0.373 + 0.038 + 0.026 = 0.436
10/27 12:26:58 PM | === Batch 62/200
10/27 12:26:58 PM | loss = 0.346 + 0.029 + 0.025 = 0.400
10/27 12:26:58 PM | === Batch 63/200
10/27 12:26:59 PM | loss = 0.424 + 0.022 + 0.025 = 0.471
10/27 12:26:59 PM | === Batch 64/200
10/27 12:26:59 PM | loss = 0.365 + 0.046 + 0.025 = 0.436
10/27 12:26:59 PM | === Batch 65/200
10/27 12:27:00 PM | loss = 0.368 + 0.048 + 0.024 = 0.441
10/27 12:27:00 PM | === Batch 66/200
10/27 12:27:00 PM | loss = 0.318 + 0.030 + 0.024 = 0.372
10/27 12:27:00 PM | === Batch 67/200
10/27 12:27:01 PM | loss = 0.347 + 0.010 + 0.024 = 0.381
10/27 12:27:01 PM | === Batch 68/200
10/27 12:27:01 PM | loss = 0.322 + 0.004 + 0.024 = 0.350
10/27 12:27:02 PM | === Batch 69/200
10/27 12:27:02 PM | loss = 0.350 + 0.037 + 0.024 = 0.410
10/27 12:27:02 PM | === Batch 70/200
10/27 12:27:02 PM | loss = 0.359 + 0.054 + 0.024 = 0.436
10/27 12:27:03 PM | === Batch 71/200
10/27 12:27:03 PM | loss = 0.197 + 0.050 + 0.023 = 0.270
10/27 12:27:03 PM | === Batch 72/200
10/27 12:27:03 PM | loss = 0.327 + 0.023 + 0.023 = 0.374
10/27 12:27:04 PM | === Batch 73/200
10/27 12:27:04 PM | loss = 0.284 + 0.025 + 0.023 = 0.332
10/27 12:27:04 PM | === Batch 74/200
10/27 12:27:04 PM | loss = 0.256 + 0.027 + 0.023 = 0.307
10/27 12:27:05 PM | === Batch 75/200
10/27 12:27:05 PM | loss = 0.266 + 0.009 + 0.023 = 0.297
10/27 12:27:05 PM | === Batch 76/200
10/27 12:27:05 PM | loss = 0.364 + 0.018 + 0.023 = 0.405
10/27 12:27:06 PM | === Batch 77/200
10/27 12:27:06 PM | loss = 0.330 + 0.008 + 0.022 = 0.361
10/27 12:27:06 PM | === Batch 78/200
10/27 12:27:07 PM | loss = 0.370 + 0.022 + 0.023 = 0.415
10/27 12:27:07 PM | === Batch 79/200
10/27 12:27:07 PM | loss = 0.323 + 0.006 + 0.022 = 0.352
10/27 12:27:07 PM | === Batch 80/200
10/27 12:27:08 PM | loss = 0.290 + 0.042 + 0.022 = 0.355
10/27 12:27:08 PM | === Batch 81/200
10/27 12:27:08 PM | loss = 0.233 + 0.063 + 0.022 = 0.318
10/27 12:27:09 PM | === Batch 82/200
10/27 12:27:09 PM | loss = 0.221 + 0.060 + 0.022 = 0.302
10/27 12:27:09 PM | === Batch 83/200
10/27 12:27:09 PM | loss = 0.248 + 0.034 + 0.022 = 0.304
10/27 12:27:10 PM | === Batch 84/200
10/27 12:27:10 PM | loss = 0.354 + 0.011 + 0.022 = 0.387
10/27 12:27:10 PM | === Batch 85/200
10/27 12:27:10 PM | loss = 0.384 + 0.014 + 0.022 = 0.420
10/27 12:27:11 PM | === Batch 86/200
10/27 12:27:11 PM | loss = 0.320 + 0.028 + 0.022 = 0.370
10/27 12:27:11 PM | === Batch 87/200
10/27 12:27:11 PM | loss = 0.339 + 0.044 + 0.021 = 0.405
10/27 12:27:12 PM | === Batch 88/200
10/27 12:27:12 PM | loss = 0.261 + 0.040 + 0.021 = 0.322
10/27 12:27:12 PM | === Batch 89/200
10/27 12:27:12 PM | loss = 0.335 + 0.016 + 0.021 = 0.372
10/27 12:27:13 PM | === Batch 90/200
10/27 12:27:13 PM | loss = 0.377 + 0.025 + 0.021 = 0.424
10/27 12:27:13 PM | === Batch 91/200
10/27 12:27:13 PM | loss = 0.233 + 0.024 + 0.021 = 0.277
10/27 12:27:14 PM | === Batch 92/200
10/27 12:27:14 PM | loss = 0.242 + 0.015 + 0.021 = 0.277
10/27 12:27:14 PM | === Batch 93/200
10/27 12:27:14 PM | loss = 0.226 + 0.030 + 0.020 = 0.277
10/27 12:27:15 PM | === Batch 94/200
10/27 12:27:15 PM | loss = 0.261 + 0.026 + 0.020 = 0.307
10/27 12:27:15 PM | === Batch 95/200
10/27 12:27:15 PM | loss = 0.173 + 0.004 + 0.020 = 0.197
10/27 12:27:16 PM | === Batch 96/200
10/27 12:27:16 PM | loss = 0.300 + 0.038 + 0.020 = 0.357
10/27 12:27:16 PM | === Batch 97/200
10/27 12:27:16 PM | loss = 0.276 + 0.036 + 0.020 = 0.332
10/27 12:27:17 PM | === Batch 98/200
10/27 12:27:17 PM | loss = 0.267 + 0.002 + 0.020 = 0.288
10/27 12:27:17 PM | === Batch 99/200
10/27 12:27:17 PM | loss = 0.237 + 0.014 + 0.019 = 0.271
10/27 12:27:18 PM | === Batch 100/200
10/27 12:27:18 PM | loss = 0.244 + 0.007 + 0.019 = 0.270
10/27 12:27:18 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.92  0.00  0.02  0.85  0.67  0.00  0.00  0.98  0.01  0.99 ]
[ 0.98  0.02  0.98  0.01  0.98  0.94  0.98  0.08  0.99  0.99  1.00  0.70 ]
[ 1.00  0.98  0.93  0.99  0.01  1.00  0.29  0.99  0.12  0.02  0.02  0.97 ]
[ 0.93  0.88  1.00  0.99  0.97  0.99  0.98  0.84  0.91  0.94  1.00  0.99 ]
[ 0.93  0.99  0.99  0.97  0.53  1.00  0.99  0.04  1.00  0.97  0.97  0.92 ]
[ 0.99  0.96  0.97  0.01  0.63  0.79  0.52  0.98  0.20  0.90  0.09  0.68 ]
[ 0.96  0.99  0.70  0.98  0.99  0.82  0.04  0.03  0.99  1.00  1.00  0.96 ]
[ 0.01  0.95  0.03  1.00  0.18  0.83  1.00  0.00  0.99  0.99  0.02  1.00 ]
[ 0.98  1.00  0.02  0.80  0.97  0.01  0.01  0.97  0.94  0.02  1.00  1.00 ]
[ 0.93  0.01  0.01  0.01  0.99  0.54  0.01  0.01  1.00  0.03  0.52  0.95 ]
[ 0.01  1.00  1.00  0.18  0.99  0.99  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.98  1.00  1.00  0.94  1.00  1.00  0.59  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.98 ]

10/27 12:27:18 PM | === Batch 101/200
10/27 12:27:19 PM | loss = 0.334 + 0.017 + 0.019 = 0.370
10/27 12:27:19 PM | === Batch 102/200
10/27 12:27:19 PM | loss = 0.268 + 0.000 + 0.019 = 0.288
10/27 12:27:19 PM | === Batch 103/200
10/27 12:27:20 PM | loss = 0.254 + 0.044 + 0.019 = 0.317
10/27 12:27:20 PM | === Batch 104/200
10/27 12:27:20 PM | loss = 0.197 + 0.066 + 0.019 = 0.281
10/27 12:27:20 PM | === Batch 105/200
10/27 12:27:21 PM | loss = 0.216 + 0.066 + 0.019 = 0.300
10/27 12:27:21 PM | === Batch 106/200
10/27 12:27:21 PM | loss = 0.326 + 0.047 + 0.019 = 0.392
10/27 12:27:21 PM | === Batch 107/200
10/27 12:27:22 PM | loss = 0.326 + 0.013 + 0.019 = 0.358
10/27 12:27:22 PM | === Batch 108/200
10/27 12:27:22 PM | loss = 0.383 + 0.040 + 0.019 = 0.442
10/27 12:27:22 PM | === Batch 109/200
10/27 12:27:23 PM | loss = 0.210 + 0.054 + 0.019 = 0.282
10/27 12:27:23 PM | === Batch 110/200
10/27 12:27:23 PM | loss = 0.184 + 0.032 + 0.018 = 0.235
10/27 12:27:24 PM | === Batch 111/200
10/27 12:27:24 PM | loss = 0.205 + 0.016 + 0.018 = 0.240
10/27 12:27:24 PM | === Batch 112/200
10/27 12:27:24 PM | loss = 0.121 + 0.040 + 0.018 = 0.179
10/27 12:27:25 PM | === Batch 113/200
10/27 12:27:25 PM | loss = 0.233 + 0.043 + 0.018 = 0.294
10/27 12:27:25 PM | === Batch 114/200
10/27 12:27:25 PM | loss = 0.315 + 0.032 + 0.018 = 0.364
10/27 12:27:26 PM | === Batch 115/200
10/27 12:27:26 PM | loss = 0.229 + 0.006 + 0.018 = 0.253
10/27 12:27:26 PM | === Batch 116/200
10/27 12:27:26 PM | loss = 0.231 + 0.037 + 0.018 = 0.286
10/27 12:27:27 PM | === Batch 117/200
10/27 12:27:27 PM | loss = 0.309 + 0.044 + 0.018 = 0.370
10/27 12:27:27 PM | === Batch 118/200
10/27 12:27:28 PM | loss = 0.259 + 0.016 + 0.017 = 0.292
10/27 12:27:28 PM | === Batch 119/200
10/27 12:27:28 PM | loss = 0.298 + 0.036 + 0.017 = 0.351
10/27 12:27:28 PM | === Batch 120/200
10/27 12:27:29 PM | loss = 0.156 + 0.064 + 0.017 = 0.238
10/27 12:27:29 PM | === Batch 121/200
10/27 12:27:29 PM | loss = 0.272 + 0.073 + 0.017 = 0.362
10/27 12:27:30 PM | === Batch 122/200
10/27 12:27:30 PM | loss = 0.269 + 0.065 + 0.017 = 0.351
10/27 12:27:30 PM | === Batch 123/200
10/27 12:27:30 PM | loss = 0.285 + 0.043 + 0.017 = 0.345
10/27 12:27:31 PM | === Batch 124/200
10/27 12:27:31 PM | loss = 0.199 + 0.009 + 0.017 = 0.225
10/27 12:27:31 PM | === Batch 125/200
10/27 12:27:31 PM | loss = 0.249 + 0.045 + 0.017 = 0.312
10/27 12:27:32 PM | === Batch 126/200
10/27 12:27:32 PM | loss = 0.334 + 0.061 + 0.018 = 0.412
10/27 12:27:32 PM | === Batch 127/200
10/27 12:27:32 PM | loss = 0.172 + 0.041 + 0.017 = 0.230
10/27 12:27:33 PM | === Batch 128/200
10/27 12:27:33 PM | loss = 0.223 + 0.008 + 0.017 = 0.248
10/27 12:27:33 PM | === Batch 129/200
10/27 12:27:33 PM | loss = 0.247 + 0.031 + 0.017 = 0.295
10/27 12:27:34 PM | === Batch 130/200
10/27 12:27:34 PM | loss = 0.213 + 0.035 + 0.017 = 0.265
10/27 12:27:34 PM | === Batch 131/200
10/27 12:27:34 PM | loss = 0.258 + 0.022 + 0.017 = 0.297
10/27 12:27:35 PM | === Batch 132/200
10/27 12:27:35 PM | loss = 0.235 + 0.006 + 0.017 = 0.258
10/27 12:27:35 PM | === Batch 133/200
10/27 12:27:35 PM | loss = 0.411 + 0.002 + 0.017 = 0.430
10/27 12:27:36 PM | === Batch 134/200
10/27 12:27:36 PM | loss = 0.273 + 0.029 + 0.017 = 0.319
10/27 12:27:36 PM | === Batch 135/200
10/27 12:27:36 PM | loss = 0.132 + 0.041 + 0.016 = 0.190
10/27 12:27:37 PM | === Batch 136/200
10/27 12:27:37 PM | loss = 0.172 + 0.036 + 0.016 = 0.224
10/27 12:27:37 PM | === Batch 137/200
10/27 12:27:37 PM | loss = 0.212 + 0.017 + 0.016 = 0.246
10/27 12:27:38 PM | === Batch 138/200
10/27 12:27:38 PM | loss = 0.266 + 0.018 + 0.016 = 0.299
10/27 12:27:38 PM | === Batch 139/200
10/27 12:27:39 PM | loss = 0.367 + 0.021 + 0.016 = 0.404
10/27 12:27:39 PM | === Batch 140/200
10/27 12:27:39 PM | loss = 0.210 + 0.006 + 0.016 = 0.233
10/27 12:27:39 PM | === Batch 141/200
10/27 12:27:40 PM | loss = 0.268 + 0.013 + 0.016 = 0.297
10/27 12:27:40 PM | === Batch 142/200
10/27 12:27:40 PM | loss = 0.191 + 0.005 + 0.016 = 0.212
10/27 12:27:40 PM | === Batch 143/200
10/27 12:27:41 PM | loss = 0.220 + 0.019 + 0.016 = 0.255
10/27 12:27:41 PM | === Batch 144/200
10/27 12:27:41 PM | loss = 0.227 + 0.013 + 0.016 = 0.256
10/27 12:27:41 PM | === Batch 145/200
10/27 12:27:42 PM | loss = 0.323 + 0.019 + 0.016 = 0.357
10/27 12:27:42 PM | === Batch 146/200
10/27 12:27:42 PM | loss = 0.261 + 0.031 + 0.016 = 0.308
10/27 12:27:42 PM | === Batch 147/200
10/27 12:27:43 PM | loss = 0.273 + 0.028 + 0.016 = 0.317
10/27 12:27:43 PM | === Batch 148/200
10/27 12:27:43 PM | loss = 0.191 + 0.013 + 0.016 = 0.220
10/27 12:27:44 PM | === Batch 149/200
10/27 12:27:44 PM | loss = 0.252 + 0.017 + 0.016 = 0.285
10/27 12:27:44 PM | === Batch 150/200
10/27 12:27:44 PM | loss = 0.212 + 0.013 + 0.016 = 0.241
10/27 12:27:45 PM | === Batch 151/200
10/27 12:27:45 PM | loss = 0.327 + 0.018 + 0.016 = 0.361
10/27 12:27:45 PM | === Batch 152/200
10/27 12:27:45 PM | loss = 0.179 + 0.031 + 0.016 = 0.225
10/27 12:27:46 PM | === Batch 153/200
10/27 12:27:46 PM | loss = 0.158 + 0.027 + 0.016 = 0.201
10/27 12:27:46 PM | === Batch 154/200
10/27 12:27:47 PM | loss = 0.189 + 0.008 + 0.016 = 0.212
10/27 12:27:47 PM | === Batch 155/200
10/27 12:27:47 PM | loss = 0.298 + 0.030 + 0.016 = 0.344
10/27 12:27:47 PM | === Batch 156/200
10/27 12:27:48 PM | loss = 0.313 + 0.033 + 0.016 = 0.362
10/27 12:27:48 PM | === Batch 157/200
10/27 12:27:48 PM | loss = 0.237 + 0.000 + 0.016 = 0.253
10/27 12:27:48 PM | === Batch 158/200
10/27 12:27:49 PM | loss = 0.222 + 0.052 + 0.016 = 0.290
10/27 12:27:49 PM | === Batch 159/200
10/27 12:27:49 PM | loss = 0.133 + 0.084 + 0.015 = 0.232
10/27 12:27:49 PM | === Batch 160/200
10/27 12:27:50 PM | loss = 0.181 + 0.096 + 0.015 = 0.292
10/27 12:27:50 PM | === Batch 161/200
10/27 12:27:50 PM | loss = 0.209 + 0.091 + 0.015 = 0.315
10/27 12:27:50 PM | === Batch 162/200
10/27 12:27:51 PM | loss = 0.186 + 0.072 + 0.015 = 0.273
10/27 12:27:51 PM | === Batch 163/200
10/27 12:27:51 PM | loss = 0.286 + 0.040 + 0.015 = 0.340
10/27 12:27:51 PM | === Batch 164/200
10/27 12:27:52 PM | loss = 0.196 + 0.005 + 0.015 = 0.216
10/27 12:27:52 PM | === Batch 165/200
10/27 12:27:52 PM | loss = 0.314 + 0.019 + 0.015 = 0.349
10/27 12:27:52 PM | === Batch 166/200
10/27 12:27:53 PM | loss = 0.177 + 0.002 + 0.015 = 0.195
10/27 12:27:53 PM | === Batch 167/200
10/27 12:27:53 PM | loss = 0.124 + 0.034 + 0.015 = 0.173
10/27 12:27:53 PM | === Batch 168/200
10/27 12:27:54 PM | loss = 0.162 + 0.051 + 0.014 = 0.227
10/27 12:27:54 PM | === Batch 169/200
10/27 12:27:54 PM | loss = 0.210 + 0.052 + 0.014 = 0.277
10/27 12:27:54 PM | === Batch 170/200
10/27 12:27:55 PM | loss = 0.212 + 0.040 + 0.014 = 0.266
10/27 12:27:55 PM | === Batch 171/200
10/27 12:27:55 PM | loss = 0.239 + 0.016 + 0.014 = 0.269
10/27 12:27:55 PM | === Batch 172/200
10/27 12:27:56 PM | loss = 0.238 + 0.022 + 0.015 = 0.274
10/27 12:27:56 PM | === Batch 173/200
10/27 12:27:56 PM | loss = 0.185 + 0.032 + 0.015 = 0.232
10/27 12:27:57 PM | === Batch 174/200
10/27 12:27:57 PM | loss = 0.265 + 0.017 + 0.014 = 0.296
10/27 12:27:57 PM | === Batch 175/200
10/27 12:27:57 PM | loss = 0.272 + 0.022 + 0.014 = 0.308
10/27 12:27:58 PM | === Batch 176/200
10/27 12:27:58 PM | loss = 0.162 + 0.043 + 0.014 = 0.218
10/27 12:27:58 PM | === Batch 177/200
10/27 12:27:59 PM | loss = 0.156 + 0.047 + 0.014 = 0.218
10/27 12:27:59 PM | === Batch 178/200
10/27 12:27:59 PM | loss = 0.135 + 0.038 + 0.014 = 0.187
10/27 12:27:59 PM | === Batch 179/200
10/27 12:28:00 PM | loss = 0.208 + 0.015 + 0.014 = 0.237
10/27 12:28:00 PM | === Batch 180/200
10/27 12:28:00 PM | loss = 0.206 + 0.022 + 0.014 = 0.242
10/27 12:28:01 PM | === Batch 181/200
10/27 12:28:01 PM | loss = 0.314 + 0.032 + 0.014 = 0.361
10/27 12:28:01 PM | === Batch 182/200
10/27 12:28:01 PM | loss = 0.140 + 0.015 + 0.014 = 0.168
10/27 12:28:02 PM | === Batch 183/200
10/27 12:28:02 PM | loss = 0.082 + 0.022 + 0.014 = 0.118
10/27 12:28:02 PM | === Batch 184/200
10/27 12:28:02 PM | loss = 0.186 + 0.039 + 0.014 = 0.239
10/27 12:28:03 PM | === Batch 185/200
10/27 12:28:03 PM | loss = 0.214 + 0.042 + 0.013 = 0.270
10/27 12:28:03 PM | === Batch 186/200
10/27 12:28:04 PM | loss = 0.121 + 0.031 + 0.014 = 0.165
10/27 12:28:04 PM | === Batch 187/200
10/27 12:28:04 PM | loss = 0.211 + 0.007 + 0.014 = 0.232
10/27 12:28:04 PM | === Batch 188/200
10/27 12:28:05 PM | loss = 0.150 + 0.031 + 0.014 = 0.194
10/27 12:28:05 PM | === Batch 189/200
10/27 12:28:05 PM | loss = 0.103 + 0.043 + 0.014 = 0.159
10/27 12:28:05 PM | === Batch 190/200
10/27 12:28:06 PM | loss = 0.215 + 0.028 + 0.014 = 0.257
10/27 12:28:06 PM | === Batch 191/200
10/27 12:28:06 PM | loss = 0.123 + 0.008 + 0.014 = 0.145
10/27 12:28:06 PM | === Batch 192/200
10/27 12:28:07 PM | loss = 0.197 + 0.024 + 0.014 = 0.234
10/27 12:28:07 PM | === Batch 193/200
10/27 12:28:07 PM | loss = 0.204 + 0.025 + 0.014 = 0.243
10/27 12:28:07 PM | === Batch 194/200
10/27 12:28:08 PM | loss = 0.132 + 0.011 + 0.014 = 0.156
10/27 12:28:08 PM | === Batch 195/200
10/27 12:28:08 PM | loss = 0.199 + 0.017 + 0.014 = 0.229
10/27 12:28:08 PM | === Batch 196/200
10/27 12:28:09 PM | loss = 0.233 + 0.018 + 0.014 = 0.264
10/27 12:28:09 PM | === Batch 197/200
10/27 12:28:09 PM | loss = 0.173 + 0.006 + 0.013 = 0.192
10/27 12:28:09 PM | === Batch 198/200
10/27 12:28:10 PM | loss = 0.155 + 0.013 + 0.013 = 0.181
10/27 12:28:10 PM | === Batch 199/200
10/27 12:28:10 PM | loss = 0.190 + 0.006 + 0.013 = 0.209
10/27 12:28:10 PM | === Batch 200/200
10/27 12:28:11 PM | loss = 0.285 + 0.016 + 0.013 = 0.315
10/27 12:28:11 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.96  0.00  0.01  0.95  0.86  0.00  0.00  0.99  0.00  0.99 ]
[ 0.98  0.02  0.99  0.00  0.97  0.99  0.98  0.03  0.99  0.99  1.00  0.87 ]
[ 1.00  0.99  0.89  1.00  0.01  1.00  0.60  0.99  0.04  0.01  0.02  0.98 ]
[ 0.79  0.95  1.00  1.00  0.98  0.99  0.95  0.75  0.85  0.93  1.00  0.99 ]
[ 0.52  0.98  0.99  0.97  0.95  0.98  0.99  0.28  1.00  0.99  1.00  0.87 ]
[ 0.99  0.99  0.86  0.01  0.73  0.94  0.05  0.96  0.03  0.92  0.05  0.16 ]
[ 0.98  0.99  0.98  0.99  1.00  0.93  0.02  0.01  1.00  1.00  1.00  0.97 ]
[ 0.00  0.92  0.02  1.00  0.30  0.94  1.00  0.00  0.99  0.99  0.01  1.00 ]
[ 0.98  1.00  0.01  0.96  0.98  0.00  0.01  0.99  0.96  0.01  1.00  1.00 ]
[ 0.97  0.01  0.00  0.00  1.00  0.85  0.01  0.00  1.00  0.01  0.96  0.98 ]
[ 0.01  1.00  1.00  0.04  0.99  0.99  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.99  1.00  1.00  0.98  1.00  1.00  0.03  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.99 ]

10/27 12:28:11 PM | ===
Best loss was 0.12 at iteration 183

10/27 12:28:11 PM | Looking for optimal threshold...
10/27 12:28:11 PM | Testing threshold 0.5
10/27 12:28:11 PM | Distance to target: 4,770,800
10/27 12:28:11 PM | Testing threshold 0.75
10/27 12:28:11 PM | Distance to target: 5,699,328
10/27 12:28:11 PM | Testing threshold 0.625
10/27 12:28:11 PM | Distance to target: 1,453,824
10/27 12:28:11 PM | Testing threshold 0.6875
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.71875
10/27 12:28:11 PM | Distance to target: 3,193,024
10/27 12:28:11 PM | Testing threshold 0.703125
10/27 12:28:11 PM | Distance to target: 915,168
10/27 12:28:11 PM | Testing threshold 0.6953125
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.69140625
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.693359375
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6943359375
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.69384765625
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.693603515625
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6937255859375
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.69366455078125
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.693695068359375
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936798095703125
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936721801757812
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936759948730469
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936740875244141
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936750411987305
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936745643615723
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936743259429932
10/27 12:28:11 PM | Distance to target: 137,376
10/27 12:28:11 PM | Testing threshold 0.6936742067337036
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936742663383484
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936742961406708
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.693674311041832
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743184924126
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743222177029
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.693674324080348
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743250116706
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743254773319
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743257101625
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743258265778
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743258847855
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259138893
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259284412
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259357172
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259393552
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259411742
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259420837
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259425384
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259427658
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259428795
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259429363
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259429647
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.693674325942979
10/27 12:28:11 PM | Distance to target: 88,544
10/27 12:28:11 PM | Testing threshold 0.6936743259429861
10/27 12:28:12 PM | Distance to target: 88,544
10/27 12:28:12 PM | Testing threshold 0.6936743259429896
10/27 12:28:12 PM | Distance to target: 88,544
10/27 12:28:12 PM | Testing threshold 0.6936743259429914
10/27 12:28:12 PM | Distance to target: 88,544
10/27 12:28:12 PM | Testing threshold 0.6936743259429923
10/27 12:28:12 PM | Distance to target: 88,544
10/27 12:28:13 PM | attribution_score[0:12]: 
[ 1.00  1.00  1.00  0.00  0.00  1.00  1.00  0.00  0.00  1.00  0.00  1.00 ]
[ 1.00  0.00  1.00  0.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  0.00  1.00  0.00  1.00  0.00  0.00  0.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 0.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  0.00  1.00  1.00  0.00  1.00  0.00  1.00  0.00  0.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  0.00  0.00  1.00  1.00  1.00  1.00 ]
[ 0.00  1.00  0.00  1.00  0.00  1.00  1.00  0.00  1.00  1.00  0.00  1.00 ]
[ 1.00  1.00  0.00  1.00  1.00  0.00  0.00  1.00  1.00  0.00  1.00  1.00 ]
[ 1.00  0.00  0.00  0.00  1.00  1.00  0.00  0.00  1.00  0.00  1.00  1.00 ]
[ 0.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]

10/27 12:28:13 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(29, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(56, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(91, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(114, 199, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(199, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(199, 165, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(165, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(165, 193, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(193, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(193, 276, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(276, 205, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(205, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(205, 267, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(267, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(267, 406, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(406, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(406, 468, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(468, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(468, 510, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(510, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=510, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/27 12:28:15 PM | ----------------------------------------
10/27 12:28:15 PM | Performances input model:
10/27 12:28:15 PM |  - flops: 314,294,272
10/27 12:28:15 PM |  - params: 14,991,946
10/27 12:28:15 PM |  - accuracy: 93.95999908447266
10/27 12:28:15 PM | ----------------------------------------
10/27 12:28:15 PM | Performances pseudo-pruned model:
10/27 12:28:15 PM |  - flops: 314,294,272
10/27 12:28:15 PM |  - params: 14,991,946
10/27 12:28:15 PM |  - accuracy: 88.29000091552734
10/27 12:28:15 PM | ----------------------------------------
10/27 12:28:15 PM | Performances pruned model:
10/27 12:28:15 PM |  - flops: 145,614,744
10/27 12:28:15 PM |  - params: 7,532,703
10/27 12:28:15 PM |  - accuracy: 88.29000091552734
