10/27 12:02:07 PM | args = Namespace(output_dir='result/test/test1', loaded_model_path='./checkpoints/cifar10/vgg_16_bn.pt', resume=False, test_only=False, mode='prune', batch_size=64, nb_batches=200, Mflops_target=108.61, lr=0.5, momentum=0.9, beta=6.0, gamma=0.2, gpu='1', num_workers=4, dataset='cifar10', arch='vgg_16_bn', save_plot=False, seed=1, lr_finetuning=0.03, epoch_finetuning=200, wd=0.002, data_dir='./data/cifar10/', print_freq=200, num_classes=10, device_ids=[1], device=device(type='cuda', index=0), name_base='')
10/27 12:02:19 PM | ----------------------------------------
10/27 12:02:19 PM | ==> Building model...
10/27 12:02:19 PM | ----------------------------------------
10/27 12:02:19 PM | ==> Loading weights into the model...
10/27 12:02:19 PM | ----------------------------------------
10/27 12:02:19 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=512, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/27 12:02:21 PM | Save modules info...
10/27 12:02:21 PM | FLOPS pruning ratio is 0.65
10/27 12:02:21 PM | Pruning with information flow
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: None
10/27 12:02:21 PM | cout: [0]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [0]
10/27 12:02:21 PM | cout: [0]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [0]
10/27 12:02:21 PM | cout: [0]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [0]
10/27 12:02:21 PM | cout: [1]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [1]
10/27 12:02:21 PM | cout: [1]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [1]
10/27 12:02:21 PM | cout: [1]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:02:21 PM | cin: [1]
10/27 12:02:21 PM | cout: [1]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [1]
10/27 12:02:21 PM | cout: [2]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [2]
10/27 12:02:21 PM | cout: [2]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [2]
10/27 12:02:21 PM | cout: [2]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [2]
10/27 12:02:21 PM | cout: [3]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [3]
10/27 12:02:21 PM | cout: [3]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [3]
10/27 12:02:21 PM | cout: [3]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:02:21 PM | cin: [3]
10/27 12:02:21 PM | cout: [3]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [3]
10/27 12:02:21 PM | cout: [4]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [4]
10/27 12:02:21 PM | cout: [4]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [4]
10/27 12:02:21 PM | cout: [4]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [4]
10/27 12:02:21 PM | cout: [5]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [5]
10/27 12:02:21 PM | cout: [5]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [5]
10/27 12:02:21 PM | cout: [5]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [5]
10/27 12:02:21 PM | cout: [6]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [6]
10/27 12:02:21 PM | cout: [6]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [6]
10/27 12:02:21 PM | cout: [6]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:02:21 PM | cin: [6]
10/27 12:02:21 PM | cout: [6]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [6]
10/27 12:02:21 PM | cout: [7]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [7]
10/27 12:02:21 PM | cout: [7]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [7]
10/27 12:02:21 PM | cout: [7]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [7]
10/27 12:02:21 PM | cout: [8]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [8]
10/27 12:02:21 PM | cout: [8]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [8]
10/27 12:02:21 PM | cout: [8]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [8]
10/27 12:02:21 PM | cout: [9]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [9]
10/27 12:02:21 PM | cout: [9]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [9]
10/27 12:02:21 PM | cout: [9]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
10/27 12:02:21 PM | cin: [9]
10/27 12:02:21 PM | cout: [9]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [9]
10/27 12:02:21 PM | cout: [10]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [10]
10/27 12:02:21 PM | cout: [10]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [10]
10/27 12:02:21 PM | cout: [10]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [10]
10/27 12:02:21 PM | cout: [11]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [11]
10/27 12:02:21 PM | cout: [11]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [11]
10/27 12:02:21 PM | cout: [11]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
10/27 12:02:21 PM | cin: [11]
10/27 12:02:21 PM | cout: [12]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
10/27 12:02:21 PM | cin: [12]
10/27 12:02:21 PM | cout: [12]
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: [12]
10/27 12:02:21 PM | cout: [12]
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Linear(in_features=512, out_features=512, bias=True)
10/27 12:02:21 PM | cin: [12]
10/27 12:02:21 PM | cout: None
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: ReLU(inplace=True)
10/27 12:02:21 PM | cin: None
10/27 12:02:21 PM | cout: None
10/27 12:02:21 PM | active
10/27 12:02:21 PM | -----
10/27 12:02:21 PM | module: Linear(in_features=512, out_features=10, bias=True)
10/27 12:02:21 PM | cin: None
10/27 12:02:21 PM | cout: None
10/27 12:02:21 PM | inactive
10/27 12:02:21 PM | Used masks: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
10/27 12:02:21 PM | 13 unique masks in total
10/27 12:02:22 PM | === Batch 1/200
10/27 12:02:22 PM | loss = 0.494 + 4.266 + 0.040 = 4.800
10/27 12:02:23 PM | attribution_score[0:12]: 
[ 0.94  0.94  0.94  0.85  0.85  0.85  0.85  0.85  0.85  0.94  0.85  0.94 ]
[ 0.85  0.85  0.94  0.85  0.94  0.85  0.94  0.85  0.85  0.94  0.94  0.85 ]
[ 0.94  0.94  0.85  0.94  0.85  0.94  0.94  0.94  0.94  0.85  0.85  0.94 ]
[ 0.85  0.85  0.94  0.94  0.85  0.85  0.94  0.85  0.94  0.94  0.94  0.94 ]
[ 0.85  0.94  0.94  0.85  0.85  0.94  0.85  0.85  0.94  0.94  0.85  0.94 ]
[ 0.85  0.94  0.94  0.85  0.85  0.85  0.85  0.94  0.85  0.85  0.85  0.85 ]
[ 0.94  0.94  0.85  0.85  0.94  0.85  0.85  0.94  0.94  0.94  0.94  0.85 ]
[ 0.85  0.85  0.85  0.94  0.85  0.85  0.94  0.85  0.94  0.94  0.85  0.85 ]
[ 0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.85  0.94 ]
[ 0.85  0.85  0.85  0.85  0.94  0.85  0.85  0.85  0.85  0.85  0.85  0.85 ]
[ 0.85  0.85  0.94  0.85  0.85  0.94  0.85  0.85  0.85  0.85  0.94  0.85 ]
[ 0.94  0.94  0.94  0.85  0.94  0.85  0.85  0.94  0.94  0.94  0.94  0.85 ]
[ 0.94  0.94  0.94  0.85  0.94  0.94  0.85  0.85  0.94  0.94  0.85  0.85 ]

10/27 12:02:23 PM | === Batch 2/200
10/27 12:02:23 PM | loss = 0.132 + 4.025 + 0.045 = 4.202
10/27 12:02:23 PM | attribution_score[0:12]: 
[ 0.96  0.95  0.92  0.77  0.77  0.78  0.77  0.77  0.77  0.94  0.78  0.95 ]
[ 0.83  0.78  0.94  0.77  0.95  0.81  0.95  0.77  0.81  0.95  0.95  0.78 ]
[ 0.96  0.95  0.77  0.96  0.78  0.95  0.95  0.96  0.92  0.77  0.78  0.95 ]
[ 0.77  0.79  0.95  0.95  0.77  0.87  0.94  0.85  0.95  0.96  0.95  0.96 ]
[ 0.77  0.95  0.92  0.77  0.77  0.95  0.77  0.77  0.95  0.92  0.79  0.92 ]
[ 0.77  0.95  0.94  0.77  0.77  0.77  0.79  0.95  0.78  0.77  0.78  0.78 ]
[ 0.92  0.94  0.77  0.77  0.95  0.77  0.79  0.92  0.95  0.91  0.96  0.77 ]
[ 0.77  0.77  0.77  0.95  0.77  0.77  0.95  0.77  0.95  0.94  0.77  0.77 ]
[ 0.77  0.77  0.77  0.78  0.78  0.77  0.77  0.77  0.77  0.77  0.78  0.94 ]
[ 0.78  0.77  0.77  0.77  0.94  0.77  0.77  0.77  0.77  0.77  0.77  0.77 ]
[ 0.77  0.84  0.95  0.78  0.78  0.95  0.82  0.77  0.84  0.78  0.96  0.82 ]
[ 0.95  0.96  0.94  0.84  0.96  0.79  0.77  0.96  0.94  0.95  0.95  0.85 ]
[ 0.96  0.96  0.96  0.81  0.96  0.96  0.77  0.86  0.95  0.96  0.85  0.78 ]

10/27 12:02:23 PM | === Batch 3/200
10/27 12:02:23 PM | loss = 0.180 + 3.515 + 0.057 = 3.752
10/27 12:02:24 PM | attribution_score[0:12]: 
[ 0.97  0.96  0.89  0.67  0.67  0.73  0.67  0.67  0.67  0.94  0.69  0.96 ]
[ 0.80  0.71  0.95  0.67  0.96  0.80  0.93  0.68  0.80  0.96  0.96  0.70 ]
[ 0.96  0.96  0.68  0.97  0.70  0.96  0.94  0.96  0.90  0.68  0.71  0.96 ]
[ 0.67  0.77  0.96  0.96  0.68  0.91  0.94  0.85  0.96  0.96  0.96  0.96 ]
[ 0.68  0.95  0.92  0.67  0.75  0.96  0.72  0.71  0.96  0.92  0.71  0.88 ]
[ 0.75  0.96  0.93  0.67  0.68  0.69  0.71  0.96  0.70  0.77  0.69  0.69 ]
[ 0.89  0.94  0.73  0.69  0.96  0.70  0.71  0.88  0.97  0.91  0.97  0.71 ]
[ 0.67  0.69  0.67  0.96  0.67  0.70  0.94  0.67  0.96  0.93  0.67  0.68 ]
[ 0.69  0.68  0.68  0.68  0.70  0.67  0.67  0.67  0.73  0.67  0.69  0.96 ]
[ 0.71  0.67  0.67  0.67  0.92  0.69  0.67  0.67  0.68  0.68  0.67  0.67 ]
[ 0.67  0.87  0.96  0.70  0.72  0.94  0.84  0.73  0.86  0.77  0.97  0.84 ]
[ 0.95  0.97  0.92  0.87  0.96  0.71  0.80  0.97  0.93  0.96  0.95  0.87 ]
[ 0.97  0.97  0.97  0.82  0.96  0.98  0.80  0.89  0.95  0.97  0.88  0.77 ]

10/27 12:02:24 PM | === Batch 4/200
10/27 12:02:24 PM | loss = 0.585 + 2.906 + 0.071 = 3.561
10/27 12:02:24 PM | attribution_score[0:12]: 
[ 0.98  0.97  0.85  0.55  0.55  0.72  0.57  0.56  0.55  0.91  0.58  0.96 ]
[ 0.74  0.62  0.95  0.55  0.96  0.77  0.91  0.59  0.80  0.96  0.96  0.60 ]
[ 0.97  0.97  0.57  0.98  0.59  0.96  0.93  0.96  0.86  0.58  0.65  0.97 ]
[ 0.61  0.74  0.96  0.96  0.64  0.93  0.94  0.81  0.96  0.96  0.97  0.96 ]
[ 0.57  0.95  0.94  0.64  0.77  0.96  0.71  0.63  0.97  0.91  0.75  0.83 ]
[ 0.78  0.97  0.91  0.59  0.60  0.65  0.62  0.95  0.59  0.79  0.59  0.58 ]
[ 0.84  0.93  0.74  0.68  0.95  0.64  0.63  0.83  0.97  0.92  0.98  0.67 ]
[ 0.55  0.59  0.56  0.97  0.56  0.69  0.93  0.55  0.96  0.91  0.55  0.64 ]
[ 0.60  0.58  0.56  0.60  0.61  0.55  0.56  0.58  0.74  0.55  0.67  0.97 ]
[ 0.69  0.57  0.56  0.56  0.90  0.67  0.55  0.55  0.65  0.56  0.56  0.57 ]
[ 0.55  0.91  0.96  0.61  0.72  0.92  0.88  0.77  0.89  0.81  0.98  0.87 ]
[ 0.95  0.98  0.90  0.91  0.96  0.67  0.84  0.98  0.90  0.97  0.95  0.91 ]
[ 0.97  0.98  0.98  0.85  0.97  0.99  0.85  0.92  0.96  0.97  0.91  0.79 ]

10/27 12:02:24 PM | === Batch 5/200
10/27 12:02:24 PM | loss = 0.503 + 2.349 + 0.084 = 2.936
10/27 12:02:25 PM | attribution_score[0:12]: 
[ 0.98  0.97  0.79  0.42  0.43  0.74  0.48  0.44  0.43  0.88  0.46  0.96 ]
[ 0.68  0.51  0.95  0.43  0.95  0.76  0.88  0.49  0.82  0.97  0.96  0.51 ]
[ 0.97  0.98  0.47  0.98  0.48  0.96  0.91  0.95  0.81  0.46  0.57  0.97 ]
[ 0.57  0.73  0.96  0.97  0.62  0.95  0.94  0.80  0.96  0.96  0.97  0.95 ]
[ 0.45  0.95  0.96  0.58  0.82  0.97  0.73  0.53  0.98  0.90  0.78  0.77 ]
[ 0.81  0.97  0.87  0.49  0.52  0.60  0.51  0.95  0.48  0.79  0.47  0.63 ]
[ 0.81  0.92  0.79  0.74  0.95  0.62  0.53  0.76  0.97  0.94  0.98  0.68 ]
[ 0.43  0.48  0.44  0.98  0.45  0.70  0.90  0.44  0.96  0.87  0.43  0.65 ]
[ 0.53  0.56  0.44  0.50  0.51  0.42  0.44  0.48  0.77  0.43  0.63  0.97 ]
[ 0.67  0.48  0.43  0.43  0.87  0.68  0.42  0.43  0.66  0.44  0.44  0.47 ]
[ 0.43  0.93  0.96  0.50  0.74  0.89  0.91  0.81  0.92  0.85  0.98  0.90 ]
[ 0.95  0.99  0.86  0.93  0.96  0.63  0.89  0.98  0.87  0.97  0.96  0.93 ]
[ 0.97  0.98  0.99  0.88  0.97  0.99  0.89  0.95  0.97  0.97  0.94  0.81 ]

10/27 12:02:25 PM | === Batch 6/200
10/27 12:02:25 PM | loss = 0.743 + 1.818 + 0.088 = 2.649
10/27 12:02:25 PM | attribution_score[0:12]: 
[ 0.98  0.97  0.71  0.31  0.32  0.73  0.38  0.33  0.31  0.84  0.35  0.96 ]
[ 0.62  0.40  0.95  0.31  0.94  0.76  0.85  0.39  0.83  0.97  0.96  0.48 ]
[ 0.97  0.98  0.37  0.98  0.37  0.96  0.88  0.95  0.74  0.34  0.47  0.97 ]
[ 0.53  0.72  0.96  0.97  0.59  0.96  0.93  0.79  0.96  0.95  0.98  0.94 ]
[ 0.44  0.96  0.96  0.55  0.85  0.97  0.76  0.46  0.98  0.89  0.82  0.70 ]
[ 0.83  0.97  0.83  0.39  0.42  0.62  0.53  0.94  0.38  0.82  0.36  0.61 ]
[ 0.82  0.91  0.83  0.78  0.94  0.62  0.44  0.68  0.98  0.95  0.99  0.69 ]
[ 0.31  0.37  0.34  0.98  0.35  0.72  0.86  0.33  0.96  0.84  0.32  0.64 ]
[ 0.51  0.51  0.32  0.39  0.40  0.31  0.33  0.39  0.81  0.33  0.58  0.98 ]
[ 0.66  0.39  0.32  0.32  0.83  0.70  0.31  0.31  0.71  0.33  0.34  0.37 ]
[ 0.33  0.95  0.96  0.39  0.77  0.91  0.94  0.86  0.94  0.86  0.99  0.93 ]
[ 0.95  0.99  0.81  0.95  0.96  0.64  0.92  0.99  0.82  0.97  0.96  0.95 ]
[ 0.97  0.98  0.99  0.91  0.97  0.99  0.92  0.96  0.97  0.96  0.96  0.84 ]

10/27 12:02:25 PM | === Batch 7/200
10/27 12:02:25 PM | loss = 0.970 + 1.349 + 0.078 = 2.397
10/27 12:02:26 PM | attribution_score[0:12]: 
[ 0.98  0.97  0.61  0.21  0.24  0.72  0.32  0.24  0.22  0.83  0.26  0.96 ]
[ 0.63  0.31  0.95  0.22  0.93  0.73  0.85  0.29  0.81  0.97  0.96  0.48 ]
[ 0.97  0.98  0.27  0.98  0.29  0.96  0.84  0.95  0.65  0.25  0.36  0.97 ]
[ 0.46  0.69  0.96  0.96  0.52  0.97  0.94  0.82  0.96  0.95  0.98  0.94 ]
[ 0.45  0.96  0.97  0.49  0.88  0.98  0.79  0.38  0.99  0.87  0.87  0.63 ]
[ 0.83  0.97  0.77  0.30  0.33  0.66  0.57  0.94  0.31  0.84  0.26  0.61 ]
[ 0.85  0.90  0.84  0.80  0.93  0.60  0.37  0.58  0.98  0.96  0.99  0.71 ]
[ 0.22  0.29  0.25  0.98  0.26  0.72  0.80  0.24  0.95  0.79  0.24  0.62 ]
[ 0.50  0.46  0.23  0.33  0.35  0.21  0.23  0.34  0.83  0.27  0.51  0.98 ]
[ 0.65  0.29  0.22  0.22  0.80  0.68  0.22  0.22  0.77  0.23  0.26  0.27 ]
[ 0.25  0.96  0.96  0.33  0.79  0.93  0.95  0.89  0.95  0.86  0.99  0.95 ]
[ 0.95  0.99  0.77  0.97  0.96  0.68  0.93  0.99  0.79  0.97  0.97  0.97 ]
[ 0.97  0.99  0.99  0.93  0.97  1.00  0.94  0.97  0.98  0.96  0.96  0.84 ]

10/27 12:02:26 PM | === Batch 8/200
10/27 12:02:26 PM | loss = 1.221 + 0.955 + 0.070 = 2.246
10/27 12:02:26 PM | attribution_score[0:12]: 
[ 0.99  0.97  0.51  0.14  0.18  0.69  0.25  0.16  0.14  0.85  0.18  0.96 ]
[ 0.66  0.24  0.95  0.15  0.92  0.71  0.86  0.21  0.80  0.97  0.96  0.48 ]
[ 0.97  0.98  0.19  0.98  0.21  0.96  0.79  0.96  0.55  0.17  0.27  0.97 ]
[ 0.39  0.65  0.96  0.96  0.44  0.98  0.94  0.82  0.96  0.94  0.98  0.95 ]
[ 0.47  0.95  0.97  0.43  0.89  0.98  0.80  0.30  0.99  0.84  0.90  0.62 ]
[ 0.82  0.97  0.71  0.21  0.24  0.65  0.61  0.94  0.26  0.85  0.19  0.60 ]
[ 0.88  0.89  0.85  0.80  0.93  0.56  0.31  0.47  0.98  0.96  0.99  0.75 ]
[ 0.15  0.23  0.18  0.99  0.19  0.72  0.74  0.16  0.95  0.74  0.18  0.61 ]
[ 0.48  0.48  0.16  0.28  0.37  0.14  0.16  0.33  0.82  0.21  0.45  0.98 ]
[ 0.61  0.21  0.16  0.15  0.82  0.63  0.15  0.15  0.81  0.16  0.25  0.19 ]
[ 0.21  0.97  0.96  0.26  0.77  0.95  0.96  0.92  0.96  0.83  0.99  0.95 ]
[ 0.96  1.00  0.75  0.97  0.96  0.74  0.94  0.99  0.81  0.98  0.97  0.97 ]
[ 0.98  0.99  1.00  0.94  0.97  1.00  0.95  0.98  0.98  0.96  0.97  0.81 ]

10/27 12:02:26 PM | === Batch 9/200
10/27 12:02:27 PM | loss = 1.228 + 0.632 + 0.064 = 1.924
10/27 12:02:27 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.41  0.09  0.13  0.64  0.20  0.11  0.09  0.86  0.13  0.97 ]
[ 0.70  0.18  0.95  0.10  0.90  0.67  0.88  0.15  0.76  0.97  0.96  0.50 ]
[ 0.97  0.98  0.13  0.98  0.15  0.96  0.76  0.97  0.43  0.12  0.19  0.98 ]
[ 0.31  0.59  0.96  0.96  0.38  0.98  0.94  0.82  0.96  0.93  0.98  0.95 ]
[ 0.47  0.95  0.97  0.34  0.89  0.98  0.80  0.23  0.99  0.84  0.92  0.61 ]
[ 0.81  0.96  0.63  0.15  0.17  0.64  0.61  0.93  0.22  0.83  0.13  0.56 ]
[ 0.90  0.88  0.86  0.81  0.92  0.52  0.25  0.36  0.98  0.97  0.99  0.75 ]
[ 0.10  0.20  0.13  0.99  0.13  0.71  0.67  0.11  0.95  0.69  0.16  0.59 ]
[ 0.49  0.51  0.11  0.25  0.40  0.10  0.10  0.34  0.79  0.17  0.45  0.98 ]
[ 0.56  0.15  0.12  0.11  0.85  0.58  0.11  0.11  0.86  0.12  0.25  0.14 ]
[ 0.17  0.97  0.97  0.19  0.72  0.96  0.96  0.94  0.97  0.80  0.99  0.95 ]
[ 0.96  1.00  0.79  0.98  0.97  0.79  0.94  0.99  0.83  0.98  0.98  0.98 ]
[ 0.98  0.99  1.00  0.95  0.98  1.00  0.96  0.98  0.99  0.96  0.97  0.77 ]

10/27 12:02:27 PM | === Batch 10/200
10/27 12:02:27 PM | loss = 1.245 + 0.378 + 0.059 = 1.682
10/27 12:02:27 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.32  0.06  0.09  0.60  0.15  0.07  0.06  0.85  0.09  0.97 ]
[ 0.74  0.13  0.95  0.07  0.88  0.67  0.89  0.10  0.76  0.98  0.95  0.54 ]
[ 0.97  0.98  0.09  0.98  0.11  0.96  0.71  0.97  0.34  0.08  0.13  0.98 ]
[ 0.26  0.52  0.96  0.96  0.36  0.98  0.95  0.79  0.95  0.91  0.98  0.95 ]
[ 0.44  0.96  0.97  0.27  0.89  0.98  0.79  0.17  0.99  0.85  0.94  0.61 ]
[ 0.81  0.96  0.54  0.10  0.12  0.61  0.59  0.92  0.18  0.79  0.09  0.50 ]
[ 0.89  0.85  0.87  0.81  0.92  0.49  0.20  0.27  0.98  0.97  0.99  0.74 ]
[ 0.07  0.18  0.09  0.99  0.09  0.70  0.58  0.07  0.95  0.63  0.16  0.57 ]
[ 0.52  0.57  0.08  0.25  0.42  0.07  0.07  0.38  0.74  0.14  0.50  0.99 ]
[ 0.54  0.11  0.08  0.08  0.87  0.58  0.08  0.07  0.90  0.08  0.25  0.11 ]
[ 0.13  0.97  0.98  0.15  0.68  0.97  0.96  0.95  0.98  0.78  0.99  0.96 ]
[ 0.97  1.00  0.84  0.98  0.98  0.83  0.95  0.99  0.83  0.98  0.98  0.98 ]
[ 0.99  0.99  1.00  0.95  0.98  1.00  0.97  0.98  0.99  0.96  0.97  0.73 ]

10/27 12:02:27 PM | === Batch 11/200
10/27 12:02:28 PM | loss = 1.486 + 0.191 + 0.055 = 1.732
10/27 12:02:28 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.25  0.04  0.07  0.54  0.11  0.05  0.04  0.84  0.06  0.97 ]
[ 0.78  0.10  0.96  0.05  0.85  0.69  0.90  0.07  0.76  0.98  0.95  0.56 ]
[ 0.97  0.98  0.06  0.99  0.08  0.96  0.66  0.97  0.25  0.05  0.09  0.98 ]
[ 0.22  0.44  0.96  0.96  0.35  0.98  0.94  0.75  0.95  0.89  0.98  0.96 ]
[ 0.40  0.96  0.97  0.21  0.89  0.98  0.77  0.12  0.99  0.86  0.94  0.62 ]
[ 0.81  0.96  0.44  0.07  0.09  0.58  0.55  0.90  0.15  0.75  0.06  0.44 ]
[ 0.88  0.80  0.87  0.81  0.93  0.46  0.16  0.22  0.98  0.98  1.00  0.72 ]
[ 0.04  0.18  0.07  0.99  0.07  0.70  0.52  0.05  0.95  0.55  0.16  0.55 ]
[ 0.56  0.64  0.06  0.26  0.45  0.05  0.05  0.42  0.69  0.12  0.56  0.99 ]
[ 0.54  0.08  0.06  0.05  0.89  0.58  0.06  0.05  0.92  0.06  0.24  0.08 ]
[ 0.09  0.98  0.98  0.11  0.62  0.98  0.97  0.96  0.98  0.76  1.00  0.96 ]
[ 0.97  1.00  0.88  0.98  0.98  0.86  0.95  0.99  0.82  0.98  0.99  0.99 ]
[ 0.99  0.99  1.00  0.96  0.98  1.00  0.98  0.99  0.99  0.97  0.98  0.68 ]

10/27 12:02:28 PM | === Batch 12/200
10/27 12:02:28 PM | loss = 1.407 + 0.035 + 0.052 = 1.493
10/27 12:02:28 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.18  0.03  0.05  0.48  0.09  0.03  0.03  0.81  0.04  0.97 ]
[ 0.80  0.08  0.96  0.03  0.80  0.72  0.90  0.05  0.77  0.98  0.94  0.56 ]
[ 0.97  0.98  0.04  0.99  0.05  0.96  0.60  0.97  0.18  0.04  0.06  0.98 ]
[ 0.18  0.35  0.96  0.96  0.35  0.99  0.94  0.70  0.95  0.85  0.98  0.96 ]
[ 0.37  0.96  0.96  0.16  0.88  0.98  0.75  0.09  0.99  0.87  0.95  0.61 ]
[ 0.81  0.96  0.34  0.05  0.06  0.54  0.50  0.89  0.12  0.71  0.04  0.36 ]
[ 0.86  0.75  0.86  0.80  0.92  0.41  0.12  0.19  0.98  0.98  1.00  0.70 ]
[ 0.03  0.17  0.05  0.99  0.05  0.66  0.45  0.03  0.95  0.46  0.16  0.53 ]
[ 0.59  0.71  0.04  0.28  0.47  0.03  0.04  0.46  0.65  0.10  0.62  0.99 ]
[ 0.55  0.06  0.04  0.04  0.91  0.59  0.05  0.04  0.94  0.04  0.22  0.07 ]
[ 0.07  0.98  0.99  0.10  0.62  0.98  0.97  0.97  0.99  0.78  1.00  0.97 ]
[ 0.98  1.00  0.90  0.99  0.99  0.87  0.96  1.00  0.81  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.97  0.99  1.00  0.98  0.99  0.99  0.97  0.98  0.59 ]

10/27 12:02:28 PM | === Batch 13/200
10/27 12:02:29 PM | loss = 1.451 + 0.202 + 0.049 = 1.703
10/27 12:02:29 PM | attribution_score[0:12]: 
[ 0.99  0.98  0.17  0.02  0.04  0.46  0.07  0.02  0.02  0.82  0.03  0.97 ]
[ 0.83  0.07  0.97  0.02  0.77  0.75  0.92  0.04  0.78  0.98  0.94  0.56 ]
[ 0.98  0.98  0.03  0.99  0.04  0.96  0.63  0.97  0.15  0.03  0.04  0.98 ]
[ 0.17  0.34  0.97  0.96  0.37  0.99  0.94  0.70  0.95  0.85  0.98  0.96 ]
[ 0.38  0.96  0.96  0.14  0.88  0.98  0.75  0.07  0.99  0.89  0.95  0.60 ]
[ 0.83  0.96  0.33  0.04  0.05  0.53  0.47  0.88  0.11  0.69  0.03  0.30 ]
[ 0.84  0.75  0.85  0.81  0.93  0.41  0.10  0.19  0.98  0.98  1.00  0.72 ]
[ 0.02  0.17  0.04  0.99  0.04  0.62  0.43  0.02  0.95  0.46  0.17  0.53 ]
[ 0.65  0.78  0.03  0.32  0.51  0.03  0.03  0.51  0.64  0.09  0.70  1.00 ]
[ 0.58  0.05  0.03  0.03  0.92  0.63  0.04  0.03  0.96  0.03  0.20  0.06 ]
[ 0.05  0.98  0.99  0.09  0.65  0.99  0.98  0.97  0.99  0.81  1.00  0.97 ]
[ 0.98  1.00  0.92  0.99  0.99  0.89  0.96  1.00  0.81  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.97  0.99  1.00  0.98  0.99  0.99  0.98  0.98  0.56 ]

10/27 12:02:29 PM | === Batch 14/200
10/27 12:02:29 PM | loss = 1.381 + 0.194 + 0.046 = 1.621
10/27 12:02:29 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.18  0.01  0.03  0.49  0.06  0.02  0.01  0.83  0.02  0.97 ]
[ 0.85  0.07  0.97  0.02  0.78  0.78  0.93  0.03  0.81  0.98  0.94  0.59 ]
[ 0.98  0.98  0.02  0.99  0.03  0.97  0.67  0.97  0.15  0.02  0.04  0.98 ]
[ 0.16  0.35  0.97  0.97  0.43  0.99  0.95  0.71  0.95  0.86  0.98  0.96 ]
[ 0.42  0.97  0.96  0.13  0.89  0.98  0.78  0.07  0.99  0.91  0.96  0.62 ]
[ 0.85  0.96  0.34  0.03  0.04  0.55  0.47  0.88  0.10  0.70  0.03  0.25 ]
[ 0.81  0.76  0.84  0.82  0.94  0.44  0.09  0.21  0.98  0.98  1.00  0.76 ]
[ 0.02  0.18  0.03  0.99  0.03  0.60  0.40  0.02  0.95  0.49  0.20  0.55 ]
[ 0.71  0.84  0.03  0.39  0.56  0.02  0.02  0.59  0.68  0.09  0.78  1.00 ]
[ 0.65  0.04  0.03  0.02  0.94  0.69  0.03  0.02  0.97  0.03  0.17  0.05 ]
[ 0.05  0.99  0.99  0.09  0.71  0.99  0.98  0.98  0.99  0.86  1.00  0.97 ]
[ 0.98  1.00  0.94  0.99  0.99  0.91  0.97  1.00  0.84  0.99  0.99  0.99 ]
[ 0.99  0.99  1.00  0.98  0.99  1.00  0.98  0.99  1.00  0.98  0.97  0.52 ]

10/27 12:02:30 PM | === Batch 15/200
10/27 12:02:30 PM | loss = 1.291 + 0.032 + 0.044 = 1.366
10/27 12:02:30 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.20  0.01  0.03  0.54  0.06  0.01  0.01  0.86  0.02  0.98 ]
[ 0.88  0.06  0.97  0.01  0.81  0.82  0.94  0.02  0.83  0.98  0.95  0.65 ]
[ 0.98  0.98  0.02  0.99  0.03  0.97  0.73  0.97  0.16  0.02  0.03  0.98 ]
[ 0.17  0.37  0.97  0.97  0.49  0.99  0.95  0.75  0.95  0.89  0.99  0.97 ]
[ 0.48  0.97  0.96  0.14  0.89  0.97  0.82  0.06  0.99  0.93  0.96  0.66 ]
[ 0.87  0.97  0.38  0.03  0.03  0.60  0.47  0.90  0.10  0.72  0.02  0.24 ]
[ 0.77  0.80  0.83  0.83  0.95  0.48  0.09  0.25  0.98  0.98  1.00  0.80 ]
[ 0.01  0.21  0.03  0.99  0.03  0.58  0.37  0.01  0.96  0.53  0.25  0.59 ]
[ 0.76  0.88  0.02  0.48  0.60  0.02  0.02  0.67  0.72  0.08  0.84  1.00 ]
[ 0.72  0.04  0.02  0.02  0.95  0.74  0.03  0.02  0.98  0.02  0.15  0.05 ]
[ 0.04  0.99  0.99  0.11  0.77  0.99  0.98  0.98  0.99  0.89  1.00  0.98 ]
[ 0.99  1.00  0.96  0.99  0.99  0.93  0.98  1.00  0.86  0.99  1.00  0.99 ]
[ 0.99  1.00  1.00  0.98  0.99  1.00  0.99  0.99  1.00  0.98  0.97  0.51 ]

10/27 12:02:30 PM | === Batch 16/200
10/27 12:02:30 PM | loss = 1.212 + 0.121 + 0.041 = 1.375
10/27 12:02:31 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.23  0.01  0.02  0.56  0.06  0.01  0.01  0.88  0.02  0.98 ]
[ 0.89  0.06  0.97  0.01  0.84  0.84  0.95  0.02  0.84  0.98  0.96  0.69 ]
[ 0.98  0.98  0.02  0.99  0.03  0.98  0.78  0.97  0.16  0.01  0.02  0.98 ]
[ 0.17  0.37  0.98  0.97  0.53  0.99  0.95  0.77  0.95  0.90  0.99  0.97 ]
[ 0.54  0.98  0.97  0.14  0.89  0.97  0.84  0.06  0.99  0.95  0.97  0.70 ]
[ 0.88  0.97  0.42  0.02  0.03  0.65  0.49  0.91  0.10  0.72  0.02  0.22 ]
[ 0.76  0.83  0.80  0.84  0.96  0.50  0.08  0.27  0.98  0.99  1.00  0.83 ]
[ 0.01  0.23  0.02  0.99  0.02  0.57  0.37  0.01  0.96  0.56  0.28  0.62 ]
[ 0.78  0.92  0.02  0.56  0.64  0.01  0.02  0.74  0.76  0.08  0.89  1.00 ]
[ 0.76  0.03  0.02  0.02  0.95  0.78  0.03  0.01  0.98  0.02  0.14  0.05 ]
[ 0.03  0.99  1.00  0.12  0.82  0.99  0.98  0.98  1.00  0.92  1.00  0.98 ]
[ 0.99  1.00  0.97  0.99  0.99  0.94  0.98  1.00  0.89  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  0.99  1.00  0.98  0.97  0.48 ]

10/27 12:02:31 PM | === Batch 17/200
10/27 12:02:31 PM | loss = 1.316 + 0.223 + 0.039 = 1.579
10/27 12:02:31 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.25  0.01  0.02  0.58  0.05  0.01  0.01  0.90  0.01  0.98 ]
[ 0.91  0.06  0.97  0.01  0.85  0.85  0.96  0.02  0.84  0.98  0.96  0.72 ]
[ 0.98  0.98  0.02  0.99  0.02  0.98  0.80  0.98  0.16  0.01  0.02  0.98 ]
[ 0.16  0.35  0.98  0.97  0.58  0.99  0.95  0.75  0.96  0.90  0.99  0.97 ]
[ 0.59  0.98  0.97  0.14  0.89  0.98  0.86  0.05  0.99  0.96  0.97  0.73 ]
[ 0.88  0.97  0.46  0.02  0.03  0.69  0.53  0.92  0.11  0.75  0.01  0.20 ]
[ 0.76  0.87  0.78  0.85  0.96  0.52  0.07  0.27  0.99  0.99  1.00  0.87 ]
[ 0.01  0.23  0.02  0.99  0.02  0.55  0.35  0.01  0.96  0.59  0.29  0.65 ]
[ 0.81  0.94  0.02  0.60  0.69  0.01  0.02  0.79  0.79  0.07  0.92  1.00 ]
[ 0.79  0.03  0.02  0.01  0.96  0.80  0.02  0.01  0.99  0.02  0.13  0.05 ]
[ 0.03  0.99  1.00  0.13  0.85  0.99  0.99  0.98  1.00  0.94  1.00  0.98 ]
[ 0.99  1.00  0.97  0.99  0.99  0.95  0.98  1.00  0.90  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.96  0.45 ]

10/27 12:02:31 PM | === Batch 18/200
10/27 12:02:31 PM | loss = 1.275 + 0.289 + 0.037 = 1.601
10/27 12:02:32 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.27  0.01  0.02  0.59  0.05  0.01  0.01  0.91  0.01  0.98 ]
[ 0.91  0.05  0.97  0.01  0.86  0.85  0.97  0.01  0.84  0.98  0.97  0.73 ]
[ 0.98  0.98  0.01  0.99  0.02  0.98  0.83  0.98  0.15  0.01  0.02  0.98 ]
[ 0.15  0.33  0.98  0.97  0.59  0.99  0.95  0.74  0.96  0.91  0.99  0.97 ]
[ 0.64  0.98  0.96  0.15  0.89  0.98  0.88  0.05  1.00  0.97  0.97  0.73 ]
[ 0.88  0.97  0.49  0.02  0.02  0.73  0.57  0.92  0.12  0.80  0.01  0.17 ]
[ 0.78  0.90  0.74  0.86  0.96  0.55  0.07  0.24  0.99  0.99  1.00  0.89 ]
[ 0.01  0.22  0.02  0.99  0.02  0.57  0.36  0.01  0.95  0.62  0.29  0.67 ]
[ 0.84  0.95  0.01  0.62  0.73  0.01  0.01  0.82  0.82  0.06  0.94  1.00 ]
[ 0.79  0.03  0.01  0.01  0.97  0.81  0.02  0.01  0.99  0.01  0.13  0.05 ]
[ 0.03  0.99  1.00  0.13  0.88  0.99  0.99  0.98  1.00  0.95  1.00  0.98 ]
[ 0.99  1.00  0.97  0.99  1.00  0.96  0.98  1.00  0.91  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.96  0.42 ]

10/27 12:02:32 PM | === Batch 19/200
10/27 12:02:32 PM | loss = 1.005 + 0.326 + 0.036 = 1.367
10/27 12:02:32 PM | attribution_score[0:12]: 
[ 0.99  0.99  0.28  0.00  0.02  0.60  0.04  0.01  0.00  0.92  0.01  0.99 ]
[ 0.92  0.05  0.97  0.01  0.88  0.85  0.98  0.01  0.83  0.98  0.97  0.73 ]
[ 0.98  0.98  0.01  0.99  0.02  0.98  0.85  0.98  0.14  0.01  0.02  0.98 ]
[ 0.13  0.31  0.98  0.97  0.59  0.99  0.94  0.70  0.96  0.91  0.99  0.96 ]
[ 0.69  0.99  0.96  0.15  0.88  0.98  0.89  0.05  1.00  0.97  0.97  0.73 ]
[ 0.87  0.96  0.53  0.02  0.02  0.78  0.60  0.92  0.13  0.83  0.01  0.16 ]
[ 0.81  0.92  0.68  0.86  0.96  0.54  0.06  0.21  0.99  0.99  1.00  0.92 ]
[ 0.01  0.21  0.02  0.99  0.01  0.60  0.36  0.01  0.94  0.66  0.27  0.68 ]
[ 0.86  0.96  0.01  0.61  0.78  0.01  0.01  0.84  0.83  0.05  0.95  1.00 ]
[ 0.80  0.02  0.01  0.01  0.97  0.80  0.02  0.01  0.99  0.01  0.13  0.04 ]
[ 0.03  0.99  1.00  0.13  0.89  0.99  0.99  0.99  1.00  0.96  1.00  0.98 ]
[ 0.99  1.00  0.98  0.99  1.00  0.96  0.98  1.00  0.92  0.99  1.00  1.00 ]
[ 1.00  1.00  1.00  0.98  0.99  1.00  0.99  1.00  1.00  0.98  0.96  0.43 ]

10/27 12:02:32 PM | === Batch 20/200
10/27 12:02:33 PM | loss = 0.969 + 0.341 + 0.035 = 1.345
10/27 12:02:33 PM | === Batch 21/200
10/27 12:02:33 PM | loss = 1.091 + 0.342 + 0.034 = 1.467
10/27 12:02:33 PM | === Batch 22/200
10/27 12:02:34 PM | loss = 1.048 + 0.331 + 0.033 = 1.412
10/27 12:02:34 PM | === Batch 23/200
10/27 12:02:34 PM | loss = 0.857 + 0.311 + 0.032 = 1.200
10/27 12:02:35 PM | === Batch 24/200
10/27 12:02:35 PM | loss = 1.073 + 0.284 + 0.031 = 1.388
10/27 12:02:35 PM | === Batch 25/200
10/27 12:02:35 PM | loss = 1.028 + 0.251 + 0.031 = 1.311
10/27 12:02:36 PM | === Batch 26/200
10/27 12:02:36 PM | loss = 0.971 + 0.214 + 0.030 = 1.216
10/27 12:02:36 PM | === Batch 27/200
10/27 12:02:36 PM | loss = 1.039 + 0.180 + 0.030 = 1.249
10/27 12:02:37 PM | === Batch 28/200
10/27 12:02:37 PM | loss = 1.234 + 0.146 + 0.029 = 1.409
10/27 12:02:37 PM | === Batch 29/200
10/27 12:02:38 PM | loss = 0.964 + 0.119 + 0.029 = 1.111
10/27 12:02:38 PM | === Batch 30/200
10/27 12:02:38 PM | loss = 0.954 + 0.089 + 0.028 = 1.071
10/27 12:02:38 PM | === Batch 31/200
10/27 12:02:39 PM | loss = 1.044 + 0.058 + 0.028 = 1.130
10/27 12:02:39 PM | === Batch 32/200
10/27 12:02:39 PM | loss = 1.002 + 0.028 + 0.028 = 1.058
10/27 12:02:39 PM | === Batch 33/200
10/27 12:02:40 PM | loss = 1.033 + 0.005 + 0.028 = 1.066
10/27 12:02:40 PM | === Batch 34/200
10/27 12:02:40 PM | loss = 1.020 + 0.014 + 0.027 = 1.061
10/27 12:02:41 PM | === Batch 35/200
10/27 12:02:41 PM | loss = 1.054 + 0.022 + 0.026 = 1.102
10/27 12:02:41 PM | === Batch 36/200
10/27 12:02:41 PM | loss = 0.910 + 0.021 + 0.026 = 0.956
10/27 12:02:42 PM | === Batch 37/200
10/27 12:02:42 PM | loss = 0.945 + 0.015 + 0.025 = 0.985
10/27 12:02:42 PM | === Batch 38/200
10/27 12:02:42 PM | loss = 1.225 + 0.001 + 0.025 = 1.251
10/27 12:02:43 PM | === Batch 39/200
10/27 12:02:43 PM | loss = 0.840 + 0.044 + 0.025 = 0.908
10/27 12:02:43 PM | === Batch 40/200
10/27 12:02:44 PM | loss = 1.052 + 0.020 + 0.024 = 1.096
10/27 12:02:44 PM | === Batch 41/200
10/27 12:02:44 PM | loss = 1.027 + 0.031 + 0.024 = 1.081
10/27 12:02:45 PM | === Batch 42/200
10/27 12:02:45 PM | loss = 0.742 + 0.058 + 0.023 = 0.823
10/27 12:02:45 PM | === Batch 43/200
10/27 12:02:45 PM | loss = 0.809 + 0.072 + 0.023 = 0.904
10/27 12:02:46 PM | === Batch 44/200
10/27 12:02:46 PM | loss = 0.842 + 0.076 + 0.023 = 0.941
10/27 12:02:46 PM | === Batch 45/200
10/27 12:02:46 PM | loss = 0.978 + 0.069 + 0.023 = 1.070
10/27 12:02:47 PM | === Batch 46/200
10/27 12:02:47 PM | loss = 0.920 + 0.055 + 0.023 = 0.998
10/27 12:02:47 PM | === Batch 47/200
10/27 12:02:47 PM | loss = 0.820 + 0.034 + 0.022 = 0.877
10/27 12:02:48 PM | === Batch 48/200
10/27 12:02:48 PM | loss = 0.825 + 0.008 + 0.022 = 0.855
10/27 12:02:48 PM | === Batch 49/200
10/27 12:02:49 PM | loss = 0.852 + 0.047 + 0.022 = 0.921
10/27 12:02:49 PM | === Batch 50/200
10/27 12:02:49 PM | loss = 0.940 + 0.038 + 0.022 = 1.001
10/27 12:02:49 PM | === Batch 51/200
10/27 12:02:50 PM | loss = 0.850 + 0.016 + 0.022 = 0.888
10/27 12:02:50 PM | === Batch 52/200
10/27 12:02:50 PM | loss = 0.952 + 0.042 + 0.021 = 1.015
10/27 12:02:51 PM | === Batch 53/200
10/27 12:02:51 PM | loss = 0.821 + 0.058 + 0.021 = 0.899
10/27 12:02:51 PM | === Batch 54/200
10/27 12:02:51 PM | loss = 0.982 + 0.065 + 0.021 = 1.068
10/27 12:02:52 PM | === Batch 55/200
10/27 12:02:52 PM | loss = 0.914 + 0.063 + 0.021 = 0.998
10/27 12:02:52 PM | === Batch 56/200
10/27 12:02:53 PM | loss = 0.801 + 0.055 + 0.021 = 0.877
10/27 12:02:53 PM | === Batch 57/200
10/27 12:02:53 PM | loss = 0.814 + 0.040 + 0.021 = 0.874
10/27 12:02:53 PM | === Batch 58/200
10/27 12:02:54 PM | loss = 0.948 + 0.023 + 0.021 = 0.992
10/27 12:02:54 PM | === Batch 59/200
10/27 12:02:54 PM | loss = 0.849 + 0.005 + 0.020 = 0.874
10/27 12:02:55 PM | === Batch 60/200
10/27 12:02:55 PM | loss = 0.924 + 0.028 + 0.020 = 0.973
10/27 12:02:55 PM | === Batch 61/200
10/27 12:02:55 PM | loss = 0.955 + 0.007 + 0.020 = 0.982
10/27 12:02:56 PM | === Batch 62/200
10/27 12:02:56 PM | loss = 1.015 + 0.037 + 0.019 = 1.071
10/27 12:02:56 PM | === Batch 63/200
10/27 12:02:56 PM | loss = 0.975 + 0.069 + 0.019 = 1.062
10/27 12:02:57 PM | === Batch 64/200
10/27 12:02:57 PM | loss = 0.829 + 0.092 + 0.019 = 0.939
10/27 12:02:57 PM | === Batch 65/200
10/27 12:02:58 PM | loss = 0.861 + 0.107 + 0.018 = 0.987
10/27 12:02:58 PM | === Batch 66/200
10/27 12:02:58 PM | loss = 0.746 + 0.114 + 0.018 = 0.878
10/27 12:02:58 PM | === Batch 67/200
10/27 12:02:59 PM | loss = 0.877 + 0.114 + 0.018 = 1.008
10/27 12:02:59 PM | === Batch 68/200
10/27 12:02:59 PM | loss = 0.732 + 0.107 + 0.018 = 0.857
10/27 12:03:00 PM | === Batch 69/200
10/27 12:03:00 PM | loss = 0.818 + 0.094 + 0.018 = 0.930
10/27 12:03:00 PM | === Batch 70/200
10/27 12:03:01 PM | loss = 0.872 + 0.077 + 0.018 = 0.966
10/27 12:03:01 PM | === Batch 71/200
10/27 12:03:01 PM | loss = 0.680 + 0.056 + 0.018 = 0.754
10/27 12:03:01 PM | === Batch 72/200
10/27 12:03:02 PM | loss = 0.845 + 0.031 + 0.018 = 0.894
10/27 12:03:02 PM | === Batch 73/200
10/27 12:03:02 PM | loss = 0.753 + 0.001 + 0.018 = 0.772
10/27 12:03:03 PM | === Batch 74/200
10/27 12:03:03 PM | loss = 0.722 + 0.057 + 0.018 = 0.797
10/27 12:03:03 PM | === Batch 75/200
10/27 12:03:03 PM | loss = 0.741 + 0.057 + 0.017 = 0.815
10/27 12:03:04 PM | === Batch 76/200
10/27 12:03:04 PM | loss = 0.854 + 0.009 + 0.017 = 0.881
10/27 12:03:04 PM | === Batch 77/200
10/27 12:03:05 PM | loss = 0.841 + 0.040 + 0.017 = 0.897
10/27 12:03:05 PM | === Batch 78/200
10/27 12:03:05 PM | loss = 0.867 + 0.075 + 0.017 = 0.959
10/27 12:03:06 PM | === Batch 79/200
10/27 12:03:06 PM | loss = 0.823 + 0.102 + 0.016 = 0.941
10/27 12:03:06 PM | === Batch 80/200
10/27 12:03:07 PM | loss = 0.754 + 0.121 + 0.016 = 0.890
10/27 12:03:07 PM | === Batch 81/200
10/27 12:03:07 PM | loss = 0.667 + 0.132 + 0.016 = 0.814
10/27 12:03:07 PM | === Batch 82/200
10/27 12:03:08 PM | loss = 0.631 + 0.138 + 0.016 = 0.784
10/27 12:03:08 PM | === Batch 83/200
10/27 12:03:08 PM | loss = 0.672 + 0.137 + 0.016 = 0.824
10/27 12:03:08 PM | === Batch 84/200
10/27 12:03:09 PM | loss = 0.817 + 0.131 + 0.016 = 0.964
10/27 12:03:09 PM | === Batch 85/200
10/27 12:03:09 PM | loss = 1.022 + 0.120 + 0.015 = 1.158
10/27 12:03:10 PM | === Batch 86/200
10/27 12:03:10 PM | loss = 0.567 + 0.109 + 0.015 = 0.691
10/27 12:03:10 PM | === Batch 87/200
10/27 12:03:10 PM | loss = 0.834 + 0.092 + 0.015 = 0.941
10/27 12:03:11 PM | === Batch 88/200
10/27 12:03:11 PM | loss = 0.706 + 0.075 + 0.015 = 0.796
10/27 12:03:11 PM | === Batch 89/200
10/27 12:03:11 PM | loss = 0.833 + 0.054 + 0.015 = 0.903
10/27 12:03:12 PM | === Batch 90/200
10/27 12:03:12 PM | loss = 0.836 + 0.034 + 0.015 = 0.885
10/27 12:03:12 PM | === Batch 91/200
10/27 12:03:12 PM | loss = 0.708 + 0.013 + 0.015 = 0.736
10/27 12:03:13 PM | === Batch 92/200
10/27 12:03:13 PM | loss = 0.801 + 0.019 + 0.015 = 0.835
10/27 12:03:13 PM | === Batch 93/200
10/27 12:03:14 PM | loss = 0.705 + 0.015 + 0.015 = 0.734
10/27 12:03:14 PM | === Batch 94/200
10/27 12:03:14 PM | loss = 0.619 + 0.015 + 0.015 = 0.648
10/27 12:03:14 PM | === Batch 95/200
10/27 12:03:15 PM | loss = 0.707 + 0.032 + 0.014 = 0.753
10/27 12:03:15 PM | === Batch 96/200
10/27 12:03:15 PM | loss = 0.828 + 0.044 + 0.014 = 0.886
10/27 12:03:15 PM | === Batch 97/200
10/27 12:03:16 PM | loss = 0.845 + 0.051 + 0.014 = 0.910
10/27 12:03:16 PM | === Batch 98/200
10/27 12:03:16 PM | loss = 0.780 + 0.054 + 0.014 = 0.848
10/27 12:03:17 PM | === Batch 99/200
10/27 12:03:17 PM | loss = 0.780 + 0.052 + 0.014 = 0.846
10/27 12:03:17 PM | === Batch 100/200
10/27 12:03:17 PM | loss = 0.663 + 0.047 + 0.014 = 0.723
10/27 12:03:18 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.13  0.00  0.00  0.45  0.01  0.00  0.00  0.99  0.00  0.99 ]
[ 0.99  0.01  1.00  0.00  0.98  0.93  1.00  0.00  0.90  1.00  0.99  0.67 ]
[ 1.00  0.98  0.00  1.00  0.00  1.00  0.06  0.99  0.01  0.00  0.00  0.99 ]
[ 0.15  0.03  1.00  0.98  0.76  0.99  0.85  0.40  0.52  0.85  1.00  0.97 ]
[ 0.80  0.99  1.00  0.05  0.58  0.98  0.11  0.01  1.00  0.92  0.94  0.99 ]
[ 0.93  1.00  0.20  0.00  0.01  0.81  0.11  0.97  0.03  0.84  0.00  0.32 ]
[ 0.94  0.99  0.04  0.99  0.99  0.16  0.01  0.02  0.99  1.00  1.00  1.00 ]
[ 0.00  0.08  0.01  1.00  0.00  0.96  0.97  0.00  0.99  0.99  0.01  0.95 ]
[ 1.00  1.00  0.00  0.88  0.99  0.00  0.00  0.95  0.95  0.01  1.00  1.00 ]
[ 0.89  0.00  0.00  0.00  1.00  0.96  0.00  0.00  1.00  0.00  0.02  0.01 ]
[ 0.00  1.00  1.00  0.01  0.98  1.00  1.00  1.00  1.00  0.99  1.00  1.00 ]
[ 1.00  1.00  0.99  1.00  1.00  0.98  1.00  1.00  0.01  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.99  0.01 ]

10/27 12:03:18 PM | === Batch 101/200
10/27 12:03:18 PM | loss = 0.757 + 0.036 + 0.014 = 0.807
10/27 12:03:18 PM | === Batch 102/200
10/27 12:03:19 PM | loss = 0.748 + 0.023 + 0.014 = 0.785
10/27 12:03:19 PM | === Batch 103/200
10/27 12:03:19 PM | loss = 0.726 + 0.009 + 0.014 = 0.748
10/27 12:03:19 PM | === Batch 104/200
10/27 12:03:20 PM | loss = 0.698 + 0.015 + 0.014 = 0.728
10/27 12:03:20 PM | === Batch 105/200
10/27 12:03:20 PM | loss = 0.648 + 0.008 + 0.014 = 0.670
10/27 12:03:20 PM | === Batch 106/200
10/27 12:03:21 PM | loss = 0.781 + 0.018 + 0.014 = 0.813
10/27 12:03:21 PM | === Batch 107/200
10/27 12:03:21 PM | loss = 0.861 + 0.031 + 0.013 = 0.906
10/27 12:03:21 PM | === Batch 108/200
10/27 12:03:22 PM | loss = 0.806 + 0.038 + 0.013 = 0.857
10/27 12:03:22 PM | === Batch 109/200
10/27 12:03:22 PM | loss = 0.596 + 0.038 + 0.013 = 0.647
10/27 12:03:23 PM | === Batch 110/200
10/27 12:03:23 PM | loss = 0.562 + 0.034 + 0.013 = 0.609
10/27 12:03:23 PM | === Batch 111/200
10/27 12:03:23 PM | loss = 0.844 + 0.027 + 0.013 = 0.884
10/27 12:03:24 PM | === Batch 112/200
10/27 12:03:24 PM | loss = 0.509 + 0.022 + 0.013 = 0.544
10/27 12:03:24 PM | === Batch 113/200
10/27 12:03:24 PM | loss = 0.774 + 0.014 + 0.013 = 0.802
10/27 12:03:25 PM | === Batch 114/200
10/27 12:03:25 PM | loss = 0.728 + 0.005 + 0.013 = 0.746
10/27 12:03:25 PM | === Batch 115/200
10/27 12:03:25 PM | loss = 0.678 + 0.016 + 0.013 = 0.707
10/27 12:03:26 PM | === Batch 116/200
10/27 12:03:26 PM | loss = 0.709 + 0.001 + 0.013 = 0.724
10/27 12:03:26 PM | === Batch 117/200
10/27 12:03:26 PM | loss = 0.773 + 0.008 + 0.013 = 0.794
10/27 12:03:27 PM | === Batch 118/200
10/27 12:03:27 PM | loss = 0.810 + 0.012 + 0.013 = 0.835
10/27 12:03:27 PM | === Batch 119/200
10/27 12:03:27 PM | loss = 0.731 + 0.012 + 0.013 = 0.756
10/27 12:03:28 PM | === Batch 120/200
10/27 12:03:28 PM | loss = 0.661 + 0.007 + 0.013 = 0.681
10/27 12:03:28 PM | === Batch 121/200
10/27 12:03:28 PM | loss = 0.824 + 0.005 + 0.013 = 0.843
10/27 12:03:29 PM | === Batch 122/200
10/27 12:03:29 PM | loss = 0.811 + 0.007 + 0.013 = 0.831
10/27 12:03:29 PM | === Batch 123/200
10/27 12:03:29 PM | loss = 0.748 + 0.011 + 0.013 = 0.773
10/27 12:03:30 PM | === Batch 124/200
10/27 12:03:30 PM | loss = 0.577 + 0.011 + 0.013 = 0.601
10/27 12:03:30 PM | === Batch 125/200
10/27 12:03:30 PM | loss = 0.734 + 0.005 + 0.013 = 0.752
10/27 12:03:31 PM | === Batch 126/200
10/27 12:03:31 PM | loss = 0.764 + 0.010 + 0.013 = 0.787
10/27 12:03:31 PM | === Batch 127/200
10/27 12:03:31 PM | loss = 0.536 + 0.005 + 0.013 = 0.555
10/27 12:03:32 PM | === Batch 128/200
10/27 12:03:32 PM | loss = 0.716 + 0.013 + 0.013 = 0.742
10/27 12:03:32 PM | === Batch 129/200
10/27 12:03:32 PM | loss = 0.668 + 0.016 + 0.013 = 0.697
10/27 12:03:33 PM | === Batch 130/200
10/27 12:03:33 PM | loss = 0.595 + 0.014 + 0.013 = 0.622
10/27 12:03:33 PM | === Batch 131/200
10/27 12:03:33 PM | loss = 0.704 + 0.007 + 0.013 = 0.723
10/27 12:03:34 PM | === Batch 132/200
10/27 12:03:34 PM | loss = 0.628 + 0.009 + 0.013 = 0.649
10/27 12:03:34 PM | === Batch 133/200
10/27 12:03:34 PM | loss = 0.813 + 0.002 + 0.013 = 0.829
10/27 12:03:35 PM | === Batch 134/200
10/27 12:03:35 PM | loss = 0.668 + 0.005 + 0.012 = 0.685
10/27 12:03:35 PM | === Batch 135/200
10/27 12:03:36 PM | loss = 0.448 + 0.004 + 0.012 = 0.464
10/27 12:03:36 PM | === Batch 136/200
10/27 12:03:36 PM | loss = 0.659 + 0.001 + 0.012 = 0.672
10/27 12:03:36 PM | === Batch 137/200
10/27 12:03:37 PM | loss = 0.679 + 0.016 + 0.012 = 0.707
10/27 12:03:37 PM | === Batch 138/200
10/27 12:03:37 PM | loss = 0.631 + 0.025 + 0.012 = 0.668
10/27 12:03:37 PM | === Batch 139/200
10/27 12:03:38 PM | loss = 0.829 + 0.029 + 0.012 = 0.870
10/27 12:03:38 PM | === Batch 140/200
10/27 12:03:38 PM | loss = 0.558 + 0.027 + 0.012 = 0.597
10/27 12:03:38 PM | === Batch 141/200
10/27 12:03:39 PM | loss = 0.685 + 0.021 + 0.012 = 0.718
10/27 12:03:39 PM | === Batch 142/200
10/27 12:03:39 PM | loss = 0.690 + 0.013 + 0.012 = 0.715
10/27 12:03:39 PM | === Batch 143/200
10/27 12:03:40 PM | loss = 0.700 + 0.003 + 0.012 = 0.715
10/27 12:03:40 PM | === Batch 144/200
10/27 12:03:40 PM | loss = 0.700 + 0.021 + 0.012 = 0.733
10/27 12:03:41 PM | === Batch 145/200
10/27 12:03:41 PM | loss = 0.670 + 0.007 + 0.012 = 0.689
10/27 12:03:41 PM | === Batch 146/200
10/27 12:03:41 PM | loss = 0.682 + 0.021 + 0.012 = 0.716
10/27 12:03:42 PM | === Batch 147/200
10/27 12:03:42 PM | loss = 0.717 + 0.040 + 0.012 = 0.770
10/27 12:03:42 PM | === Batch 148/200
10/27 12:03:42 PM | loss = 0.545 + 0.052 + 0.012 = 0.609
10/27 12:03:43 PM | === Batch 149/200
10/27 12:03:43 PM | loss = 0.677 + 0.057 + 0.012 = 0.747
10/27 12:03:43 PM | === Batch 150/200
10/27 12:03:43 PM | loss = 0.670 + 0.059 + 0.012 = 0.741
10/27 12:03:44 PM | === Batch 151/200
10/27 12:03:44 PM | loss = 0.847 + 0.055 + 0.012 = 0.915
10/27 12:03:44 PM | === Batch 152/200
10/27 12:03:45 PM | loss = 0.552 + 0.046 + 0.012 = 0.610
10/27 12:03:45 PM | === Batch 153/200
10/27 12:03:45 PM | loss = 0.562 + 0.032 + 0.012 = 0.606
10/27 12:03:45 PM | === Batch 154/200
10/27 12:03:46 PM | loss = 0.701 + 0.015 + 0.012 = 0.728
10/27 12:03:46 PM | === Batch 155/200
10/27 12:03:46 PM | loss = 0.749 + 0.008 + 0.012 = 0.770
10/27 12:03:47 PM | === Batch 156/200
10/27 12:03:47 PM | loss = 0.598 + 0.007 + 0.012 = 0.618
10/27 12:03:47 PM | === Batch 157/200
10/27 12:03:48 PM | loss = 0.600 + 0.016 + 0.012 = 0.627
10/27 12:03:48 PM | === Batch 158/200
10/27 12:03:48 PM | loss = 0.636 + 0.028 + 0.012 = 0.676
10/27 12:03:48 PM | === Batch 159/200
10/27 12:03:49 PM | loss = 0.529 + 0.032 + 0.012 = 0.573
10/27 12:03:49 PM | === Batch 160/200
10/27 12:03:49 PM | loss = 0.634 + 0.035 + 0.012 = 0.680
10/27 12:03:50 PM | === Batch 161/200
10/27 12:03:50 PM | loss = 0.801 + 0.037 + 0.011 = 0.850
10/27 12:03:50 PM | === Batch 162/200
10/27 12:03:50 PM | loss = 0.570 + 0.038 + 0.011 = 0.620
10/27 12:03:51 PM | === Batch 163/200
10/27 12:03:51 PM | loss = 0.760 + 0.036 + 0.011 = 0.807
10/27 12:03:51 PM | === Batch 164/200
10/27 12:03:51 PM | loss = 0.682 + 0.031 + 0.011 = 0.725
10/27 12:03:52 PM | === Batch 165/200
10/27 12:03:52 PM | loss = 0.658 + 0.023 + 0.011 = 0.692
10/27 12:03:52 PM | === Batch 166/200
10/27 12:03:52 PM | loss = 0.572 + 0.010 + 0.011 = 0.593
10/27 12:03:53 PM | === Batch 167/200
10/27 12:03:53 PM | loss = 0.645 + 0.011 + 0.011 = 0.667
10/27 12:03:53 PM | === Batch 168/200
10/27 12:03:53 PM | loss = 0.596 + 0.005 + 0.011 = 0.613
10/27 12:03:54 PM | === Batch 169/200
10/27 12:03:54 PM | loss = 0.801 + 0.015 + 0.011 = 0.827
10/27 12:03:54 PM | === Batch 170/200
10/27 12:03:54 PM | loss = 0.574 + 0.026 + 0.011 = 0.611
10/27 12:03:55 PM | === Batch 171/200
10/27 12:03:55 PM | loss = 0.711 + 0.030 + 0.011 = 0.752
10/27 12:03:55 PM | === Batch 172/200
10/27 12:03:56 PM | loss = 0.543 + 0.030 + 0.011 = 0.584
10/27 12:03:56 PM | === Batch 173/200
10/27 12:03:56 PM | loss = 0.524 + 0.026 + 0.011 = 0.561
10/27 12:03:56 PM | === Batch 174/200
10/27 12:03:57 PM | loss = 0.634 + 0.020 + 0.011 = 0.665
10/27 12:03:57 PM | === Batch 175/200
10/27 12:03:57 PM | loss = 0.580 + 0.010 + 0.011 = 0.601
10/27 12:03:57 PM | === Batch 176/200
10/27 12:03:58 PM | loss = 0.516 + 0.001 + 0.011 = 0.528
10/27 12:03:58 PM | === Batch 177/200
10/27 12:03:58 PM | loss = 0.564 + 0.009 + 0.011 = 0.584
10/27 12:03:58 PM | === Batch 178/200
10/27 12:03:59 PM | loss = 0.487 + 0.015 + 0.011 = 0.514
10/27 12:03:59 PM | === Batch 179/200
10/27 12:03:59 PM | loss = 0.598 + 0.016 + 0.011 = 0.626
10/27 12:03:59 PM | === Batch 180/200
10/27 12:04:00 PM | loss = 0.651 + 0.013 + 0.011 = 0.675
10/27 12:04:00 PM | === Batch 181/200
10/27 12:04:00 PM | loss = 0.719 + 0.008 + 0.011 = 0.738
10/27 12:04:00 PM | === Batch 182/200
10/27 12:04:01 PM | loss = 0.510 + 0.000 + 0.011 = 0.522
10/27 12:04:01 PM | === Batch 183/200
10/27 12:04:01 PM | loss = 0.401 + 0.019 + 0.011 = 0.431
10/27 12:04:02 PM | === Batch 184/200
10/27 12:04:02 PM | loss = 0.624 + 0.004 + 0.011 = 0.640
10/27 12:04:02 PM | === Batch 185/200
10/27 12:04:02 PM | loss = 0.495 + 0.021 + 0.011 = 0.527
10/27 12:04:03 PM | === Batch 186/200
10/27 12:04:03 PM | loss = 0.725 + 0.038 + 0.011 = 0.773
10/27 12:04:03 PM | === Batch 187/200
10/27 12:04:04 PM | loss = 0.575 + 0.048 + 0.010 = 0.634
10/27 12:04:04 PM | === Batch 188/200
10/27 12:04:04 PM | loss = 0.472 + 0.052 + 0.010 = 0.534
10/27 12:04:04 PM | === Batch 189/200
10/27 12:04:05 PM | loss = 0.493 + 0.053 + 0.010 = 0.557
10/27 12:04:05 PM | === Batch 190/200
10/27 12:04:05 PM | loss = 0.747 + 0.050 + 0.010 = 0.807
10/27 12:04:06 PM | === Batch 191/200
10/27 12:04:06 PM | loss = 0.442 + 0.044 + 0.010 = 0.497
10/27 12:04:06 PM | === Batch 192/200
10/27 12:04:06 PM | loss = 0.600 + 0.036 + 0.010 = 0.646
10/27 12:04:07 PM | === Batch 193/200
10/27 12:04:07 PM | loss = 0.561 + 0.024 + 0.011 = 0.596
10/27 12:04:07 PM | === Batch 194/200
10/27 12:04:07 PM | loss = 0.450 + 0.011 + 0.011 = 0.472
10/27 12:04:08 PM | === Batch 195/200
10/27 12:04:08 PM | loss = 0.570 + 0.008 + 0.011 = 0.588
10/27 12:04:08 PM | === Batch 196/200
10/27 12:04:09 PM | loss = 0.613 + 0.000 + 0.010 = 0.624
10/27 12:04:09 PM | === Batch 197/200
10/27 12:04:09 PM | loss = 0.614 + 0.020 + 0.010 = 0.645
10/27 12:04:09 PM | === Batch 198/200
10/27 12:04:10 PM | loss = 0.486 + 0.035 + 0.010 = 0.531
10/27 12:04:10 PM | === Batch 199/200
10/27 12:04:10 PM | loss = 0.497 + 0.044 + 0.010 = 0.550
10/27 12:04:10 PM | === Batch 200/200
10/27 12:04:11 PM | loss = 0.737 + 0.048 + 0.010 = 0.795
10/27 12:04:11 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.04  0.00  0.00  0.81  0.00  0.00  0.00  0.99  0.00  1.00 ]
[ 0.99  0.00  1.00  0.00  0.99  0.97  1.00  0.00  0.96  1.00  1.00  0.31 ]
[ 1.00  0.99  0.00  1.00  0.00  1.00  0.06  0.99  0.00  0.00  0.00  0.99 ]
[ 0.26  0.04  1.00  1.00  0.93  0.99  0.95  0.69  0.25  0.31  1.00  0.92 ]
[ 0.90  0.16  1.00  0.74  0.94  0.99  0.09  0.00  1.00  0.99  0.99  0.99 ]
[ 0.99  1.00  0.04  0.00  0.01  0.98  0.04  0.93  0.02  0.17  0.00  0.94 ]
[ 0.97  1.00  0.02  0.99  1.00  0.10  0.01  0.01  0.97  1.00  1.00  1.00 ]
[ 0.00  0.01  0.00  1.00  0.00  0.99  0.99  0.00  1.00  1.00  0.02  0.93 ]
[ 1.00  1.00  0.00  0.98  1.00  0.00  0.00  0.99  0.98  0.02  1.00  1.00 ]
[ 0.95  0.00  0.00  0.00  1.00  0.98  0.00  0.00  1.00  0.00  0.03  0.01 ]
[ 0.00  1.00  1.00  0.01  0.99  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.99  1.00  1.00  0.99  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00 ]

10/27 12:04:11 PM | ===
Best loss was 0.43 at iteration 183

10/27 12:04:11 PM | Looking for optimal threshold...
10/27 12:04:11 PM | Testing threshold 0.5
10/27 12:04:11 PM | Distance to target: 805,664
10/27 12:04:11 PM | Testing threshold 0.75
10/27 12:04:11 PM | Distance to target: 5,930,608
10/27 12:04:11 PM | Testing threshold 0.625
10/27 12:04:11 PM | Distance to target: 1,873,600
10/27 12:04:11 PM | Testing threshold 0.5625
10/27 12:04:11 PM | Distance to target: 864,688
10/27 12:04:11 PM | Testing threshold 0.53125
10/27 12:04:11 PM | Distance to target: 485,584
10/27 12:04:11 PM | Testing threshold 0.546875
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5390625
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.53515625
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.537109375
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5361328125
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.53564453125
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.535888671875
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5360107421875
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.53607177734375
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.536102294921875
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361175537109375
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5361099243164062
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361137390136719
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361156463623047
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361166000366211
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361170768737793
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5361168384552002
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5361167192459106
10/27 12:04:11 PM | Distance to target: 55,728
10/27 12:04:11 PM | Testing threshold 0.5361166596412659
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361166894435883
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167043447495
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.53611671179533
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167155206203
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167173832655
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167183145881
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167187802494
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.53611671901308
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167191294953
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.536116719187703
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192168068
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192313587
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192386347
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192422727
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192440917
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192450011
10/27 12:04:11 PM | Distance to target: 214,928
10/27 12:04:11 PM | Testing threshold 0.5361167192454559
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192456833
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.536116719245797
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192458538
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192458822
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192458964
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192459035
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192459071
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192459089
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:12 PM | Testing threshold 0.5361167192459098
10/27 12:04:12 PM | Distance to target: 214,928
10/27 12:04:13 PM | attribution_score[0:12]: 
[ 1.00  1.00  0.00  0.00  0.00  1.00  0.00  0.00  0.00  1.00  0.00  1.00 ]
[ 1.00  0.00  1.00  0.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.00  1.00  0.00  1.00  0.00  1.00  0.00  0.00  0.00  1.00 ]
[ 0.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  0.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  0.00  0.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  0.00  0.00  0.00  1.00  0.00  1.00  0.00  0.00  0.00  1.00 ]
[ 1.00  1.00  0.00  1.00  1.00  0.00  0.00  0.00  1.00  1.00  1.00  1.00 ]
[ 0.00  0.00  0.00  1.00  0.00  1.00  1.00  0.00  1.00  1.00  0.00  1.00 ]
[ 1.00  1.00  0.00  1.00  1.00  0.00  0.00  1.00  1.00  0.00  1.00  1.00 ]
[ 1.00  0.00  0.00  0.00  1.00  1.00  0.00  0.00  1.00  0.00  0.00  0.00 ]
[ 0.00  1.00  1.00  0.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00  1.00  1.00  1.00 ]
[ 1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  1.00  0.00 ]

10/27 12:04:14 PM | VGG(
  (features): Sequential(
    (conv0): Conv2d(3, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm0): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (conv1): Conv2d(26, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv3): Conv2d(49, 75, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm3): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU(inplace=True)
    (conv4): Conv2d(75, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm4): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU(inplace=True)
    (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv6): Conv2d(94, 169, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm6): BatchNorm2d(169, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU(inplace=True)
    (conv7): Conv2d(169, 136, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm7): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU(inplace=True)
    (conv8): Conv2d(136, 163, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm8): BatchNorm2d(163, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU(inplace=True)
    (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv10): Conv2d(163, 233, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm10): BatchNorm2d(233, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU(inplace=True)
    (conv11): Conv2d(233, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm11): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU(inplace=True)
    (conv12): Conv2d(180, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm12): BatchNorm2d(255, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU(inplace=True)
    (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv14): Conv2d(255, 375, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm14): BatchNorm2d(375, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu14): ReLU(inplace=True)
    (conv15): Conv2d(375, 446, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm15): BatchNorm2d(446, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu15): ReLU(inplace=True)
    (conv16): Conv2d(446, 501, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (norm16): BatchNorm2d(501, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu16): ReLU(inplace=True)
  )
  (classifier): Sequential(
    (linear1): Linear(in_features=501, out_features=512, bias=True)
    (norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU(inplace=True)
    (linear2): Linear(in_features=512, out_features=10, bias=True)
  )
)
10/27 12:04:15 PM | ----------------------------------------
10/27 12:04:15 PM | Performances input model:
10/27 12:04:15 PM |  - flops: 314,294,272
10/27 12:04:15 PM |  - params: 14,991,946
10/27 12:04:15 PM |  - accuracy: 93.95999908447266
10/27 12:04:15 PM | ----------------------------------------
10/27 12:04:15 PM | Performances pseudo-pruned model:
10/27 12:04:15 PM |  - flops: 314,294,272
10/27 12:04:15 PM |  - params: 14,991,946
10/27 12:04:15 PM |  - accuracy: 82.72999572753906
10/27 12:04:15 PM | ----------------------------------------
10/27 12:04:15 PM | Performances pruned model:
10/27 12:04:15 PM |  - flops: 108,714,444
10/27 12:04:15 PM |  - params: 6,438,579
10/27 12:04:15 PM |  - accuracy: 82.72999572753906
